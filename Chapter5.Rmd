# Discussion and Conclusions

## Results Discussion

### Summary of Stages 1-4

The list of statistics used to assess each imputation was as follows:

1. Mean Scale Score Error (MASE) for Scales A and B
1. Mean of inidividual item scores for Scales A and B
1. St.dev inidividual item scores for Scales A and B
1. Cronbach's alpha of Scales A and B
1. Split half correlation between mean(A1, A2, A3) and mean(B1, B2, B3). These were the three variables in each scale that were subject to missingness and imputation.

Additionally, for the first round where there was no default class imputation so the ability of each method was measured by attribute (individual data points) and by class (individual or row).

At stage 1, there was a focus on the best rule selection step, as well as looking for any evidence of different behaviour between the datasets. Rule selection variants were based on the findings in @yin2003cpar and @li2001cmar, suggesting that sets of CARs often gave better results than just selecting the best rule by precedence. Two very clear outcomes emerged:

Firstly, there were clear differences in the performance on each metric between datasets. However, countrary to most assertions in the literature, the parameters after imputing the MNAR dataset rarely, if ever, had the largest bias of the three. It was expected that MNAR data would provide the most challenge to any imputation method but this was not found to be the case.

Secondly, among the methods tried, there was no statistically significant difference between any of them for any of the statistics measured. As a result, only the "best rule" variant was put forward to the subsequent stages as it was the least computationally expensive, although these computational differences would also be very small.

At stage 2, the default class methods were introduced. These compensate for ARM based imputation's inability to find a valid estimate for every missing data point by setting the majority class after the initial imputation as the default class, or drawing from a posterior frequency distribution. There was very strong evidence that method 2 had a larger bias in the scale totals and Cronbach's alpha, while doing better than method 1 for individual scale item standard deviations. It may be that the non-deterministic approach produces estimates with a slightly higher variance which is desirable for preserving the population standard deviations, but this should not come at the cost of iter-item reliability which is crucial for Likert scale analysis. As a result, method 1 is favoured for the next stage.

At stage 3, four precedence methods were compared, namely the default *Confidence*, Laplace's accuracy, Chi squared and wighted Chi squared [@li2001cmar]. Chi squared and weighted Chi squared appeared to be ahead of the other two variants in nearly all of the visual assessments although only a few of the statistical tests backed this up with strong to very strong evidence. There was nothing to separate Chi squared from weighted Chi squared, so the slightly less computationally expensive Chi squared measure was put forward to the next stage.

Stage 4 saw the introduction of the iterative-sequential procedural variants. This stage took the longest time to complete because *apriori* and rule filtering has to occur at every sequential step. The difference between variants was very pronounced, with statistical tests returning extremely small p-values but the results were nuanced, without a clear leader in all measures and datasets. However, it was clear that a sequential-iterative process with propensity generally returned the least biased parameters. Adding aggregate features often made a useful difference but this was less clear cut and the computational time complexity increased to the point where further experimentation with this variant was no longer practical. Therefore the iterative-sequential with propensity method, using the base dataset, only was put forward for the final benchmark.

### Summary of Results from the Benchmark Stage

To do

## Further Work

Detecting, with statistical significance, the patterns and small differences seen during some of the experimental stages required a much larger set of experimental replications which was not practical to run with the available resources. A factor in this is the performance of the *apriori* algorithm which makes the ARM based method relatively slow compared to any of the benchmarks. When combined into an iterative process, the method may be too slow to run for larger datasets. Converting the method to use the faster FP-Growth algorithm may be necessary for real-world applications. The ability to limit rule discovery by only relevant variables might also be faster, rather than finding all rules and pruning but this approach is not available in the arules package for R. In any case, it will be useful to run some experiments on much more powerful hardware that can churn through thousands of repetitions more speedily to confirm the apparent patterns by means of greater statistical power.

Expanding the dataset to include additional aggregate scores did have a positive effect on performance  but this came with a commensurate lenghtening of computation time. This indicates that additional information about the multi-item scale is beneficial and further investigation seems worthwhile if the time-complexity of the algorithm can be reduced. Other avenues to explore include using a weighting, so that rules implicating related items are more important than rules between unrelated items. This could be based on an initial reliability analysis. Another approach would be to split the dataset vertically such that only related items are considered together. However, this last approach would miss out small but potentially significant relationships through the rest of the dataset.

The technique has many tuning parameters that were not explored during the experimental stages and any of these could lead to further refinements. 

* During the rule discovery stages of the experiment, *Support* and *Confidence* were fixed. The values used had been found to provide the best balance of speed and finding enough rules during proto-typing but would be different for different datasets. There may be benefit to carrying out research that formalizes the best setting for these parameters or provides a useful rule of thumb.

* @liu2001classification proposes a flexible *Support* to ensure that minority classes are better represented during the CARs discovery. This would ensure that the imputed values were not overwhelmed by the majority class.

* The number of splits for the propensity based algorithm was fixed at two. As propensity added significant advantage when it was introduced, there may be value in understanding the effect of a more granular propensity model.

* Incorporating cumulative odds models and/or penalized regression, both of which are recommended for use in ordinal data analysis. For example, either of these could have been applied at the default class step of the algorithm or to weight or set rule precedence.

* The entire experiment was conducted with just one master model for synthetic data which leaves many scenarios unexplored. The results of the work done so far show some potential as the method performed well with severely asymmetric data and in some scenarios performed better on MNAR data than on other patterns of missingness. There was also a suprising discovery, that some of the benchmark methods had a tendency to drastically bias the data in a way that makes unrelated items appear to be much more correlated than they are in the underlying population, while the novel technique was immune to this effect. These nuanced results may prove to be useful in niche applications with real-world data so there is much scope for introducing other synthetic data models or any of the real-world datasets found during the literature review.

## Project Appraisal

As a project draws to a close it is always valuable to look back and reflect on the distance travelled. There has been a significant learning journey, both academic and non-academic and it is important to acknowledge this so as to be able to draw on these experiences in the future. This section recapitulates the objectives set at the start of the project and assesses whether they have been met effectively and what broader lessons can be learned (Objective 10). See Table \@ref(tab:objectives)

The literature began, in \secref{sec:survques} covering the topic of surveys, their widespread use, commercial and strategic value, and some historical context. This was especially useful to serve as a link between the project's experimental scope and the Business Intelligence context of the degree programme.

Soon after starting the study and research for \secref{sec:missing} on patterns of missingness (Objective 2) and \secref{sec:charsurv} on ordinal data and Likert scales (Objective 1), it became clear that this project was going to be far more involved and challenging than first anticipated. There was a body of theory and research on these topics stretching back decades to the early development of statistics and a realization that these were rich fields of research in their own right. A long-term benefit from this work was the curation of a list of prolific authors and important articles for future reference. Also gained was an increased familiarity with specialized analytic tools and techniques. An important action to take-away is to look for ways to gain a clearer understanding of the depth and breadth of background detail at the project proposal stage. This is important to avoid a situation where the foundational reading list balloons before a project has really got off the ground.

It was essential to understand the pros and cons of state of the art imputation techniques (Objectve 3) before setting about developing and implementing a new algorithm. The complicated theoretical foundations for MI, discussed in \secref{sec:currap} turned out to be one of the deepest learning experiences. Contrastingly, \secref{sec:assrul} (Objective 4) was easier to compile as this topic was more familiar, but on reflection \secref{sec:classrul} would have benefitted from more theory and more formalism and proofs when arguing that CARs could be applied to the imputation problem. Time should be spent on developing these skills further.

Coding and prototyping began in a previous project and continued with the search for suitable datasets and effective techniques for synthesizing realistic Likert scale data (Objective 6) and this resulted in many possible options which were incorporated into the algorithm. The choice to build a software package according to best practices for R development reaped dividends during the experimental phases because code encapsulation and re-usability minimised typographical errors and facilitated bug-fixes and algorithm refinement. 

After all the reading from the literature review, the algorithmic design (Objective 7) came almost intuitively but this is another area would have benefitted from more academic rigour and formalism. At the time that this task was tackled, prototype code was stable and mature and the complete solution was already evolving through the iterative and incremental process of testing and refactoring the code base. The resulting routines, specifically the iterative-sequential imputation, ran extremely slowly even compared to MI. A more formal approach to analyse the algorithmic time complexity and look for major improvements to time performance should be adopted from the outset and will be required before moving the new algorithm into real-world applications. One work-around could have been to try to migrate to a cloud-based infrastructure such as AWS or Microsoft Azure to gain access to greater computing power but, by the time this obstacle was discovered, there was not enough time to even begin appraising such a major change of plan and there was no alternative but to persevere with a very long running process.

The implementation, coding and automation (Objective 8) of the experimental phase was not without its challenges and many late hours were spent grappling with the idiosyncrasies of the R environment. This was very intensive period but was to be expected for such a multi-faceted project. Also, this is where the long-running iterative-sequential algorithm appeared as a significant challenge and required a reduction in the number of replica experiments for the project to be able to finish in a reasonable time. Despite these challenges, careful planning of the multiple-stages and the use of very simple and practical statistical tests allowed for clear and consistent interpretation of the results and rapid transition through the stages of the experiment.

The other significant challenge was simply to deal with the wealth of material discovered during the literature review and trying to control the scope of the project. The multi-stage experimental design is perhaps the clearest manifestation of the way the project content fanned out and was extremely challenging to edit. As a result, the project took many more hours and much more effort that originally planned for. The most important lesson still to be learned from this is how to maintain a narrower focus and not try to tackle every open research question at once.