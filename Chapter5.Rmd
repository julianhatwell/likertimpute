# Discussion and Conclusions

## Results Discussion

### Summary of Stages 1-4

The list of statistics used to assess each imputation was as follows:

1. Mean Scale Score Error (MASE) for the summed scores of Scales A and B
1. Means for Scales A and B
1. Standard deviations for Scales A and B
1. Cronbach's alpha of Scales A and B
1. Split-half correlation between mean(A1, A2, A3) and mean(B1, B2, B3). These were the three variables in each scale that were subject to missingness and imputation.

Additionally, for the first round where there was no default class imputation so the ability of each method was measured by attribute (individual data points) and by subject (data row).

At stage 1, there was a focus on the best rule selection step, as well as looking for any evidence of different behaviour between the datasets. Rule selection variants were based on the findings in @yin2003cpar and @li2001cmar. These articles suggested that sets of CARs in combination often gave better results than just selecting the best rule by precedence. For this part of the experiment, mean, majority vote and right-hand side frequency were compared to just taking the best rule by precedence. Two very clear outcomes emerged:

Firstly, there were clear differences in the performance on each metric between datasets. However, countrary to most assertions in the literature, the parameters after imputing the MNAR dataset rarely, if ever, had the largest bias of the three. It was expected that MNAR data would provide the most challenge to any imputation method but this was not found to be the case.

Secondly, among the methods tried, there was no statistically significant difference between any of them for any of the statistics measured. As a result, only the "best rule" variant was put forward to the subsequent stages as it was the least computationally expensive, although these computational differences would also be very small.

The minimum case Ability statistic was $\approx$ 0.87 $\pm$ 0.07, meaning that more than 85% of incomplete cases in the untreated data were recovered by the imputation. Depending on the specific requirements of the subsequent analysis, this might be acceptable. Drilling down to the level of individual data points, the Ability ranged from $\approx$ 0.68 $\pm$ 0.09 for MCAR data to $\approx$ 0.78 $\pm$ 0.06 for MAR data. This is somewhat concerning as it implies that up to 35% of individual data points might be substituted with the default class rather than a value that was directly related to the observed data. This might go some way to eplaining the method's tendency to over-emphasize any existing skew in the data.

At stage 2, the default class methods were introduced. These compensate for ARM based imputation's inability to find a valid estimate for every missing data point by setting any remaining missing values to a default class. This stage of the experiment compared two methods:

1. Default class set to the majority class after the initial imputation. 
1. Tabulate the class frequencies after imputation and use this as a posterior frequency distribution to drawing random values.

There was very strong evidence that method 2 had a larger bias in the scale totals and Cronbach's alpha, while doing better than method 1 for individual scale item standard deviations. It it very likely that the stochastic nature of method 2 produces estimates with a higher variance. This is desirable for preserving the population standard deviations but should not come at the cost of iter-item reliability which is crucial for Likert scale analysis. As a result, method 1 was favoured for the next stage.

At stage 3, four precedence methods were compared, namely the default *Confidence*, Laplace's accuracy, Chi squared and wighted Chi squared [@li2001cmar]. Chi squared and weighted Chi squared appeared to be ahead of the other two variants in nearly all of the visual assessments although only a few of the statistical tests backed this up with strong to very strong evidence. There was nothing to separate Chi squared from weighted Chi squared, so the slightly less computationally expensive Chi squared measure was put forward to the next stage.

Stage 4 saw the introduction of the iterative-sequential procedural variants. This stage took the longest time to complete because rule generation with *apriori* and rule filtering/pruning has to occur at every sequential step. The difference between variants was very pronounced, with statistical tests returning extremely small p-values. Neverthelss, the results were nuanced, without a clear leader in all measures and datasets. It was apparent that a sequential-iterative process with propensity generally returned the least biased parameters. Adding aggregate features to the dataset often made a useful difference but this was less clear cut and the computational time complexity increased to the point that further experimentation with this variant was no longer practical. Therefore, the only method to be put forward to the final benchmark was the iterative-sequential method with propensity, using the base dataset.

### Summary of Results from the Benchmark Stage

The AR-based imputation process was set up to run as a multiple imputation method, similar to MICE. That is to say, it ran in an iterative-sequential mode until convergence, and the entire process was run for $m$ = 5 times, with a completed dataset collected for each $m$.

On symmetric data, the benchmark MI methods, MI (EMB) and MICE, had the largest MASE on both scales while preserving the Scale standard deviations. For the single imputation benchmark methods, the reverse was true. They had a more deflated Scale standard deviations but maintained the overall Scale scores a little more faithfully. There is little detectable difference in Scale means between any of the methods. The AR-based method was not completely consistent. It returned a smaller bias for MASE on both scales than either of the benchmark MI methods and a better standard deviation score than the single imputation methods on Scale A. It did worst of all on individual item standard deviations for Scale B. For these measures, the AR-based method also performed worst of all on MAR data. This data differs in that there is the inclusion of an additional two co-variates and it is possible that these result in some spurious or useless rules being generated.

On the Cronbach's alpha statistic, the AR-based method did better than all the benchmark's apart from MICE and with the exception of the MAR data again. It also compared very well to both MI and MICE for maintaining the split-half correlation between the unrelated items. The single imputation methods performed disasterously for this statistic and would be sure to increase the risk of fallacious conclusions in any subsequent analysis.

On severly asymmetric data, it was rather a different story, with the AR-based method performing less well than the benchmarks on MASE, Scale means and standard deviations, and Cronbach's alpha. It is reasonable to suggest that these symptoms stem from the ruleset being overwhelmed by rules that favour the default class. This would reduce the variance in the estimates, and pull the mean towards the default class which is skewed to the lower end of the scale. The effect is not drastic, but it is detectable against the benchmarks. The only result that was consistent with the symmetric data set was the very well preserved split-half correlation.

Given these results, the most honest conclusion is that the new AR-based method is not yet ready for real-world applications. While it compares well to the MI and MICE benchmarks on symmetric data, it currently runs much too slowly. MICE appears to do better and more consistently on every measure and should be the recommended imputation method used under all the circumstances tested in this research.

## Further Work

Detecting, with statistical significance, the patterns and small differences seen during some of the experimental stages required a much larger set of experimental replications which was not practical to run with the available resources. A factor in this is the performance of the *apriori* algorithm which makes the AR-based method relatively slow compared to any of the benchmarks. When combined into an iterative process, the method may be too slow to run for larger datasets. Converting the method to use the faster FP-Growth algorithm may be necessary for real-world applications. The ability to limit rule discovery by only relevant variables might also be faster, rather than finding all rules then pruning. Unfortunately this approach is not available in the arules package for R. In any case, it will be useful to run some experiments on much more powerful hardware that can churn through thousands of repetitions more speedily to confirm the apparent patterns by means of greater statistical power.

Expanding the dataset to include additional aggregate scores did have a positive effect on performance, but this came with a commensurate inflation in computation time. The additional information about the multi-item scale appeared to be beneficial and further investigation is worthwhile if the time-complexity of the algorithm can be reduced. Other avenues to explore include using a weighting, so that rules implicating related items are more important than rules between unrelated items. This could be based on an initial reliability analysis. Another approach would be to split the dataset vertically such that only related items are considered together. However, this last approach would miss out small but potentially significant relationships through the rest of the dataset.

Although this project has not delivered a new world-class imputation method, the nuanced results suggest the method has potential. For example, examining the diagnostic plot in Figures \@ref(fig:dens-diag1) and \@ref(fig:dens-diag1), it is possible to see that the AR-based method yields a distribution of estimated values that is strongly influenced by the skew in the severely asymmetric data while the pattern produced by MICE has a strong central tendency, which is a manifestation of the underlying normality assumption. Although the results of this project don't empirically demonstrate the benefits of the AR-based model's behaviour, it is still expected to give useful results in practice. Much of the over-production of value 1 by the AR-based method, at least in Figure \@ref(fig:dens-diag1), arises from the $\approx$ 25% of default class imputations and resolving some of the remaining research questions would directly address this problem:

1. Comparatively poor performance on severely asymmetric or skewed data requires a rule discovery and pruning approach that allows better representation of minority classes. @liu2001classification proposes using multiple-class minimum *Support* to ensure that the imputed values were not overwhelmed by the majority class.

2. Comparatively larger downward bias of individual item standard deviations could be addressed by further investigation of the stochastic variants which were put aside after the early stages of the experiments. These include the rhs frequency rule selection and the frequency distribution based default class method. As stated in the results section, there was no detectable difference between any of the methods in stage 1 and it would be reasonable to re-run some of the experiments again with different alternatives.

```{r dens-diag1, fig.height=3, fig.cap="Imputation diagnostic plot comparing distribution of untreated data with distribution of estimated values."}
load("sample_results2.RData")

res <- sample_results$wu_sasym_MNAR_0.3_1000
m <- 5
varnames <-  paste0(rep(c("A", "B"), each = 3), 1:3)
allvarnames <- paste0(rep(c("A", "B"), each = 6), 1:6)
methods <- c("amelia", "mice", "ari")
completed_results <- list()
for (meth in methods) {
  completed_results[[meth]] <- list()
  for (v in varnames) {
    pooled <- matrix(NA, nrow = 1000, ncol = m)
    for (n in 1:m) {
      pooled[, n] <- res[[meth]][[n]][[v]]
    }
    completed_results[[meth]][[v]] <- round(rowMeans(pooled))
  }
  completed_results[[meth]] <- cbind(completed_results[[meth]]
                                     , sample_results$wu_sasym_MNAR_0.3_1000$untreated[, c(4:6, 10:12)])
  completed_results[[meth]] <- completed_results[[meth]][, allvarnames]
}

all_results <- append(completed_results
                            , list(PM = sample_results$wu_sasym_MNAR_0.3_1000$PM
                            , CIM = sample_results$wu_sasym_MNAR_0.3_1000$CIM
                            , TW = sample_results$wu_sasym_MNAR_0.3_1000$TW
                            , ICS = sample_results$wu_sasym_MNAR_0.3_1000$ICS
                            , untreated = sample_results$wu_sasym_MNAR_0.3_1000$untreated))

results_byvar <- list()
for (v in varnames) {
  for (dt in names(res)) {
    results_byvar[[v]][[dt]] <- all_results[[dt]][[v]]
  }
  results_byvar[[v]] <- as.data.frame(results_byvar[[v]])
}


myFills <- c(ari = myPal[6], mice = myPal[8])
myCols <- c(untreated = myPal[7])

v <- "A1"
g <- ggplot(data = results_byvar[[v]][res$to_impute$mim$B_j[[v]], ]
            , aes(x = untreated
                  , colour = "untreated"))
g <- g + geom_density(data = results_byvar[[v]][res$to_impute$mim$B_comp_j[[v]], ]
                      , aes(x = ari
                            , fill = "ari")
                      , alpha = 0.5
                      , colour = "transparent")
g <- g + geom_density(data = results_byvar[[v]][res$to_impute$mim$B_comp_j[[v]], ]
                      , aes(x = mice
                            , fill = "mice")
                      , alpha = 0.5
                      , colour = "transparent")
g <- g + geom_density() # data = sample_results$wu_sasym_MNAR_0.3_1000$untreated
g + myGgTheme + theme_bw() + xlim(0, 8) +
  scale_fill_manual(name = "imputation \nmethod"
                    , values = myFills
                    , guide = "legend") +
  scale_colour_manual(name = ""
                    , values = myCols
                    , guide = "legend") +
  labs(title = paste("Density of untreated data and estimated values \nby ARImpute and MICE for variable", v, "on MNAR data")
       , x = "Data Value")
```

```{r dens-diag2, fig.height=3, fig.cap="Imputation diagnostic plot comparing distribution of untreated data with distribution of estimated values."}
v <- "A3"
g <- ggplot(data = results_byvar[[v]][res$to_impute$mim$B_j[[v]], ]
            , aes(x = untreated
                  , colour = "untreated"))
g <- g + geom_density(data = results_byvar[[v]][res$to_impute$mim$B_comp_j[[v]], ]
                      , aes(x = ari
                            , fill = "ari")
                      , alpha = 0.5
                      , colour = "transparent")
g <- g + geom_density(data = results_byvar[[v]][res$to_impute$mim$B_comp_j[[v]], ]
                      , aes(x = mice
                            , fill = "mice")
                      , alpha = 0.5
                      , colour = "transparent")
g <- g + geom_density() # data = sample_results$wu_sasym_MNAR_0.3_1000$untreated
g + myGgTheme + theme_bw() + xlim(0, 8) +
  scale_fill_manual(name = "imputation \nmethod"
                    , values = myFills
                    , guide = "legend") +
  scale_colour_manual(name = ""
                    , values = myCols
                    , guide = "legend") +
  labs(title = paste("Density of untreated data and estimated values \nby ARImpute and MICE for variable", v, "on MNAR data")
       , x = "Data Value")
```

The AR-based technique has many tuning parameters that were not explored during the experimental stages and any of these could also lead to further refinements:

* During the experiment, *Support* and *Confidence* were fixed for rule discovery. The chosen values were found to provide the best balance of speed and finding enough rules but would be different for different datasets. There may be benefit to further research that formalizes the best tuning for these parameters or provides a useful rule of thumb.

* Splits for the propensity based algorithm was fixed at two. As propensity added significant advantage when it was introduced, there may be value in understanding the effect of increasing this number.

* Incorporating cumulative odds models and/or penalized regression. Both are recommended for use in ordinal data analysis and either could have been applied at the default class step of the algorithm or to weight or set rule precedence.

* Allowing iterative-sequential methods to run for more iterations, as the stopping parameters were set in interests of reasonable completion time for the experiments. There was scope for the routines to be allowed to run for longer.

Throughout the experimental phase, the AR-based method appeared to have a specific advantage of performing very consistently with MNAR data, even though this was expected to be the most challenge based on the findings in the literature review. However, when the benchmark methods were brought in for Stage 5, it was obvious that there was no special effect as each method had the same pattern in performance over the different patterns of missingness. The whole notion that MNAR data is such a challenge for imputation has been called into question during this project. This is potentially a very significant area of research in its own right. In any case, the entire experiment was conducted with just one master model for synthetic data which leaves many scenarios unexplored. The literature review unearthed a number of useful real-world datasets and different methods of simulating MNAR data have been built into the arulesimp package for R. Other data scenarios, such as bigger or smaller sample sizes and different proportions of missing data are yet to be explored.

There was also a suprising discovery, that some of the benchmark methods had a tendency to drastically bias the data in a way that makes unrelated items appear to be much more correlated than they are in the underlying population. As such, these single imputation techniques may only be useful for imputing within one Likert scale at a time, which is potentially important information for the community. The novel technique, along with MI and MICE, were immune to this effect. 
The nuanced results from this project may prove to be useful in niche applications with real-world data. There is much scope for introducing other synthetic data models or any of the real-world datasets found during the literature review in future research.

## Project Appraisal

As a project draws to a close it is always valuable to look back and reflect on the distance travelled. There has been a significant learning journey, both academic and non-academic and it is important to acknowledge this so as to be able to draw on these experiences in the future. This section recapitulates the objectives set at the start of the project and assesses whether they have been met effectively and what broader lessons can be learned (Objective 10). See Table \@ref(tab:objectives)

The literature review began with \secref{sec:survques} covering the topic of surveys, their widespread use, commercial and strategic value, and some historical context. This was especially useful to serve as a link between the project's experimental scope and the Business Intelligence context of the degree programme for which it is submitted. Soon after starting the study and research for \secref{sec:charsurv} on ordinal data and Likert scales (Objective 1) and \secref{sec:missing} on patterns of missingness (Objective 2), it became clear that this project was going to be far more involved and challenging than first anticipated. There was a body of theory and research on these topics stretching back decades to the early development of statistics and a realization that these were rich fields of research in their own right. A long-term benefit from this work was the curation of a list of prolific authors and important articles for future reference. Also gained was an increased familiarity with specialized analytic tools and techniques. An important action to take-away is to look for ways to gain a clearer understanding of the depth and breadth of background detail at the project proposal stage. This is important to avoid a situation where the foundational reading list balloons before a project has really got off the ground.

It was essential to understand the pros and cons of state of the art imputation techniques (Objectve 3) before setting about developing and implementing a new algorithm. The complicated theoretical foundations for MI, discussed in \secref{sec:currap} turned out to be one of the deepest learning experiences. Contrastingly, \secref{sec:assrul} (Objective 4) was easier to compile as this topic was more familiar, but on reflection \secref{sec:classrul} would have benefitted from more theory and more formalism and proofs when arguing that CARs could be applied to the imputation problem. Time should be spent on developing these skills further.

Coding and prototyping began in a previous project and continued with the search for suitable datasets and effective techniques for synthesizing realistic Likert scale data (Objective 6) and this resulted in many possible options which were incorporated into the accompanying software library for R. The choice to build a software package according to best practices for R development reaped dividends during the experimental phases because code encapsulation and re-usability minimised typographical errors and facilitated bug-fixes and algorithm refinement. 

After all the reading from the literature review, the algorithmic design (Objective 7) came almost intuitively but this is another area would have benefitted from more academic rigour and formalism. At the time that this task was tackled, prototype code was stable and mature and the complete solution was already evolving through the iterative and incremental process of testing and refactoring the code base. The resulting routines, specifically the iterative-sequential imputation, ran extremely slowly even compared to MI. A more formal approach to analyse the algorithmic time complexity and look for major improvements to time performance should be adopted from the outset and will be required before moving the new algorithm into real-world applications. One work-around could have been to try to migrate to a cloud-based infrastructure such as AWS or Microsoft Azure to gain access to greater computing power but, by the time this obstacle was discovered, there was not enough time to begin appraising such a major change of plan and there was no alternative but to persevere with a very long running process.

The implementation, coding and automation (Objective 8) of the experimental phase was not without its challenges and many late hours were spent grappling with the idiosyncrasies of the R environment. This was very intensive period but was to be expected for such a multi-faceted project where novel solutions were being designed and implemented. Also, this is where the long-running iterative-sequential algorithm appeared as a significant challenge and required a reduction in the number of replica experiments for the project to be able to finish in a reasonable time. Despite these challenges, careful planning of the multiple-stages and the use of very simple and practical statistical tests allowed for clear and consistent interpretation of the results and rapid transition through the stages of the experiment.

The other significant challenge was simply to deal with the wealth of material discovered during the literature review and trying to control the scope of the project. The multi-stage experimental design is perhaps the clearest manifestation of the way the project content fanned out and was extremely challenging to edit. As a result, the project took many more hours and much more effort that originally planned for. The most important lesson still to be learned from this is how to maintain a narrower focus and not try to tackle every open research question at once.