# Literature Review

In this section, various subtopics are reviewed in a logical order that reflects the motivation for this research question: The breadth of use of surveys and questionnaires and the problem of missing data, the well-understood underlying models and current popular and state of the art solutions are reviewed systematically and in detail before exploring the potential suitability of association rules as a basis for a new imputation method.

## The Problem Domain

### The Importance of Surveys and Questionnaires

@groves2011three [p. 861] states that "the systematic observation of social and economic phenomena has roots deep in history" but describes the 1930's-1960's period as foundational for the development and popularization of the survey method involving structured questions, random sampling and statistical inference. It was also a golden age with respect to response rates, perhaps due to the novelty and the small numbers of surveys carried out, relative to today. Understandably, the earlier historical discussions in the article are backed up by fewer references than paragraphs covering more recent decades. Nevertheless, the author is well regarded and prolific on this subject matter. As a potted history of surveys and questionnaires, the article provides a very plausible interpretation of the influence of sociological and technological factors which explain why the survey method became so ubiquitous up to and during the 1990's but began to see a decline in usage thereafter. For example, one suggestion is that the popularity of the method was driven in part by usage in the consumer and service sectors as a means to gain customer feedback and thereby maximize profits. This speaks to economic common sense. The aid of computers in the 1960's reduced processing costs dramatically, allowing greater numbers of participants, up to hundreds of thousands, @schultz2015psychology. As well as brute force, more sophisticated computational analysis would have been another driver both in commerce and government research. In academia, the publication of archived data "permitted the rise of quantitative methods in many social science disciplines," @groves2011three [p. 865]. 

Consistent with the description of a decline in popularity in the 1990's, @sheehan2001mail posits that even the change from face-to-face and postal to email-based surveys provided only a temporary stay in the overall decline in usage in more recent times. However, there is no shortage of examples to suggest that the method is still incredibly important today, [@schultz2015psychology; @pantanowitz2008evaluating; @bono2007missing; @shrive2006dealing; @evans2005value]. 

@groves2011three goes on to make a number of reasoned predictions about future adaptations of the survey method in light of the emergence of a multitude of new data sources (of which the author's description matches what is now commonly coined as "Big Data") and continues to suggest that these new sources of data are collected organically, i.e. they are "unstructured", so there's an ongoing requirement for the "designed data" of surveys. One technological development is not discussed in detail in the article, namely the emerging popularity of web services for the design and distribution of online surveys (SurveyMonkey, Qualtrix, www.onlinesurveys.ac.uk, SmartSurvey, SogoSurvey to name but a few). This could be because the article is not recent enough or perhaps as @solomon2001conducting describes, because of the caution with which this new technology was viewed by the academic community. Such caution may relate to statistical issues such as coverage bias resulting from "the skew in demographics of the internet population versus the general public" [@hayslett2005pixels p. 78]. It may be combined with the practical need for researchers to become more familiar with the technology before being able to construct surveys with sufficiant statistical rigour. Nevertheless, @schultz2015psychology states that most companies now use web-based surveys. In fact, the authors suggest that the value to business is not limited to customer feedback as merely carrying out a survey in an industrial/organisational setting can raise staff morale and reduce conflict between management and unions.

@evans2005value shows that online surveys were already a rapidly growing, multi-million dollar industry by the early 2000's. The article describes many weaknesses as well as strengths in the sector. Indeed, it was very much a clarion call for addressing the weaknesses to consolidate the value of online surveys for the modern day. It would be safe to assume that these have been addressed for there has been a rise and rise of services for the creation of casual surveys and polls by the "citizen analyst" as well as seasoned pollsters, which are distributed through social media and quick customer ratings devices throughout malls, airports and other public infrastructure. These technological innovations introduce the need for development of new methods that stand up to analytical scrutiny and offer many new opportunities given their widespread dissemination.

### Characteristics of Survey Data

@johnson2009working states that nominal and ordinal variables are the predominant data types in surveys. This is certainly in line with anecdotal and personal experience at least. When collecting demographic information, most routine information will be categorical, e.g. eye colour, hair colour, ethnic origin. Height, weight, income and age are frequently discretized in to buckets or intervals. Beyond these simple segmentations, market researchers, researchers in the social sciences and other disciplines often use surveys to collect data on subjective attitudes, perceptions and psychological traits. Indeed, @christensen2010ordinal [p.3] states that "ordered categorical data, or simply ordinal data, are commonplace in scientific disciplines where humans are used as measurement instruments." Ordinal variables are ideal for capturing gradings, ratings, opinions, degree of agreement, rankings, levels of attainment and so on. They are also very practical in terms of survey design considerations. In this respect @schultz2015psychology [p.37] explains that "fixed-alternative questions simplify the survey and allow more questions to be asked," and also "answers to fixed-alternative questions can be recorded more easily and accurately than can answers to open ended questions."

There are two categories of ordinal data, according to @anderson1984regression cited in @gertheiss2009penalized

* grouped continuous variables (GCV) - an underlying, continuous variable discretized into buckets, shingles or categories, such as might be returned by a histogram function over age, height or weight. 
* assessed ordered categorial variables (AOCV) - a judgement given by some actor or assessor on the grade, level or rank of a set of information which is not among the observed variables.

Ordinal data generally take the form of a categorical variable with levels $k \in \{0,\ \dots,\ K\}$ or $k \in \{1,\ \dots,\ K\}$ or sometimes a symmetric form $k \in \{-K,\ \dots,\ 0,\ \dots,\ K\}$ and all are interpreted as a monotonic, ranking representation $Least \succ Less \succ More \succ Most$. Ordinal variables of type AOCV can be used individually but are often combined in a survey as multiple related items (multiple-item response scales). Likert scales are a special case of ordinal variable and a special case of multiple-item response, whereby a collection of statements are scored by the respondent using an integer series which is anchored to phrases of sentiment or attitude (e.g. 1 = strongly disagree, 5 = strongly agree). The collection of responses measure the outwardly manifested facets of some latent concept or factor. The respondents' positions on the latent factor can then be estimated by appropriately summing, averaging or otherwise transforming the multiple responses of the individual items [@carpita2011imputation]. Likert scales are ubiquitous in behavioural science, marketing, usability, customer feedback, psychological and clinical research. Although the term Likert scale is sometimes used in the literature to refer to single items following the classic agree/disagree scoring, the correct definition refers to a combined scale as described here. A thorough treatment of Likert scales is given in @gliem2003calculating.

It is common in the literature to find misunderstandings and disagreement about how ordinal variables ought to be treated analytically. In fact, this is one of the great debates of statistics in the last few decades. A range of conflicting views which is typical of the long-running discussion is reviewed in @gertheiss2011testing reviews. The disagreements centre on the question of whether it is appropriate to use the integer class labels directly as explanatory variables in a regression model. @johnson2009working gives detailed examples of why ordinal variables cannot be treated the same as numerical scales and @gertheiss2009penalized explains how this assertion holds even when the ordinal variable is a discretised version of an underlying continuous scale, as in GCV. Essentially, this is because the interpretation depends on arbitrarily assigned categories and mid-point estimates. Upper and lower unbound categories may hide any number of extreme values. This would seem to be very sound reasoning and @joreskog2005structural provides a formalization of the concept of inferred discretization, reproduced in \@ref(eq:ordvar1)-\@ref(eq:ordvar2). These show the relationship between $z$, an observed ordinal variable which varies between 1 to $m$ integer values and $z^*$, an underlying, unobserved variable which has a range from $-\infty$ to $\infty$.

\begin{equation} 
  z = i \Longleftrightarrow \tau_{i-1} < z^* < \tau_i, \quad i = \{1, 2,\ \dots,\ m\}
  (\#eq:ordvar1)
\end{equation}

where

\begin{equation} 
  -\infty = \tau_0 < \tau_1 < \tau_2 < \dots < \tau_{m-1} < \tau_m = \infty
  (\#eq:ordvar2)
\end{equation}

The $\tau$ terms are arbitrary thresholds and are unknown population parameters.Their values may be estimated in some circumstances, such as from percentage counts of responses $z$ when $z^*$ is assumed normal.

When analyzing individual items of Likert and similar scales the debates are even more polarized. @joreskog2005structural takes the firm position that it is wrong even to use means, variances and covariances. The reasons given are that the integer values do not represent a continuous scale. They should not be attributed metric properties as they have neither an origin nor units. @jamieson2004likert goes even further, stating that it is also wrong to presume that intervals between categories are equal. There is no assumption in \@ref(eq:ordvar2) that the $\tau$ terms follow a regular pattern. In practice, this means that the barrier to reach the highly polarised strongly disagree or strongly agree may be inconsistent with a move from neutral to either agree or disagree. Also, these types of scales are often skewed or multimodal and require non-parametric tests such as Mann-Whitney-Wilcoxon (MWW) for analysis. @gliem2003calculating asserts that these data types are intended to be used as a combined scale measuring a latent concept, and asserts that their individual analysis leads to erroneous conclusions. In the paper, an experiment is conducted in which respondents are asked to respond to the same single item a second time after a three week interval. The retest reliability coefficient turns out to be very low at 0.11. This result is compared to a well designed, Likert based survey. Unfortunately, this experimental design is potentially flawed because the test conditions do not resemble the control condition. Under test, respondents are asked a single question and may react very differently to when they are presented with a ten-item questionnaire. Respondents may be prone to providing trivial responses to a single question based on mood alone, rather than weighing up the varied statements of the multi-item scale. A differently designed experiment should provide the test group with a number of unrelated items that better approximate the look and feel of a Likert based questionnaire.

Other authors [@norman2010likert; @carifio2008resolving] strongly rebut these arguments, making the case for the robustness of parametric tests and citing various sources of empirical evidence. @de2010five compares the type I and type II error rates of the t-test against the MWW test using synthetic data which finds that there is no benefit to either test with respect to type I errors and slight differences in performance with type II error rates, and consequently the power of the test. The direction of these differences depends on the underlying populations and may favour t-tests in some cases and MWW in others. Crucially, this article only tests one (synthetic) variable at a time, so while it is useful in the context of individual ordinal variables, it does little to settle the debate on Likert scales.

Valuable contributions to the discussion come from authors and statistical researchers who specialize in discrete data analysis. Among these, the *proportional odds model* (also known as *cumulative logit model* and *ordered logit model*) is favoured for statistical analysis where the dependent variable is ordinal [@friendly2016discrete; @R-ordinal; @christensen2010ordinal]. *Penalized likelihood estimation* and *penalized regression* are favoured where the predictors are ordinal [@R-ordPens; @gertheiss2009penalized]. Tests based on these models are shown to perform better than models based on linear assumptions [@gertheiss2011testing] and predictions will always fall within the range of possible responses without the need for rounding, top-coding and truncation, approximation etc, @christensen2010ordinal.

Fortunately, there is agreement that combining individual responses into their intended scale leads to the least controversial usage and analytical or probabilistic techniques over single items in a Likert scale should be avoided where possible. Cronbach's alpha is a commonly used statistic for measuring reliability (inter-item correlation). It is usually defined as:

\begin{equation}
  \alpha = \frac{N\bar{c}}{\bar{v}+(N-1)\bar{c}}
  (\#eq:cronbach)
\end{equation}

where $N$ is the number of items, $\bar{v}$ is the mean variance and $\bar{c}$ is the mean covariance between item pairs. It is trivial to calculate $\alpha$ in most statistical software thanks to its widespread acceptance but even on this topic there has been some contention over acceptable sample size. Sample sizes of less than 300 were not advised although more recently the test was found to be robust down to sample sizes of 30 on condition that the first eigenvalue is greater than 6. Also, the general rule of thumb that an alpha of 0.7 or greater is acceptable should be treated with caution for scales greater than six individual items, [@kline1986handbook; @yurdugul2008minimum; @cortina1993coefficient] cited in @cas2016statistical. A multiple-response scale with a high $\alpha$ implies that the individual items reliably measure the same underlying construct and their combined scores can be taken as a valid continuum. In addition to Cronbach's alpha, Exploratory Factor Analysis (EFA) has become more common in recent years for reliability analysis and validation of untested scales and McDonald's omega is more reliable when inter-item correlations can't be assumed constant. This last assumption is probably true in practice far more frequently than not.

This discussion has been detailed and extensive for good reason. Correct and appropriate analysis of ordinal and Likert scales is essential for the success of this research project. A set of reliable tests is required to determine whether imputation has had a detrimental effect on the statistical properties of target variables.

### Non-Response and Missingness

>  "Respondents do not answer every question, countries do not collect statistics every year, archives are incomplete, subjects drop out of panels."

@honaker2011amelia [p.1]

> "Missing observations are the rule rather than the exception in marketing data."

@kamakura2000factor [p.490]

The practicalities of running surveys means that they regularly suffer the problem of non-response and missing data [@plumpton2016multiple; @bono2007missing; @kamakura2000factor]. In the broadest terms, non-response can occur at two levels: Unit and Item. 

* Unit non-response occurs when one or more respondents fail to return anything to the survey organisers. This may be a one-off survey, or may be one in a series of surveys in a longitudinal study. If the non-response rate is high, this can impact the overall sampling frame. An alternative definition of unit non-response is given in @leite2010performance which states that this is a block of variables all missing for a number of cases that are complete for all other variables. This sometimes occurs for structural reasons in the survey design.

* Item non-response occurs when individual items (questions) in a survey are skipped for any reason, but the whole survey is returned to the organisers.

Both types of non-response have serious consequences. However, this work is concerned only with item non-response and will be the focus for the remainder of this section, unless otherwise stated.

Understanding the model (patterns, mechanism and magnitude) of missingness in data is critical when selecting or designing techniques to recover from the resulting issues. Fortunately, there has been an enormous amount of research in this area over recent decades. For example, when examining the effect of survey length on response rates, @sheehan2001mail reports numerous prior studies that illustrate how longer surveys tend to reduce overall (unit) response rates, particularly for business-oriented surveys. Only one example is given of a study that found the opposite pattern but this is not enough to call into question the overall trend. Crucially, there is no discussion of the implication of survey length on the pattern of item non-response. Specifically it would be useful to determine whether longer surveys suffer more incomplete items towards the end than near the beginning. From the historical outline given in @groves2011three, this must be assumed to be the case. The article describes a shift to shorter questionnaires with the express purpose of reducing  hang-up rates at the same time as surveys by telephone became more popular. A ragged tail pattern of non-response, when visualised, would be a clear indicator of a survey that respondents found overly long.

```{r nonresp-reasons}
reason <- c("Failure to complete the questionnaire"
          , "Failure to complete the questionnaire"
          , "Unwillingness to answer"
          , "Unwillingness to answer"
          , "Data entry errors"
          , "Data entry errors"
          , "Structural design of questionnaire"
          , "Structural design of questionnaire")
cause <- c("Absent-mindedness"
          , "Skipping a page in error"
          , "Personal issues: income, status, health"
          , "Perceived not relevant: drug use, sexual orientation"
          , "Manual conversion of paper forms"
          , "Using incorrect marking e.g. check instead of filled box"
          , "Questions skipped depending on previous answers"
          , "Poorly designed e.g. too long"
           )
reasonstable <- data.frame(Reason = reason
                  , "Examples" = cause)
knitr::kable(reasonstable
             , caption = "Reasons for item non-response"
             , booktabs = TRUE)
```

```{r nonresp-patterns}
pattern <- c("Univariate"
             , "Multivarate"
             , "Ragged tail"
             , "Missing blocks"
             , "Missing blocks"
             , "Missing regular")
explanation <- c("Only one item has missing data"
                 , "Missing data throughout, seemingly random"
                 , "Missing all after a certain question"
                 , "Groups reply to different questions"
                 , "Groups reply to different questions"
                 , "Questions skipped depending on previous")
posscause <- c("Badly worded, irrelevant question"
               , "Any"
               , "Survey is too long"
               , "Data fusion: combined surveys"
               , "A subsample are followed up"
               , "Structural constraints")
patternstable <- data.frame(Pattern = pattern
                            , Explanation = explanation
                            , "Possible Cause" = posscause
                            , check.names = FALSE)

knitr::kable(patternstable
             , caption = "Patterns of non-response"
             , booktabs = TRUE)
```

Table \@ref(tab:nonresp-reasons) is shows a non-exhaustive list of reasons for item non-response. In a single survey instance, more than one such reason could be at play for different items. These different underlying models have hallmark patterns that researchers can observe to try to determine the mechanisms behind missingness in data. As always, routine exploratory analysis is the best starting point. Table \@ref(tab:nonresp-patterns) and Figures \@ref(fig:univmissingness) to \@ref(fig:stdgmissingness) are adapted from @kamakura2000factor to illustrate patterns of both unintentional (1-3) and intentional (4-6) non-response in surveys. These also demonstrate the usefulness of the missing values map, @unwin1996interactive. Here a simple, exploratory plot is made of a *missing indicator matrix* which is a binary matrix representing presence or absence of a value for each respondent and item. The exact format differs according to taste. @he2010missing recommends using a one to represent a missing value and a zero for present values as this reflects success of a binomial process for the non-response. However, @gelman2006data put forward cases where the reverse makes sense. This choice is somewhat trivial, so long as the research method is implemented with clarity.

```{r univmissingness, fig.height=2, fig.width=2, fig.cap="Missing Values Map showing univariate missingness"}
set.seed(101)
qnnaire <- matrix(rep(1, 2000), nrow = 100, ncol = 20)

umsng <- rbinom(100, 1, 1 - 0.2)
univ <- qnnaire
univ[, 13] <- umsng

par(mar = c(1, 1, 1, 1)
    , mgp = c(0, 0, 0)
    , cex.main = 0.8)

image(t(univ)
      , main = "Univariate Pattern"
      , xlab = "Item"
      , ylab = "Respondent"
      , xaxt = "n"
      , yaxt = "n"
      , col = c("ivory", "darkslateblue"))
```

```{r multmissingness, fig.height=2, fig.width=2, fig.cap="Missing Values Map showing multivariate missingness"}
mult <- matrix(rbinom(2000, 1, 1 - 0.05), nrow = 100, ncol = 20)

par(mar = c(1, 1, 1, 1)
    , mgp = c(0, 0, 0)
    , cex.main = 0.8)
  
image(t(mult)
      , main = "Multivariate Pattern"
      , xlab = "Item"
      , ylab = "Respondent"
      , xaxt = "n"
      , yaxt = "n"
      , col = c("ivory", "darkslateblue"))
```

```{r mtailmissingness, fig.height=2, fig.width=2, fig.cap="Missing Values Map showing ragged tail missingness"}
mult <- matrix(rbinom(2000, 1, 1 - 0.05), nrow = 100, ncol = 20)

mtail <- qnnaire
mtmsng <- rbinom(100, 1, 1 - 0.1)
mtrow <- which(mtmsng == 0)
mtcount <- 100 - sum(mtmsng)
mtcol <- round(runif(mtcount, 15, 20))

for (i in seq_along(mtrow)) {
  mtail[mtrow[i], mtcol[i]:20] <- 0  
}

par(mar = c(1, 1, 1, 1)
    , mgp = c(0, 0, 0)
    , cex.main = 0.8)
  
image(t(mtail)
      , main = "Ragged Tail Pattern"
      , xlab = "Item"
      , ylab = "Respondent"
      , xaxt = "n"
      , yaxt = "n"
      , col = c("ivory", "darkslateblue"))
```

```{r dfusmissingness, fig.height=2, fig.width=2, fig.cap="Missing Values Map showing data fusion missingness"}
dfus <- qnnaire
dfus[1:50, 11:15] <- 0
dfus[51:100, 6:10] <- 0

par(mar = c(1, 1, 1, 1)
    , mgp = c(0, 0, 0)
    , cex.main = 0.8)

image(t(dfus)
      , main = "Data Fusion Pattern"
      , xlab = "Item"
      , ylab = "Respondent"
      , xaxt = "n"
      , yaxt = "n"
      , col = c("ivory", "darkslateblue"))
```

```{r sbsmmissingness, fig.height=2, fig.width=2, fig.cap="Missing Values Map showing subsample missingness"}
sbsm <- qnnaire
sbsm[26:100, 15:20] <- 0

par(mar = c(1, 1, 1, 1)
    , mgp = c(0, 0, 0)
    , cex.main = 0.8)

image(t(sbsm)
      , main = "Subsample Pattern"
      , xlab = "Item"
      , ylab = "Respondent"
      , xaxt = "n"
      , yaxt = "n"
      , col = c("ivory", "darkslateblue"))
```

```{r stdgmissingness, fig.height=2, fig.width=2, fig.cap="Missing Values Map showing structural design missingness"}
stdg <- qnnaire
stdg_skip1 <- which(rbinom(100, 1, 1 - 0.1) == 0)
stdg_skip2 <- which(rbinom(100, 1, 1 - 0.2) == 0)
stdg[stdg_skip1, 5:6] <- 0
stdg[stdg_skip2, 10:13] <- 0

par(mar = c(1, 1, 1, 1)
    , mgp = c(0, 0, 0)
    , cex.main = 0.8)

image(t(stdg)
      , main = "Structural Pattern"
      , xlab = "Item"
      , ylab = "Respondent"
      , xaxt = "n"
      , yaxt = "n"
      , col = c("ivory", "darkslateblue"))
```

@joreskog2005structural provides an alternative missing data map in a collapsed form showing all possible patterns. Here, a missing indicator matrix with a 1 indicating missingness is used. Note that eleven cases had five or more out of six responses missing and would be useless in any analysis. This format is no use for more than a handful of items as more can lead to a combinatorial explosion which would not easily be analysed by hand. In fact, in the article a subset of just six items is used, which represent the items of a single multiple-item response variable. Another question to consider is what sorting criteria to use to present the information. The layout shown in Table \@ref(tab:missmapclpse) has been reproduced exactly as it is in the original article and the significance of the ordering of response patterns is neither obvious nor discussed.

```{r missmapclpse}
Frequency <- c(1554, 16, 12, 1, 4, 11, 31, 1, 2, 1, 4, 1, 32, 1, 1, 1, 5, 2, 1, 1, 14, 4, 4, 2, 1, 1, 2, 9)
Pattern <- c("000000"
             , "100000"
             , "010000"
             , "110000"
             , "001000"
             , "000100"
             , "000010"
             , "010010"
             , "110010"
             , "011010"
             , "000110"
             , "001110"
             , "000001"
             , "010001"
             , "110001"
             , "101001"
             , "000101"
             , "100101"
             , "001101"
             , "101101"
             , "000011"
             , "100011"
             , "010011"
             , "110011"
             , "011011"
             , "000111"
             , "011111"
             , "111111")
missingtable <- data.frame(Frequency = Frequency, Pattern = Pattern)
knitr::kable(missingtable
             , caption = "Missing data map, compact form. Source: Jöreskog (2009)"
             , booktabs = TRUE)

```

@carpita2011imputation provides an excellent formalization of useful measures and descriptors for missingness in a Likert based dataset, which are adapted and summarized as follows:

If the dataset $X$ is an $n \times p$ matrix and:

\begin{equation}
  X = \{x_{ij} :\ i \in \{1,\ 2,\ \dots,\ n\}\ ,\ j \in 1,\ 2,\ \dots,\ p\}\ \}
  (\#eq:carpitaX)
\end{equation}
then $M$, the missing indicator matrix, is also an $n \times p$ matrix and

\begin{equation}
  M = \{m_{ij} :\ i \in \{1,\ 2,\ \dots,\ n\}\ ,\ j \in 1,\ 2,\ \dots,\ p\}\ \}
  (\#eq:carpitaM)
\end{equation}
The respondent $i$ has no missing data when $m_{i+} = \sum^p_{j=1} m_{ij} = 0$ and data is missing when $m_{i+} = \sum^p_{j=1} m_{ij} > 0$. The items for respondent $i$ where data are not missing are counted as $p_{i+} = p - m_{i+}$ and their indices are collected in the set $A(i)$.

Likewise, the item $j$ has no missing data when $m_{+j} = \sum^n_{i=1} m_{ij} = 0$ and data is missing when $m_{+j} = \sum^n_{i=1} m_{ij} > 0$. There are $n_{+j} = n - m_{+j}$ respondents with observed data and the indices of these are collected for item $j$ in set $B(j)$

Donald B. Rubin is widely cited for seminal works, including @rubin1987multiple, on modeling, recovering (imputing) and inference using missing data. Of particular importance in this body of work is his formalization of the relationship between the missing values, the incomplete variable and the other variables. The patterns arising from these relationships are categorized as:

* Missing completely at random (MCAR) if process that causes the missing data is distinct from any parameter $\theta$ of the data. In other words the distribution of missing data is unrelated to the value of any variable in the dataset. It is also suggested in @gelman2006data that the probability of non-response is the same for all points of the missing indicator matrix. The missing/non-missing state of individual data points cannot be predicted from any information whatever, whether observed or unobserved other than the overall missingness rate. MCAR can be defined as:

\begin{equation}
  P(M \mid X) = P(M)
  (\#eq:MCAR)
\end{equation}

* Missing at random (MAR) is the most common scenario according to @schafer1997analysis **secondary citation** and applies if the distribution of missing data may be related to information available in other variables in the dataset but not the variable itself. The mechanism is correlated with or conditional on the observed data according to @gelman2006data, and it is often reasonable to model the missing indicator matrix as a logistic regression. The resulting probabilities give the propensity score for the point $x_{ij}$ to be missing. MAR can be defined as:

\begin{equation}
  P(M \mid X) = P(M \mid X^{obs}),\ X^{obs} = \{x_{ij} : m_{ij} = 0\}
  (\#eq:MAR)
\end{equation}

* Missing not at random (MNAR) if the distribution of missing data is related to data that is unobserved. @gelman2006data further sub-divides this category in two.
    * When missingness depends on unobserved predictors. For example, if dissatisfied customers are less likely to respond than satisfied customers, this would generate MNAR unit non-response. Another example might be a survey where people in a particular education bracket responded less frequently to questions about income but there were no questions identifying the level of educational attainment.
    * Where the distribution of missing data is related to the value of the partially missing variable itself, such that answers given by complete respondents differ substantially from answers that would have been given by non-respondents. For example, if respondents from a lower income bracket are reluctant to respond to questions about earnings.

Unobserved predictors defined as:
\begin{equation}
  P(M \mid X) = P(M \mid Z),\ Z \cap X = \varnothing,\ Z \text{are unobserved}
  (\#eq:MNARZ)
\end{equation}

Dependant on missing data:
\begin{equation}
P(M \mid X) = P(M \mid X^{mis}),\ X^{mis} = \{x_{ij} : m_{ij} = 1\}
  (\#eq:MNARX)
\end{equation}

MNAR is the most challenging situation to recover and of most concern to researchers. @jonsson2004evaluation explains that it is difficult to model a mechanism on data that is unobserved. @leite2010performance [p.73] goes further, asserting that "missing data methods cannot handle MNAR data" and @tsikriktsis2005review states that there are no statistical means to recover when missing data is MNAR and no way to ameliorate the potential bias in results based on analysis of such data. While it is not explicit, both must be referring to the second sub-division. @gelman2006data contradicts this assertion by suggesting that the problem can be mitigated by adding more predictors. The idea is to make the MAR assumption more tenable by adding more information. However, this advice assumes that more data is available. In fact, it is most likely that researchers would already provide all the data they have as it is widely recommended to use a more general model for imputation than the analytical model that follows. Perhaps more importantly than this, the advice does not actually address the MNAR limitation. Rather it tries to shift it into a different category. It would be of interest to discover methods that genuinely overcome this barrier.

In a historic article, @rubin1976inference cited in @wu2015comparison lays the foundation for these definitions, which are now used throughout the literature, and determines that ignoring the mechanism of non-response is the proper procedure when, and only when, the data is MCAR. Missingness in MAR cases is "ignorable" in regression models only when controlling for all the variables that correlate with non-response. This is an important milestone in statistics discourse and after this time, references to these assumptions and the terms "ignorable" and "not ignorable" (with respect to non-response) are seen in results analysis generally. Furthermore, @gelman2006data also suggests that most missingness is not MCAR. This means that most missingness is not ignorable (as the MAR case must at least be confirmed and controlled for before recovery). This is a crucially important point for any statistical analysis. It means that it is vital to make clear statements of assumptions made about the mechanisms of missingness found in data, and its treatment prior to evaluating results. @plumpton2016multiple reports that there is still a gap in practice with what is recommended in the literature. 66% of papers surveyed in psychology journals found an implication that missing data was present but no mention of how it was handled. 45% of papers surveyed were using complete case analysis which is riddled with counter-indications and only 8% using multiple imputation (MI), which is the state of the art for recovery from missing data problems.

## Current Approaches

### Techniques for Recovering from Missingness

> "It is the duty of researchers and analysts to firstly minimise the extent of missing data by ensuring appropriate methods for enhancing data capture are implemented, but also to handle missingness in a way best suited to the data and research question."

@plumpton2016multiple [p.13]

The author is unequivocal on the first point, and while it is stating the obvious that prevention is always better than cure, it does bear repeating. The second point provides support for the motivation behind this research. Analytical and processing techniques should be carefully chosen to meet the challenges of a given situation. The prevalence of surveys which collect mostly categorical data calls for methods better suited to data with these characteristics. 

The problems with missing data are many and of great concern to researchers. There technical problems widely discussed in the literature and best summed up as follows:

> "Inadequate handling of the missing data in a statistical analysis can lead to biased and/or inefficient estimates of parameters such as means or regression coefficients, and biased standard errors resulting in incorrect confidence intervals and significance tests."

@white2011multiple [p.377]

There is an abundance of examples in the literature of the adverse effects of missingness in survey data. These include biased parameter estimates (central tendency, dispersion and correlation coefficients) and loss of statistical power [@madow1983incomplete; @bean1995long; @roth1999missing; @raaijmakers1999effectiveness]. **Check if these discuss the recovery method used** **these are all secondary citations - check**

@white2011multiple quoted  Inadequate handling of the missing data in a statistical analysis can lead to biased and/or inefficient estimates of parameters such as means or regression coefficients, and biased standard errors resulting in incorrect confidence intervals and significance tests. In all statistical analyses, some assumptions are made about the missing data. 

The problems are reputational and a nuisance as well as technical. @plumpton2016multiple also reports that "poor handling and reporting of missing data may result in misleading conclusions and are one of the main reasons for publication rejections." @gelman2006data provides a very comprehensive treatment of the topic which serves as the basis for much of this section. The authors describe non-response in a social indicators survey as "a distraction to our main goal of studying trends in attitudes and economic conditions, and we would like to simply clean the dataset so it could be analyzed as if there were no missingness" @gelman2006data [p.529]. This  statement perfectly sums up the motivation for missing data analysis and recovery per se and illustrates why many techniques have been developed. Researchers want to analyze their data with confidence and statistical rigour, despite the prevalence of non-response.

Many statistical functions and operations simply do not work with missing data, whether calculated manually or by software. Many software systems (e.g. R, SAS, SPSS), sometimes with a warning and sometimes silently, will perform a list-wise deletion to automatically exclude cases with *any* missing values in the variables and analyze only the complete cases. If there is only one missing value out of tens or hundreds of potential predictor variables, then much valuable information is lost. Even in the era of "Big Data," where TeraBytes may be trawled for accidental insight, a well-designed survey may still have a high cost per respondent due to the discipline of sampling and selection. Rather than simply allowing statistical software to make default, often inappropriate choices, it is advisable to carry out a formal missing data analysis. Then, based on these findings, to implement some other form of pre-processing that recovers a dataset. The method chosen will have a significant impact on the reliability of any subsequent analysis. 

@carpita2011imputation describes four categories of recovery procedures as follows:

1. Ignore or omit
1. Weighting procedures
1. Model-based procedures
1. Imputation procedures

Each method has advantages and disadvantages. For example, (1) includes list-wise deletion which is simplest to implement but potentially introduces significant problems. @bono2007missing sums up advice which is well-supported throughout the literature; Although list-wise deletion is a default option, it is only appropriate for data which is MCAR. If this assumption does not hold, @plumpton2016multiple states that the complete case sample will not be representative of the underlying population. Also, the proportion of missing data must be small. However, this instruction will not hold if the dataset is wide (having many variables) because the distribution of missing data among the variables leads to geometric losses with increasing number of variables. @he2010missing reports that that for missingness between 1%-5% over only 20 variables, the proportion of discarded rows is $\approx 36\%$. In fact the expected loss for 5% MCAR data in all 20 variables would be $1-(1-0.05)^{20} \approx 64\%$ discarded cases.

@plumpton2016multiple imposes a further condition; that the missingness be found only in the dependent variable. Although this was not specifically mentioned in any of the other texts reviewed it certainly caps a plethora of significant limitations with the deletion method. The only article found to encouraged list-wise deletion under specific circumstances other than those listed was @joreskog2005structural, which offers the common sense advice that list-wise deletion is recommended for cases with all or most responses missing. Obviously these individual data rows would add little value to any statistical model. In practice, the MCAR conditions themselves are rarely provable, and even if they are, the consequence of data loss is loss of statistical power from a smaller smaple size which is highly undesirable.

Category (1) also includes simply omitting a data point from an aggregate or multiple-item scale. @carpita2011imputation mentions that this will result in an underestimated score on the latent variable. This seems like a very ill-thought out approach and would probably never be recommended in up to date research methodologies. @wu2015comparison reports that some researchers average over the remaining variables and does not recommend doing so as this will still result in bias.

Methods in category (2) can be applied after list-wise deletion to rebalance a sample and can also be used to deal with unit non-response. Essentially remaining cases are weighted or duplicated so the desired sampling and stratification proportions are restored. @he2010missing explains that this suffers the same problem as complete case analysis, in that partial data from incomplete cases is discarded and also that extreme correction will lead to erratic variance estimates. @groves2011three points out that the theoretical assumptions for this method to be valid could never be met and both @he2010missing and @gelman2006data posit that weighting becomes too complicated if more than one variable contains non-response. Indeed support for these older methods is either thin or non-existent in the contemporary literature. 

A modification of (2) is called available case analysis. It uses subsampling of the complete cases to answer different research questions. This has the problem that analyses may be based on inconsistent subsamples, representing potentially different populations. Given all the shortcomings associated with complete case analysis and following the emergence of ubiquitous computing power to run more sophisticated methods, it is not surprising they have fallen out of favour. Nevertheless, some authors [@plumpton2016multiple] **others?** suggest that routine applications of these methods persist.

@carpita2011imputation introduces category (3) but only in passing and gives Expectation Maximization (EM) and data augmentations (the Bayesian counterpart of EM) as examples. @gelman2006data [p.540] also only briefly describes (3) as a process in which a "multivariate model is required, and this can be particularly tricky when some of the variables are discrete" which also implies EM, but then unexpectedly associates this with non-ignorable missing data models, in which domain knowledge is used to model the mechanism of missingess. This latter point is somewhat inconsistent. EM algorithms form the basis of multiple imputation techniques which will be discussed shortly, so perhaps this rather brief treatment of (3) indicates that it is a foundational concept for a more sophisticated technique. The remainder of this subsection with deal with (4).

@bono2007missing explains that imputation strategies replace missing data points with substitute values that are estimated from patterns or relationships found in the observed data. This allows the full sample to be used for analysis. Many methods are well developed for continuous variables, but sources indicate that there has been much less research for datasets comprising categorical and ordinal variables, [@wu2015comparison; @finch2010imputation; @leite2010performance]. The latter give the mitigation that might be that state of the art techniques depend on specialised software but at the time of writing, most of the statistical software available today was already available. Additionally, this literature review finds that it continues to be the case. The state of the art techniques required additional configuration, transformation of variables and return results that are out of the natural variable scale, such as real, non-integer and/or non-positive values for ordinal variables.

Furthermore, there is strong evidence of differences in the performance of various imputation strategies over datasets exhibiting different underlying characteristics [@wu2015comparison; @sim2015missing; @rodwell2014comparison; @bono2007missing]. These differences were significant in real world experiments as well as with simulated data. In some cases, biased parameter estimates and incorrect confidence intervals, would have affected the choice of predictors included into a statistical model. The model and magnitude of missingness as well as other characteristics of the data were all found to determine the best performing strategy. For this reason there is real benefit in having a range of imputation methods from which to choose the most suitable in a given situation. **anything wrong with those experiments?**

There are many different kinds of imputation strategies and different ways to categorize them:

* Single (or point) versus Multiple Imputation
* Parametric versus Non-parametric Imputation
* Deterministic processes versus Non-deterministic processes

Of these, the most important distinction is probably Single versus Multiple.  

### Single Imputation

Single imputation strategies are often quick and simple to implement but lead to biased parameter estimates. This can happen for two reasons. Firstly, as @azur2011multiple explains, the analysis proceeds as if the simulated value is a true, observed value, not taking any uncertainty into account. Secondly, the choice of substitute value can affect the statistical properties of the imputed variable. Consider a univariate normal scenario where all missing values are replaced with the mean of the observed data. While this action preserves the mean of the random variable, the variance is depressed. This is demonstrated by way of a Monte Carlo simulation, following the methodology given in @robert2009introducing:

```{r sim_grandmean_imp, cache=TRUE}
set.seed(1)
n <- 1000
m <- 100
p <- 0.15
k <- 4
varx <- numeric(n)
varxmis <- numeric(n)
varximp <- numeric(n)
meanx <- numeric(n)
meanxmis <- numeric(n)
meanximp <- numeric(n)

x <- rnorm(m, mean = 10, sd = 2)

d <- list()
for (i in 1:n) {
  x_star <- sample(x, m, replace = TRUE)
  
  nonresp <- rbinom(m, size = 1, prob = p)
  x_mis <- ifelse(nonresp == 1
                  , NA
                  , x_star)
  x_imp <- ifelse(is.na(x_mis)
                  , mean(x_mis, na.rm = TRUE)
                  , x_mis)
  if ((i / n * k) %in% 1:k) {
    d[[i / n * k]] <- densityplot(~x_star+x_mis+x_imp
                                  , plot.points = FALSE
                                  , main = "Density before and after mean imputation"
                                  , xlab = "Bootstrap samples 250, 500, 750 and 1000"
                                  , auto.key = list(columns = 2)
                                  , par.settings = MyLatticeTheme
                                  , scales = MyLatticeScale)  
  }
  
  meanx[i] <- mean(x_star)
  meanxmis[i] <- mean(x_mis, na.rm = TRUE)
  meanximp[i] <- mean(x_imp)  
  varx[i] <- var(x_star)
  varxmis[i] <- var(x_mis, na.rm = TRUE)
  varximp[i] <- var(x_imp)
}

function1 <- mean
function2 <- function(x) {
  quantile(x, c(0.025, 0.975))
}

mcmc_results <- function(x) {
  t(rbind(
    data.frame(
      t(sapply(x, function1))
      , row.names = "Emp.mean")
    , sapply(x, function2)
  )[c(2, 1, 3),])
}

means <- list(sim_mean = meanx
              , mis_mean = meanxmis
              , imp_mean = meanximp)
vars <- list(sim_var = varx
     , mis_var = varxmis
     , imp_var = varximp)
```

1. A random variable $X = x_1...x_{100},\ X \sim \mathcal{N}(10, 2)$ is generated.
1. $N =$ `r n` bootstrap samples $X^*$ are taken.
1. For each $X^*$, $X^*_{mis}$ is generated by setting `r p * 100`% of values to missing by a coin flip (Bernoulli) function.
1. $X^*_{imp}$ is generated by replacing missing values with the mean of $X^*_{mis}$.
1. The empirical means and variances of $X^*$, $X^*_{mis}$ and $X^*_{imp}$ are returned with their 95% credible intervals.


```{r sim-grandmean-imp-means}
knitr::kable(mcmc_results(means)
            , caption = "Empirical mean of X, X with missing data and imputed X"
            , booktabs = TRUE)
```
```{r sim-grandmean-imp-vars}
knitr::kable(mcmc_results(vars)
            , caption = "Empirical variance of X, X with missing data and imputed X"
            , booktabs = TRUE)
```
```{r sim-grandmean-imp-plot, fig.cap="Monte Carlo simulation of biased variance estimate after mean imputation. Note how the imputed distribution are leptokurtic, with more of the density concetrated in the centre."}
c(d[[1]], d[[2]], d[[3]], d[[4]])
```

This informative example shows how the mean and variance of the MCAR data is almost identical to the underlying distribution. Using mean imputation gives a mean which is identical to the MCAR data to many decimal places, as can be seen in Table \@ref(tab:sim-grandmean-imp-means). However, the variance is much reduced in the imputed dataset, Table \@ref(tab:sim-grandmean-imp-vars). Density plots of selected bootstrap iterations show very clearly what has happened. See Figure \@ref(fig:sim-grandmean-imp-plot). Not shown here, but also notable, is that bivariate correlations would also be artificially reduced by using a constant value to replace missing data.

Mean imputation, or any other technique that relies only on a measure of central tendency is too simplistic an approach for many missing data problems. Aside from the issues demonstrated here, the method takes no information from other variables if they are available. If data are MAR, then a dependency between the missingness and the observed data has been established and should be exploited. Other simple, parametric imputation methods that do so include group mean imputation, where the mean stratified over one of more categorical variable is used. Regression imputation is better still, generating estimated values from a predictive model (usually linear, or whatever is appropriate for the data).

```{r sim_predmean_imp, cache=TRUE}
set.seed(2)
n <- 1000
m <- 250
p <- 0.20
k <- 4
corxy <- numeric(n)
corxyimp <- numeric(n)

x <- rnorm(m, mean = 10, sd = 2)
eps <- rnorm(m, mean = 0, sd = 1)
y <- 3 + x * 0.5 + eps

fitxy <- lm(y ~ x)
errs <- fitxy$residuals

MyTempTheme <- MyLatticeTheme
MyTempTheme$superpose.symbol$col <- myPalContrasts[c(5, 3)]

d <- list()
for (i in 1:n) {
  y_star <- y + sample(errs
                      , size = m
                      , replace = TRUE)
  corxy[i] <- cor(x, y_star)
  
  nonresp <- rbinom(m, size = 1, prob = p)

  df <- data.frame(y = y_star[nonresp == 0]
                 , x = x[nonresp == 0])
  fit_star1 <- lm(y ~ x, data = df)
  y_imp <- y_star
  y_pred <- predict(fit_star1
                    , newdata =
                      data.frame(x =
                                   x[nonresp == 1]))
  y_imp[nonresp == 1] <- y_pred
  corxyimp[i] <- cor(x, y_imp)
  
  if ((i / n * k) %in% 1:k) {
    d[[i / n * k]] <- 
      xyplot(y_star + y_imp ~ x
             , pch = c(1, 19)
             , cex = c(0.8, 0.6)
             , alpha = c(1, 0.3)
             , panel = function(x, y, ...) {
               panel.xyplot(x, y, ...)
               panel.xyplot(x[nonresp == 1]
                           , y_pred)
             }
             , main = "Scatter plot before and after regression imputation"
             , xlab = "Bootstrap samples 250, 500, 750 and 1000"
             , ylab = "Bootstrapped and Imputed Y"
             , auto.key = list(columns = 2)
             , par.settings = MyLatticeTheme
             , scales = MyLatticeScale)  
  }
}
corrs <- list(sim_corr = corxy
              , imp_corr = corxyimp)
```

```{r sim-predmean-imp-corrs}
knitr::kable(mcmc_results(corrs)
            , caption = "Regression imputation: Empirical correlation of X with Y and imputed Y"
            , booktabs = TRUE)
```
```{r sim-predmean-imp-plot, fig.cap="Monte Carlo simulation of regression imputation. Original values in open circles, fuchsia colour. The imputed dataset shares most of the same values in filled circles, green colour, de-emphasised with transparency. Predicted values have been emphasised in darker green. Note how they all fall on the regression line."}
c(d[[1]], d[[2]], d[[3]], d[[4]])
```

Again, it is very informative to run a simulation of the regression imputation technique to understand its shortcomings. Following again the methodology given in @robert2009introducing:

1. A random variable $X = x_1...x_{250},\ X \sim \mathcal{N}(10, 2)$ is generated.
1. A linearly dependent variable $Y$ is generated using the linear equation $Y = \beta_0 + \beta_1X + e, \ \beta_0 = 3, \ \beta_1 = 0.5, e \sim \mathcal{N}(0, 1)$
1. An OLS regression is performed on $Y \sim X$ and the residuals are saved as $err$.
1. $N =$ `r n` samples $Y^*$ are generated by bootstrapping $err$ and adding the result to $Y$.
1. For each $Y^*$, $Y^*_{mis}$ is generated by setting `r p * 100`% of values to missing by a coin flip (Bernoulli) function.
1. A new OLS regression is performed on $Y^*_{mis} \sim X,\ Y^*_{mis_i} \in Y^*_i$
1. $Y^*_{imp}$ is generated by predicting new values from $X$ using $X_i,\ Y^*_{mis_i} \notin Y^*_i$
1. The empirical correlations $Cor(X, Y^*)$ and $Cor(X, Y^*_{imp})$ are returned with their 95% credible intervals.

The results in Table \@ref(tab:sim-predmean-imp-corrs) show a systematic over-estimate of bivariate correlation because all the imputed values fall directly on the regression line. See Figure \@ref(fig:sim-predmean-imp-plot).

The examples serve to demonstrate why imputation is quite a different process to prediction. The purpose of imputation is not to achieve a measure of accuracy or smoothing, but instead to faithfully represent a distribution. As such, non-deterministic methods that estimate and draw from a posterior distribution of the parameters of the observed data have grown in popularity. The most basic of these is simple (naive) random imputation, whereby imputed values are generated by sampling with replacement from the observed data. Clearly, this method ignores any useful information from the other variables, which is undesirable. It can be improved by taking the imputed values from a bootstrapped sample of the observed data, rather than directly from the observed data. This method is known as the Approximate Bayesian Bootstrap (ABB). 

To make better use of information from other variables, methods have been developed that that do take into account information from all available variables. For example, ABB can be extended by classifying instances into different groups by their propensity score. The Approximate Bayesian Bootstrap with Propensity (ABP) method draws substitute values from within these classes. A different approach, described in @gelman2006data [p.536] is random regression imputation. This method starts with a deterministic regression imputation and adds a random error term from a normal distribution $\mathcal{N}(0, SE_{lm})$ where $SE_{lm}$ is the residual standard error from the linear model. It might also be valid to add a bootstrapped sample of the residual errors to the linear prediction.

```{r sim_randpred_imp, cache=TRUE}
d <- list()
for (i in 1:n) {
  y_star <- y + sample(errs
                      , size = m
                      , replace = TRUE)
  corxy[i] <- cor(x, y_star)
  
  nonresp <- rbinom(m, size = 1, prob = p)

  df <- data.frame(y = y_star[nonresp == 0]
                   , x = x[nonresp == 0])
  fit_star <- lm(y ~ x, data = df)
  sm_fit <- summary(fit_star)
  y_imp <- y_star
  y_pred <- predict(fit_star
                    , newdata =
                      data.frame(x =
                                   x[nonresp == 1])) +
    # random component
    rnorm(sum(nonresp)
          , mean = 0
          , sd = sm_fit$sigma)
  y_imp[nonresp == 1] <- y_pred
  corxyimp[i] <- cor(x, y_imp)
  
  if ((i / n * k) %in% 1:k) {
    d[[i / n * k]] <- 
      xyplot(y_star + y_imp ~ x
             , pch = c(1, 19)
             , cex = c(0.8, 0.6)
             , alpha = c(1, 0.3)
             , panel = function(x, y, ...) {
               panel.xyplot(x, y, ...)
               panel.xyplot(x[nonresp == 1]
                           , y_pred)
             }
             , main = "Scatter plot before and after random regression imputation"
             , xlab = "Bootstrap samples 250, 500, 750 and 1000"
             , ylab = "Bootstrapped and Imputed Y"
             , auto.key = list(columns = 2)
             , par.settings = MyLatticeTheme
             , scales = MyLatticeScale)  
  }
}
corrs <- list(sim_corr = corxy
              , imp_corr = corxyimp)
```

```{r sim-randpred-imp-corrs}
knitr::kable(mcmc_results(corrs)
            , caption = "Random Regression imputation: Empirical correlation of X with Y and imputed Y"
            , booktabs = TRUE)
```
```{r sim-randpred-imp-plot, fig.cap="Monte Carlo simulation of random regression imputation. Original values in open circles, fuchsia colour. The imputed dataset shares most of the same values in filled circles, green colour, de-emphasised with transparency. Predicted values have been emphasised in darker green. Note how they faithfully replicate the original distribution"}
c(d[[1]], d[[2]], d[[3]], d[[4]])
```

**can try to do those bayesian graphs with HDI intervals highlighted**

As can be seen from Table \@ref(tab:sim-randpred-imp-corrs) and Figure \@ref(fig:sim-randpred-imp-plot), the distribution of the imputed variable has been better preserved by this imputation process. The credible interval for the coefficient of bivariate correlation is also wider which is a desirable property as it reflects that the imputed variables have added some uncertainty. However, this is only apparent because of the Monte-Carlo simulation and would not be detected after a single imputation process. This is where multiple imputation adds value and will be discussed shortly. It is clear that wholly deterministic processes have undersirable properties for imputation and it highlights a very important issue for the current research. Association rules mining is deterministic in nature when carried out under normal circumstances. Some consideration will need to be given to the impact this might have on the proposed novel technique.

The random regression imputation procedure can be extended to a multivariate scenario, where data is missing in more than one variable by setting up a joint multi-variate normal model. @azur2011multiple states that this is rarely appropriate for wide datasets with possibly hundreds of predictor variables. @gelman2006data describes it as a complicated task better left to software supports its automation and instead suggests an "iterative regression imputation" @gelman2006data [p.539] but goes on to describe what other authors call the chained equations algotrithm. In the text, it is used to generate a single point estimate for each missing data point. However, this iterative process also underpins multiple imputation with chained equations (MICE), as described by most authors. In the next section MICE will be explored in more detail.

These methods become more and more complex when modeling data which is not multivariate normal. Such automated models must be checked for convergence and validity before the results can be used in further analysis. The authors of @gelman2006data go on to say that diagnostic tools for multivariate imputation models are an area that requires further research because there was nothing analogous to residual plots and tests at the time of publication. Subsequently, this is addressed in R package *mi*, [@R-mi; @su2011multiple], in which the same authors are involved.

Further to the formalization of measures of missingness in a dataset, @carpita2011imputation continues by defining a set of imputation strategies reported to be particularly effective for Likert scales. These are especially useful for the current research. The following are adapted from the same article and the specific case for Likert scales is given as a prelude to the definitions of the strategies themselves:

**can use equation refs to say what has been taken from other articles**
**these are from secondary citations**

Recalling the data matrix $X$ and its corresponding missing indicator matrix $M$, let $K$ be the number of ordinal categories in each item, then the value of $x_{ij}$, respondent $i$'s answer to item $j$, is: 

\begin{equation}
  x_{ij} =
  \begin{cases} 
  k \in \{1,\ 2, \ \dots, \ K\ \},\ \text{if}\ m_{ij} = 0 \\
  \text{missing}, \text{otherwise}
  \end{cases}, \ m_{ij} \in \{0,\ 1\}
  (\#eq:likert)
\end{equation}

The person mean is:
\begin{equation}
  \bar{x_{i+}} = \frac{\sum_{j \in A(i)}x_{ij}}{p_{i+}}
  (\#eq:personmean)
\end{equation}

The item mean is:
\begin{equation}
  \bar{x_{+j}} = \frac{\sum_{i \in B(j)}x_{ij}}{n_{+j}}
  (\#eq:itemmean)
\end{equation}


The person item mean is the mean of item means for each observed item for respondent $i$:
\begin{equation}
  \bar{\bar{x_{i+}}} = \frac{\sum_{j \in A(i)}\bar{x_{+j}}}{p_{i+}}
  (\#eq:personitemmean)
\end{equation}

Note that the total of observed data is:
\begin{equation}
  n_{++} = \sum^n_{i=1}p_{i+} = p_{++} = \sum^p_{j=1}n_{+j}
  (\#eq:obsdata)
\end{equation}

and the total mean is:
\begin{equation}
  \bar{x_{++}} = \frac{\sum^n_{i=1}\bar{x_{i+}}p_{i+}}{p_{++}} = \frac{\sum^p_{j=1}\bar{x_{+j}}n_{+j}}{n_{++}}
  (\#eq:totalmean)
\end{equation}

With these definitions for the special case of Likert scale data and in addtion, let $[w]$ be the integer part of $w$ and $[\circ]w = frac(w)$ is the fractional part of $w$, then various single imputation methods are defined as:

PM Person Mean imputation:
\begin{equation}
  \hat{x_{ij}}^{(PM)} = [\bar{x_{i+}}] + \epsilon_{i},\ \epsilon_{i} \sim B(1, [\circ]\bar{x_{i+}})
  (\#eq:pmimp)
\end{equation}

CIM Corrected Item Mean imputation:
\begin{equation}
  \hat{x_{ij}}^{(CIM)} = [\bar{x_{ij}}^{(CIM)}]  + \epsilon_{ij} = [\frac{\bar{x_{i+}}}{\bar{\bar{x_{i+}}}} \bar{x_{+j}}] + \epsilon_{ij},\ \epsilon_{ij} \sim B(1, [\circ]\bar{x_{ij}}^{(CIM)})
  (\#eq:cimimp)
\end{equation}

TW Two-Way Imputation:
\begin{equation}
  \hat{x_{ij}}^{(TW)} = [\bar{x_{ij}}^{(TW)}] + \epsilon_{ij} = [\bar{x_{i+}} + \bar{x_{+j}} - \bar{x_{++}}] + \epsilon_{ij},\ \epsilon_{ij} \sim B(1, [\circ]\bar{x_{ij}}^{(TW)})
  (\#eq:twimp)
\end{equation}


The use of the $\epsilon_{ij}$ term is a non-deterministic adaptation of the classical method which used ordinary rounding.

ICS Item Correlation Substitution
\begin{equation}
  \hat{x_{vi}}^{(IC)} = x_{ig},\ g = \underset{l \in A(i),\ l \ne j}{argmax}\ \rho_{jl}(i)
  (\#eq:icsimp)
\end{equation}

where $\rho_{jl}(i)$ is the correlation between between items $j, l$ and item $l$ is observed for subject $i$.

There are also non-parametric alternatives of single imputation. Many are best suited for specific situations. For example, "last value carried forward" is only applicable in longitudinal, or time-based surveys. It can result in erratic readings for trends analysis. For missing data in a nominal variable it is sometimes acceptable simply to add a new category, "missing." Logical rules, survey structure or domain knowledge can be applied. For example, a respondent reporting low or moderate drinking might have the number of alcohol units per week imputed if missing. Similarly, someone reporting they worked zero hours per week might be also assigned an income of zero. @wu2015comparison mentions the use of a random forest in the literature which is explored because it can be applied to all kinds of categorical data, requires no distributional assumption and can model multi-way interaction. It is implemented in R in the missForest package, @R-missForest. This is encouraging for the current project as is good evidence that such non-parametric approaches have gained some popularity.

The most important non-parametric imputation methods are generally referred to as "similar pattern response matching" (SPRM) [@joreskog2005structural p.3] and also Hot-deck imputation.  @gelman2006data seems to refer to Hot-deck and matching interchangeably, which seems reasonable given the descriptions of these elsewhere in the literature. @joreskog2005structural defines the intuition behind matching imputations as follows; If an individual $i$ with missing variable $j$ has same response pattern in other variables as another case $i'$, which is complete in $j$, then it is likely that variable $i$ would also match $i'$ in $j$. The article goes on to say that if multiple individuals are found with the matching response pattern then that's even stronger evidence for matching in $j$. @jonsson2004evaluation reports that nearest neighbour methods are popular in this category. These use similarity measures to find suitable donors among the cases. @gelman2006data [p.538] describes matching imputation as a "non-parametric version of regression" but it is not clear what the grounds are for this description. Certainly the method is useful where regression is challenging.

Some authors seem to differ in terms of a definition for SPRM. @bono2007missing [p.8] states that the substitute value "may be selected simpy at random or using an elaborate scheme." Similarly, @jonsson2004evaluation [p.108] states that the donors may simply be the complete cases in the dataset and that "there are different ways of picking a replacement value, for example by choosing a value from one of the donors by random or by calculating the mean of the corresponding values of the donors." Confusingly, this assertion is made just after making it clear that donor selection should depend on the case being imputed. The contradiction here is that both of these definitions suggest a valid "at random" process that would be no different to naive random imputation and would not use any information from the other cases, which is fundamental to the method described by other authors and, in the case of @jonsson2004evaluation, in the same paragraph. It is possible that both are refering to the original but now outdated definition of Hot-deck imputation, described shortly.

@bono2007missing quite rightly points out that the main disadvantages with the matching method are defining and programmatically configuring the customized lookup and selection process for each variable. For example, @carpita2011imputation proposes a non-parametric extension to the ABP method. In Approximate Bayesian Bootstrap with Propensity and Nearest Neighbour (ABPN), the donor cases are selected from the neighbourhood of each missing case, within the propensity classes. This hybrid method was shown to outperform all the benchmarks in a simulation experiment with semi-synthesised, Likert scale data. There must be a computational cost assiacted with this multi-step process but it is at least possible to envision an embarrassingly parallel architecture for its implementation. Furthermore, its non-determinstic nature makes it highly suited to multiple imputation.

It should be noted that Cold-deck imputation is also mentioned in the literature, but only as a brief aside [@kowarik2016imputation; @bono2007missing; @gelman2006data]. In this process, substitute values are found by reference to prior studies, not the same dataset where the missing values are found. The names refer to old punch-card computer systems. The cold-deck was a previous run, while the Hot-deck was the current run literally still hot out of the card-reader. Cold-deck imputation is barely perhaps because it is just a historical curiosity and not considered useful. This is not made clear in any of the sources. The literature generally focuses on modern statistical and computational techniques.

The physical, punch card based process would have been to sort the cards in a particular order and impute with the nearest available value. The computational evolution of Hot-deck imputation is to find donor cases using similarity scores between individual cases, using Euclidean distance, for example. In such cases, there would be an assumption that the matching variables would be numeric scales, where regression models presumably would also work and there would be a question as to why this method were chosen over one based on normal or multivariate normal assumptions. However, other similarity scores are available that suit categorical data better, and this is where the use of association rules may be fruitful.

Importantly, the sources show that missing values can remain after this method and should be removed by listwise deletion. This point corroborates what this current research is hoping to achieve. The intention of this research is to go a step further and find all the similarity patterns.

### Mulitple Imputation (MI)

@graham2007many states that MI and full information maximum likelihood (FIML) are the two most common techniques used with missing data, but FIML is far more computationally expensive. @wu2015comparison lists a number of other disadvantages of FIML. So, for the purpose of this research, only MI will be considered in this discussion of the state of the art. Different authors refer to EM, data augmentation and Gibbs sampling as the underpinnings of MI but @wu2015comparison argues that all these are special cases of MCMC, so it would be fair to say that MI is a MCMC process, whatever the specific implementation.

In MI, each missing value is replaced with several substitute values, referred to as "plausible values" in most descriptions of the method. This better reflects uncertainty about the model as a result of missingness in the data. Another feature of MI is a pair of assumptions: a MAR pattern and a multi-variate normal model for the full dataset $X$:

$$X \sim MVN(\Theta),\ \Theta = (\mu, \Sigma)$$
where $\mu$ is the vector of population means and $\Sigma$ is the covariance matrix.

From these two assumptions, the EM algorithm can be applied; @schafer1997analysis cited in @wu2015comparison describes this as a two step process. The posterior P-step and the imputation I-step. 

Let the dataset $X$ be comprised of observed data $X_{obs}$ and missing data $X_{mis}$. For the first iteration:

* P-step: Values for $\hat{\Theta^*}$ are estimated for $\Theta$ from a bootstrapped sample of $X_{obs}$. 

* I-step: Plausible values for $X_{mis}$ are drawn from $MVN(\hat{\Theta^*})$, the posterior predictive distribution.

On iteration $t$:

* P-step: $\hat{\Theta^t}$ is drawn from the posterior distribution of $\Theta$: $\hat{\Theta^t} \sim P(\Theta \mid X_{obs}, \ X^{t-1}_{mis})$ where $X^{t-1}_{mis}$ are the current imputed values from the previous iteration.

* I-step: Draw new values for $X^t_{mis}$ from the predictive distribution: $X^t_{mis} \sim P(X_{mis} \mid X_{obs}, \ \hat{\Theta^t})$

The model converges when $\Theta^t \approx \Theta^{t-1}$. At this point, $m$ completed datasets can be saved which will differ only in the substituted values. This is usually done at specified intervals to avoid auto-correlations between datasets. The name multiple imputation actually refers to these $m$ imputations. Unfortunately, the literature contains varying claims, referring sometimes to the iterative nature of EM algorithm, the multivariate model and the possibility of imputing more than one missing variable. Yet, neither would it be correct to refer to a different procedure that generated multiply imputed datasets as the MI algorithm originally developed by Donald Robin. It is easy to see why there is little consistency on this matter!

In the literature there can also be found many useful descriptions of MI. @white2011multiple [p.378] call this "proper imputation, because it incorporates all sources of variability and uncertainty in the imputed values, including prediction errors of the individual values and errors of estimation in the fitted coefficients of the imputation model." @leite2010performance [p.65] states that MI "restores two sources of variability: the variability of each variable and the variability of the sample covariance matrix." These description are most helpful to explain the major contribution of the method, namely that it delivers an appropriate level of uncertainty to any subsequent analysis by not treating any imputed value as if it were a true value.

After MI, any subsequent statistical analysis should be performed on each of the $m$ imputations and pooled. @graham2007many **more** reports that the resulting estimates are unbiased. Furthermore, the variance in estimates attributable to the missing data can also be easily estimated using Rubin's rules which are recapitulated widely in most sources that discuss MI in any detail. The following definitions are adapted from @graham2007many, @leite2010performance and @white2011multiple:

Let $\theta$ be a paremeter of interest, to be estimated from $m$ imputations and let $\hat{\theta_i}$ be the estimate of $\theta$ in the $i^{th}$ imputation. The point estimate $\hat{\theta}$ is simply the mean of the $m$ estimates:
\begin{equation}
  \hat{\theta} = \bar{\theta} = \frac{1}{m} \sum^m_{i = 1}\hat{\theta_i}
  (\#eq:miparamest)
\end{equation}

The within imputation variance of paramater $\theta$ is:
\begin{equation}
  W_{\theta} = \frac{1}{m} \sum^m_{i = 1} SE^2_{\hat{\theta_i}}
  (\#eq:miw)
\end{equation}

The between imputation variance of paramater $\theta$ is:
\begin{equation}
  B_{\theta} = \frac{1}{m-1} \sum^m_{i = 1} (\hat{\theta_i}-\bar{\theta})
  (\#eq:mib)
\end{equation}

The total imputation variance of paramater $\theta$ is:
\begin{equation}
  T_{\theta} = W_{\theta} + (1 + \frac{1}{m})B_\theta
  (\#eq:mib)
\end{equation}

which can be used to conduct Student's T test by converting to a standard error and adjusting degrees of freedom as shown here:
\begin{equation}
  SE_{\theta} = \sqrt{T_{\theta}},\ df = (m-1)(1+\frac{mW_{\theta}}{m+1}B_{\theta})^2
  (\#eq:mise)
\end{equation}

Another important formula in MI is the fraction of missing imformation (FMI). Although the proportion of missing data $\frac{count(D_{mis})}{count(D)}$ may be useful in the simplest of cases, it doesn't account for increased information available if variables are highly correlated. FMI is estimated for each parameter that has been estimated from MI, using the specific $W$ and $B$ for that parameter estimate. FMI is usually represented as $\gamma$
\begin{equation}
  \gamma = \frac{r+2/df + 3}{r+1},\ r = \frac{1 + \frac{1}{m}B}{W}
  (\#eq:fmi)
\end{equation}

@graham2007many explains how the FMI is used to determine the efficiency of the estimates which in turn informs the appropriate number for $m$ under experimental conditions. Efficiency is given as:
\begin{equation}
  (1+ \frac{\gamma}{m})^{-1}
  (\#eq:efficiency)
\end{equation}

Given the form and limits of the efficiency calculation, $Eff = 1,\ m \rightarrow \infty$, @rubin1987multiple [p.548-549] cited in @graham2007many stated that the "gains rapidly diminish after the first few imputations" and the implication is that there is little improvement to the MSE of parameter estimates with many imputations compared to just a few. This leads to the commonly repeated rule of thumb that $m$ = 3-5 is usually enough. Donald Rubin and Joseph Schafer are cited very widely in the literature and are conferred a great deal of authority on the subject of missing data and imputation. Nevertheless, there are some disagreements; @leite2010performance reports that $m$ = 5-10 is standard. It is not clear why there is this difference as the same secondary sources are cited. @wu2005role states that there is no support in the literature for the $m$ = 5 and that one imputation is often enough, but during this review it was the only article found to make the case that MI methods should be used for single imputation! A non-academic source was found which only used $m$ = 1 but this appeared to be a misunderstanding of the method. In stark contrast, @graham2007many demonstrates that $\gamma$ is poorly estimated for small values of $m$ using Monte-Carlo simulations and posits that 3-5 is nowhere near enough. In particular, they show that statistical power is very sensitive to small values of $m$. This is a crucial point because preserving statistical power is one of the major reasons for not using a simpler technique, such as list-wise deletion. @white2011multiple goes further, also using Monte-Carlo simulations, stating that results are not reproducible when $m$ is too low. The authors cite other experiments as well as their own work that show a clear link between confidence interval widths and the fraction of missing information in the dataset. As a result, they endorse the suggestion of 1 imputation per 1% of missing data or FMI. From the variety of experimental and formal evidence, it should be concluded that this is a valuable rule of thumb. At the time MI was developed, in the late 70's to early 80's, computing power and clock cycles would have come at a premium. With the power of modern computing today, it seems unnecessary to stick to a bare minimum number of runs for anything other than massive datasets with high incidence of missing data, @azur2011multiple. It should be fairly trivial to increase $m$ and there are no counter-indications for doing so.

There are specific issues with imputation over large scale surveys. The following list is adapted from  @plumpton2016multiple:

* A high number of variables renders the computation of joint distributions infeasible
* A high number of variables means the model is less likely to converge
* Categorical (non-Normal) variables means violation of core assumptions
* Categories with low observed frequency (sparsity in responses)
* Questions which are conditional upon previous responses
* Multiple multi-item scales which are summed (either directly or weighted) during analysis. Controversy over whether to impute individual items or scale totals
* Missing data in a single item of a multi-item scale leads to a missing total

Likert scales are not specifically mentioned they are obviously covered by the arguments given above. @plumpton2016multiple goes on to say that MI is robust to departures from normality and argues for the method's suitablility for many non-normal situations, including on Likert scales. However, it is difficult to see how these challenges are overcome using the classic MI method based entirely on a MVN model. 

@wu2015comparison describes some alternative strategies which are suitable for ordinal data: 

* Assume normality and accept imputed values on a continuous scale. These can be rounded or used as is
* Use a latent variable strategy, first accepting values on a continuous scale and then discretizing based on threshold parameters
* Use discriminant analysis to predict as if categorical
* Use a multinomial model to predict as if categorical
* Use a proportional odds model to predict an appropriate ordinal value
* Use Multiple Imputation with Chained Equations (MICE)

Three of these are prediction methods which are valid options only if the disadvantages described in the previous section are acceptable. However, they could be used to generate multiple, non-deterministic values if applied to bootstrapped samples. 

@white2011multiple [p.378] describes MICE as "a practical approach to generating imputations based on a set of imputation models, one for each variable with missing values" and alternatively names it as "sequential regression multivariate imputation." The procedure differs from MI in that it does not impute variables simultaneously from a joint distribution. @wu2015comparison [p.6] calls MICE "fully conditional imputation because prediction of missing data on one variable is conditional on the current values of all of the other variables at a specific iteration." Missing data are imputed one variable at a time, in order of increasing missingness according to @plumpton2016multiple. 

To begin, the missing values for all but the first variable are initialized with a suitable value, such as a sample with replacement from the observed values of each variable or randomly drawn from a posterior distribution. Then the observed values of the first variable are regressed on all the other variables. Substitute values for the first variable are drawn from the resulting posterior predictive distribution. Up to this point, the process is similar to the EM process described for single imputation. Before proceeding to impute the next variable, the initialized values are removed and its original state of missingness is restored before repeating the regression model and draw steps. In this way, the imputed values are introduced to the predictive model as it progresses over each variable and proceeds until all other variables with missing data have had their missing values substituted. This completes one cycle. For the first cycle, observed and initialized values are used. In subsequent cycles, observed and currently imputed values are used. The process iterates until the model converges or the maximum number of iterations is expended.  @white2011multiple reported that convergence was always reached in 10 cycles or fewer. On convergence, the model is made to yield a single imputed data set and is then reinitialzied for $m$ imputed data sets.

MICE is the most widely cited method for applying MI to non-normally distributed data and many texts that describe MICE posit it as an example of how MI performs well even when the MVN assumptions are violated. **find examples** This is not entirely accurate because MICE actually completes the imputation of non-normal data by side-stepping the MVN assumption with any number of workarounds. This is an important point because it becomes clear that non-normality is always handled by a non-parametric process! For example, @white2011multiple describes how MICE handles different variable types by transformation or *predictive mean matching* which is described 

while the results evaluation section in @plumpton2016multiple seem only to show attempts to use MI for continuous variables. It is not clear if any of the examples reported were of categorical or ordinal data.

in @R-mi and @su2011multiple as follows: 

For each observation with a missing value on a given variable, we find the observation (from among those
with observed values on that variable) with the closest predictive mean for that variable.
The observed value from this “match” is then used as the imputed value. Currently, these
predictions are obtained from bayesglm(). However, we plan to extend the predictive mean
matching options in the future. This method can be problematic when rates of missingness
are high or when the missing values fall outside the range of the observed data. We are
continuing to develop more flexible imputation models to better address the issue of creating
appropriate imputations.

@plumpton2016multiple MI has taken a long time to gain in popularity despite the proven benefits. **computational** The Amelia II package for R, @R-Amelia, overcomes some of the computational costs of this process by implementing an embarrassingly parallel architecture. 

### Success Criteria and Benchmarking of Imputation Techniques

There has been much comparable research done on benchmarking imputation techniques. The following discussion covers the several articles that align well with this project in purpose or approach and will be used to inform the experimental design that will be adopted in this study.

The goal of imputation is to maximize the use of collected data by reducing or eliminating the incidence of missingness in the dataset. This is achieved by finding substitute values for the missing data. The results of analysing the imputed dataset should be neutral (in the MCAR case) or enhanced (in the MAR case and ideally in MNAR cases) when compared to a complete case analysis. As has been demonstrated in earlier sections, this is not achieved by minimising prediction error as in a regression problem. Similarly, comparing imputed values to known values is not the best way to measure success, although this can still be useful to do. It is more important to demonstrate that the statistical properties of the imputed variables have been maintained, and to be able to measure the additional uncertainty around any parameter estimates given the initial missing state of the data. Also, @jonsson2004evaluation shows that Hot-decking methods, (which covers the association rules based method under investigation) do not always find a suitable donor value. A poor quality imputation (one which yields inaccurate analyses) is worse than no imputation at all. Thus, there may be a trade-off between finding a value to substitute each and every missing data point and preserving statistical parameters. This means that a final round of listwise deletion of remaining incomplete cases may still be necessary, with all the inherent assumptions and risks of such action. These factors suggest that the succuss, or otherwise, of an association rules based imputation method also needs take into account the efficacy of finding substitute values. 

Various statistics for evaluating and benchmarking imputation methods are suggested in the reference texts. Alternative approaches also exist; @pantanowitz2008evaluating is an example of indirectly ascertaining the quality of imputation by measuring the effect of imputation on prediction accuracy of a classification technique over the imputed dataset. Diagnostic tools are now available in a number of R packages, thanks to the work of @gelman2006data and others. Histograms and density plots, as well as scatter plots that highlight imputed data points are still a useful check but the `overimpute` method in the Amelia II Package, @R-Amelia runs a Monte-Carlo simulation using the generated imputation model over all the observed data and plots the actual vs imputed values with confidence intervals, making it very easy to see any systematic patterns.

**run example**

The question of whether to use synthetic or real-world data must also be addressed in studies of this kind. Synthetic data has the advantage that initial conditions can be tightly controlled and the effect of different conditions can be empirically assessed. However, there are often challenges to represent the data models realistically, without introducing a design bias. Most researchers use an underlying MVN model to generate synthetic data when testing the performance of MI. However, MI is based on the assumption that the data is MVN. So, in these experiments it is perhaps unsurprising the MI always performs well. MVN models also return data that is not on the natural scale of ordinal variables, such a non-integer, negative values and values greater than the maximum number of levels. The transformations required to resolve, this can also lead to poor representations such as top-coded or truncated distributions.

Testing the performance of imputation on entirely real-world or case study examples is intractable because the true value of population parameters is not known and the underlying mechanisms of missingness are unknown prior to starting the study. Most such experiments with real world data have to make compromises by creating semi-simulated data. This is done by inferring a MVN model using the means and a covariance matrix from the observed data portion of the dataset and drawing simulated samples from this model. Again, this introduces design bias. Firstly, the MVN problem is the same as in the synthetic data case. Secondly, if the data is MNAR then the simulated data set will no longer be representative.

A very pragmatic and simple process for creating rather realistic five-point Likert-scale simluations is given in @de2010five. This method simply gives proportions of each of the five possible scores in a population for different levels of simulated agreement, neutrality and disagreement, which empirically deliver specific means, standard deviations, and levels of skewness and kurtosis. It is indeed surprising that this method is not seen elsewhere in the literature. **put the table in the appendix**

```{r leite-design-matrix, results='asis'}
factor_name <- c("Number of ordinal categories"
                , "Sample size"
                , "Proportion of missing data"
                , "Skewness and Kurtosis"
                , "Missingness"
                , "Inter-item correlation")
factor_options <- c("3, 5, 7"
                , "400"
                , "10%, 20%, 30%"
                , "See Leite and Beretvas (2010)"
                , "MCAR, MAR by grouping variable"
                , "0.2, 0.8 (uniformly applied)")

bs_design_matrix <- knitr::kable(data.frame(
  "Design Factor" = factor_name
  , Options = factor_options
  , check.names = FALSE)
        , caption = "Experimental Design Matrix. Adapted from source: Leite and Beretvas (2010)")

bs_design_matrix
```

@leite2010performance uses synthetic data generating 10 variables from a MVN model and an experimental design matrix as described in Table \@ref(tab:leite-design-matrix). Skewness and kurtosis are introduced after the MVN draw using a rather complicated transformation compared to other papers reviewed such as @plumpton2016multiple and @wu2015comparison. After these steps are complete, the variables are discretised to ordinal data types but a lot of care had to be taken with the thresholds to try to preserve the initial settings. 

There are a number of problems with this research design. Firstly the MVN assumption that MI relies on has been built into the underlying data model. The MVN model also means that normality has been forced at the level of the individual items, and not at the level of a latent variable that the items represent. The inter-item correlation being uniformly set between all variables is also counter to the expectation that inter-item correlation should be high between related items of a scale but only coincidental between unrelated items. The authors claim that a ten-item scale approximates a survey, but doesn't account for Hinkin's recommended scale design of four to six items, @hinkin1997scale cited in @cas2016statistical. Just in terms of efficiency of a process, it seems that great care is taken to set up precise experimental conditions with a rather laborious process prior to the discretization step, only to have to measure everything again after and use the new values as starting conditions. 

For the analysis phase, the coefficient of correlation for each pair of variables is analysed on the discretized data before and after imputation. Holding the sample size constant for all the experiments means that the within imputatation variance $W$ was also constant. This is a convenient way to avoid having explicitly to calculate according to Rubin's rules. The variance of the correlation estimate in the one thousand replicates of each condition could be taken as $B$, the between imputation variance. Unlike other papers, these authors don't exploit the Monte-Carlo error and emperical distribution to report 95% CI for the various statistics reported.

The coefficient of correlation was measured in this experiment using Fisher's r-to-$Z_r$ transformation to correct for non-normality in the sampling distributions: 
\begin{equation}
  Z_r = \frac{1}{2} log(\frac{1+r}{1-r})
 (\#eq:fisherz)
\end{equation}

The relative bias was also calculated as follows:
\begin{equation}
  B(\hat{Z_r}) = \frac{\hat{Z_r} - \zeta_p}{\zeta_p}
 (\#eq:relbias)
\end{equation}

where $\zeta_p$ is the population parameter, based up the initializing conditions. A value of < 0.05 is acceptable. 

Under the conditions described, $B(\hat{Z_r})$ was always negative. The interpretation of this is that imputation always reduced the correlation between variables. The paper describes this as a desirable side-effect because it will reduce the power of statistical tests in accordance with the uncertainty introduced by missing data. However, no mention is made of the importance of reliability (iter-item correlations) for Likert scale analysis to be validated, which seems to be an important omission. The authors report that the MI process has been robust to violations of non-normality because the degree of skewness and kurtosis was not a significant factor in their results but, as mentioned, the underlying MVN may not have provided sufficient challenge to the MI technique.

```{r wu-design-matrix1, results='asis'}
var_type <- c("Both"
              , "Both"
              , "Both"
              , "Ordinal only"
              , "Ordinal only"
              , "Dichotomous only"
              , "Dichotomous only")
imputation_methods <- c("Normal"
                        , "Naive Rounding"
                        , "Latent"
                        , "Multinomial"
                        , "Proportional Odds Model"
                        , "Logistic Regression"
                        , "Discriminant")
ws_design_matrix <- knitr::kable(data.frame(
  "Appropriate Data Type" = var_type
  , "Imputation Strategy" = imputation_methods
                  , check.names = FALSE)
        , caption = "Withing Subjects Experimental Design Matrix: Imputation Strategies. Adapted from source: Wu et al. (2015)")

ws_design_matrix
```

```{r wu-design-matrix2, results='asis'}
factor_name <- c("Number of ordinal categories"
                , "Sample size"
                , "Proportion of missing data"
                , "Asymmetry thresholds")
factor_options <- c("2, 3, 5, 7"
                , "125, 500"
                , "30%, 50%"
                , "See Wu et al. (2015)")

bs_design_matrix <- knitr::kable(data.frame(
  "Design Factor" = factor_name
  , Options = factor_options
  , check.names = FALSE)
        , caption = "Between Subjects Experimental Design Matrix: Conditions. Adapted from source: Wu et al. (2015)")

bs_design_matrix
```

@wu2015comparison compares a number of single impuation techniques shown in Table \@ref(tab:wu-design-matrix1) by examining how well each recovers the reliability coefficient Cronbach's alpha, mean scale scores and coefficients estimated by regressing one scale on another. The data types in the design matrix unusually include a dichotomous ordinal option, leading to a slight flaw in that not every imputation strategy suits all the data so like-for-like comparisons can't be made across all conditions. Nevertheless, this is very useful when considering which is the right tool for the job at hand. 

The design matrix show in Table \@ref(tab:wu-design-matrix2) is fairly typical of similar experiments except in the missing data proportions which were unusually high. The reason given by the authors was to shine a light on the differences between the methods. However, it cannot be stated with certainty that this is a sound approach because all methods perform poorly at this level of missingness. Such an experimental design risks missing the "sweet spot" where the more modestly performing techniques start to fail while the better techniques continue to do well. Another consequence of this is the higher computational requirement as $m = 50$ imputations were generated for each run, in accordance with @white2011multiple.

Another key difference from other authors is that experimental design factors were listed by the authors as between subjects and within subjects. This was to provide clarity on the analytical method used for benchmarking and put emphasis on the purpose of the study, which was to compare different imputation strategies. The results were evaluated by means of a repeated measures ANOVA test.

The authors describe in detail their parameters for synthetic data generation using a two-factor model. Although the exact methodology for this is not specified, it could certainly be achieved with a Structural Equation Model (SEM). Two six-item continuous scales, A and B, were generated with a high mean inter-item correlation and a trivial between scale correlation. Skewness was introduced during the discretization step by using thresholds according to the method in @rhemtulla2012can cited in @wu2015comparison. A single large dataset (N=500000) was generated to estimate the population parameters. The reason given for this last step was that the parameters cannot be analytically determined after the discretization process. This seems to be a much more pragmatic approach and gives an improved representation of Likert scales data than the one used in @leite2010performance. 

**put the thresholds used in the my appendices**
**revise definitions so they're aligned with carpita**

To create a MAR process, two auxiliary, continuous variables were created and used to generate the missing values on the ordinal variables. Aux 1 was rank ordered and assinged a probability of $P = 1 - \frac{rank(Aux1)}{N}$. Values of A1-A3 were set to missing if the probability of random variable $X \sim \mathcal{U}(0,\ 1) < P$. A similar process was carried out for variables B1-B3. They were set to missing if $X < P = 1 - \frac{rank(Aux1 + Aux2)}{N}$ and the process was stopped after the desired missing proportion was reached. It is not made clear how this final step was performed. It has to be assumed that it was done using a one-at-a-time draw because the rank weights are evenly distributed over all the instances. Such a distribution means that a vectorwise application over all variables would always result in approximately half the data being set to missing. For a one-at-a-time draw, it would be essential to randomize the initial ordering but this wasn't explicitly stated in the paper.

The *case reduction* problem is identified in @jonsson2004evaluation which describes how individual cases (subjects, rows) might have so much data removed that they are rendered useless. These authors addressed the problem by discarding cases whose number of non-missing variables dropped below a certain threshold, but this must have been programmatically more complex and created further inconsistencies in the experimantal starting conditions. The approach in @wu2015comparison, to subject only the first three variables of each six-item scale to the missingness mechanism, is again a very pragmatic solution. 

For the various statistical analyses conducted on the imputed data, the standardised bias of any parameter $\theta$ was calculated as:

\begin{equation}
  B_{std} = \frac{(\frac{1}{q}\sum^q_{i=1}\hat{\theta_i}) - \theta_p}{s_{\theta}}
  (\#eq:standbias)
\end{equation}

where $q$ is the number of replications of the experiment and $s_{\theta}$ the empirical standard deviation of parameter across all replications.

And the mean squared error as:
\begin{equation}
  MSE =\frac{1}{q}\sum^q_{i=1}(\hat{\theta_i} - \theta_p)^2
  (\#eq:mse)
\end{equation}

The ratio of MSE of any test model to the MSE of the benchmark (normal) model was also measured:
\begin{equation}
  MSE\ Ratio = \frac{MSE_{test}}{MSE_{norm}}
  (\#eq:mse)
\end{equation}

A model performing better than the benchmark will have $MSE\ Ratio < 1$.

Confidence intervals at the 95% level were also calculated using the usual Student's T distribution, while for Cronbach's alpha, the following calculation was used:
\begin{equation}
  CI_{upper} = 1 - [(1 - \hat{\alpha}) \times \mathcal{F}_{(\gamma/2, \ df_1, \ df_2)}]
  (\#eq:alphaciupper)
\end{equation}
\begin{equation}
CI_{lower} = 1 - [(1 - \hat{\alpha}) \times \mathcal{F}_{(1-\gamma/2, \ df_1, \ df_2)}]
  (\#eq:alphacilower)
\end{equation}

Where $\mathcal{F}$ is the F distribution, $\gamma$ is the desired significance level, $n$ is the sample size, $p$ is the number of items, $df_1 = (n-1)$ and $df_2 = (n-1)(p-1)$. Although it is not explicitly stated in the article, this calculation could theoritically result in a negative lower bound, which is impossible for an F distribution. It is not clear if the authors truncated the lower bound at zero or simply relied on large values for the two $df$ parameters to ensure the CI was suitably tight.

CI coverage was also calculated as the proportion of 95% CIs (generated by one thousand replications of each experiment) that contained the population parameter. This is a well chosen use of the empirical distribution of CIs as it specifically addresses the research question of how well the underlying population parameters are recovered by the various imputation strategies.

@plumpton2016multiple is a very comparable study to the current research project, conducting an an assessment of a novel method of multiple imputation on multiple-item scales. The authors' motivation is that MI over large datasets of this kind is computationally infeasible and they refer heavily to a real world dataset from a secondary, peer-reviewed study which demonstrates both the validity of their motivation and their method. Their method pre-calculates the complete scale-item scores and uses these for imputation rather than all the individual item scores, which in their view, overcomes the computational challenge of the full dataset MI.

However, their study also has some apparent flaws. Using semi-simulated data from an MVN approximation of their dataset, and an MAR missingness mechanism with high proportions of missing data (18%-55%), they benchmark their method only against full MI and complete case analysis. Under these conditions, it is inevitable that their model will perform better than the complete case benchmark. Furthermore, their semi-simulated data is relatively a very small sub-sample of the available variables, and poses no challenge to the full MI algorithm. @su2011multiple discusses imputation of large-scale surveys to create public-use multiply imputed datasets, suggesting that several hundred variables might need to be imputed. So it seems to be an important omission in @plumpton2016multiple not to cover the computational performance of their novel method compared to full MI by using a more challenging number of variables. The authors only briefly mention that the number of variables would be reduced by pre-calculating the complete item scores. It is also unclear how missingness would be handled in the variables subject to the pre-calculation steps. 

**re read this paper and make sure you about these statements**

This article does provide an exceptionally useful visual evaluation of the methods in the study, using forest plots. These quickly reveal the best and worst performers, and any associations with the experimental parameters. See Figures \@ref(fig:plumptonCapture1) and \@ref(fig:plumptonCapture2).

```{r plumptonCapture1, fig.align='center', fig.cap='Forest plots showing odds ratios of logistic regression coefficients after imputation or list-wise deletion. Source: Plumpton et al. (2016)'}
knitr::include_graphics("figure/plumptonCapture1.png", auto_pdf = TRUE, dpi = NULL)
```

```{r plumptonCapture2, fig.align='center', fig.cap='Forest plots showing simulation results of three variations of the initial conditions. Confidence intervals are imperical, from one-thousand repetitions. Source: Plumpton et al. (2016)'}
knitr::include_graphics("figure/plumptonCapture2.png", auto_pdf = TRUE, dpi = NULL)
```

```{r carpita-design-matrix, results='asis'}
factor_name <- c("Number of ordinal categories"
                , "Sample size"
                , "Proportion of missing data"
                , "Missing data mechanism"
                , "Imputation Strategy"
                , "Benchmark Methods"
                , "Experimental Data")
factor_options <- c("5, 6"
                , "500, 1000, 2000"
                , "10%, 20, 30%"
                , "MCAR, MAR, MNAR"
                , "Benchmarking novel ABPN method"
                , "PM, CIM, TW, ICS"
                , "Real-world sub-sample")

cp_design_matrix <- knitr::kable(data.frame(
  "Design Factor" = factor_name
  , Options = factor_options
  , check.names = FALSE)
        , caption = "Experimental Design Matrix")

cp_design_matrix
```

Of all the articles surveyed, the study reported in @carpita2011imputation has the best aligned motivation and methodology with this research project. The purpose of the study is to compare a novel, non-parametric method with existing methods that are well suited to Likert scale data. There are many notable items in this methodology:

* Sole reliance on a large real-world dataset collected in a previous survey. This dataset was part of a peer-reviewed study and there is no reason to doubt its validity for this secondary use. No synthetic or semi-synthetic data was generated. Instead, large random draws were taken from the original dataset to make three different sized sub-samples. This process precluded the assumptions and design bias of comparable studies based on synthesised MVN data. 
* The choice of a novel non-parametric, non MI based method and further more, the choice of benchmark methods tailored for the Likert data type. These included Person Mean (PM), Correct Item Mean (CIM), Two-Way (TW) and Item Correlation Substitution (ICS) discussed in the previous section. **hyperlink or reference here** Multiple imputation was not used at all.
* The use of a mechanism to generate MNAR patterns in the data was not seen in any other study. This is remarkable when seen in context of the ease in which it is done, simply by generating MAR data but not including the co-variates in the imputation model.

For MCAR data, a fixed proportion of randomly selected values was deleted. The MAR and MNAR methods used were not seen anywhere else but appear to be very well thought out. They are defined in the @carpita2011imputation as follows:

Recalling the data matrix $X$ and its corresponding missing indicator matrix $M$. Let $Z$ be a vector of covariates, $h \in \{1,\ 2,\ \dots,\ H\}$ be the index of the covariates and $z_{ih}$ be the value of $z_h$ for respondent $i$. 

For MAR data,
\begin{equation}
  P(m_{ij} = 0 \mid z_{i1},\ z_{i2}) = \frac{exp(\alpha_0 + \beta_1z_{i1} + \beta_2z_{i2})}{1 + exp(\alpha_0 + \beta_1z_{i1} + \beta_2z_{i2})}
  (\#eq:carpitamar)
\end{equation}

Note that the probability of missingness increases as the values of $z_{ih}$ increase. This will have a similar outcome to the MAR mechanism using rank order of co-variates, described in @su2011multiple.

For MNAR (NRX) data that depended on the person mean and the item mean,
\begin{equation}
  P(m_{ij} = 0 \mid \bar{x_{i+}},\ \bar{x_{+j}}) = \frac{exp(\alpha_0 + \gamma_1 \bar{x_{i+}} + \gamma_2 \bar{x_{+j}})}{1 + exp(\alpha_0 + \gamma_1 \bar{x_{i+}} + \gamma_2 \bar{x_{+j}})}
  (\#eq:carpitamnarx)
\end{equation}

For MNAR (NRXZ) data that depended on the person mean, the item mean and the covariates,
\begin{equation}
  P(m_{ij} = 0 \mid z_{i1},\ z_{i2},\ \bar{x_{i+}},\ \bar{x_{+j}}) = \frac{exp(\alpha_0 + \beta_1z_{i1} + \beta_2z_{i2} + \gamma_1 \bar{x_{i+}} + \gamma_2 \bar{x_{+j}})}{1 + exp(\alpha_0 + \beta_1z_{i1} + \beta_2z_{i2} + \gamma_1 \bar{x_{i+}} + \gamma_2 \bar{x_{+j}})}
  (\#eq:carpitamnarxz)
\end{equation}

In the MAR, NRX and NRXZ methods, the $\beta$ and $\gamma$ terms were held constant at 1 and only the $\alpha_0$ term was varied to control the proportion of missing values. Values were set to missing if the probability of random variable $P(m_{ij} = 0 \mid \dots\ ) \le u_{ij},\ u_{ij} \sim \mathcal{U}(0,\ 1)$ which can be done in a single programmatic step when the exact parameters are known. Although the authors very helpfully include their values of $\alpha_0$, these are specific to their study. These quantities have to be determined experimentally for any other data. While this method is a very precise approach, it is simpler to use the one-at-a-time draw method inferred in @wu2015comparison.

To determine the quality of the imputation methods, the study measured Cronbach's alpha relative error CAE:
\begin{equation}
  CAE^{(*)} = \frac{\hat{\alpha}^{(*)} - \alpha}{\alpha}
  (\#eq:cae)
\end{equation}

where $\hat{\alpha}^{(*)}$ is the estimate of Cronbach's alpha after imputation with method * and $\alpha$ is the population parameter estimated from the complete, untreated data. It should be noted that this is identical to the relative bias calculation in \@ref(eq:relbias).

The Scale Score Error was also measured. Let $A'(i)$ be the set of indices of non-available items for subject $i$ and $B'(j)$ be the set of indices non-available respondents for item $j$.
Consider the total score with complete, untreated data $x_{i+}$ and with imputed data with method , $\hat{x}^{(*)}_{i+}$ For subject $i$ the scale score error (SSE) is:
\begin{equation}
  c^{(*)}_{i+} = \hat{x}^{(*)}_{i+} - x_{i+} = \sum^n_{j \in A'(i)} (\hat{x}^{(*)}_{ij} - x_{ij}),\ i \in B'(j)
  (\#eq:sse)
\end{equation}

and the Mean Absolute Scale Error is:
\begin{equation}
MASE^{(*)} = \frac{\sum^p_{i \in B'} |c^{(*)}_{i+}|}{q},\ q = count(B')
  (\#eq:mase)
\end{equation}

These statistics were estimated over one thousand repetitions of the experiment for each conditions, in a similar approach to other studies. The mean was reported with the empirical 95% CI. Also as with other studies, the sample sizes and number of ordinal categories were not found to be significant factors so these results were pooled. What is published in the final report is a three way table of results, comparing imputation methods at each of the different proportions of missingness with each of the missingness mechanisms.

@jonsson2004evaluation describes a similar benchmarking experiment with a KNN-SPRM technique. This study is relevant because the technique adopted cannot always guarantee an estimated value for all missing cases, as may also be the case for an ARM based method. The paper describes how the imputation sometimes had to be completed with a list-wise deletion of any cases that could not be fully imputed. The authors porpose some measures of the technique's *ability* to impute the dataset by counting the number of imputed and completed cases versus those cases that had to be dropped at the end of the process. The following is adapted from @jonsson2004evaluation:

Let $A$ be the number of compete cases remaining and $A'$ be the number of incomplete cases remaining. The total number of cases $C = A + A'$.

Let $A''$ be the number of cases that were fully imputed. It follows that $0 \le A'' \le A'$. The total number of complete cases after imputation is $C' = A + A''$.

Ability of the imputation technique is defined as:

\begin{equation}
  R = \frac{A''}{A'}
  (\#eq:ability)
\end{equation}

Their experiments were designed to facilitate the discovery of optimal parameter values for k of KNN. In a follow up paper, @jonsson2006benchmarking the technique is benchmarked against random, median and mode imputation techniques. It is not clear why these techniques were chosen, other than for the characteristic of chosen a natural scale value compared to mean imputation, which would result in non-integer values. There is no discussion about the relative merits of these techniques compared to other available techniques, and so the question of bias in the experimental design arises, whether these were appropriate for a benchmark comparison.

## The Case for a Novel Approach

### Association Rules

Association rules mining (ARM) was first popularised by the development of a fast algorithm for large databases, *apriori*, in @agrawal1993mining. It was developed in response to the vast amount of data collected by large retail enterprises on their customer purchase data. 
Many of the fundamental concepts of ARM were formalised in @agrawal1993mining article and its follow up, @agrawal1994fast which are considered to be the seminal works on the topic. The following definitions are adapted from @agrawal1994fast [p.2] and @tan2005chapter6 [pp.329-330]:

Let $I = \{i_1,\ i_2,\ \dots,\ i_k\}$ be the set of all items. Let $T = \{t_1,\ t_2,\ \dots,\ t_N\}$ be a set of $N$ transactions where each transaction $t_j \subseteq I$. A transaction $t_j$ is said to contain an itemset $X$ (a set of some items in $I$) if $X \subseteq t_j$.

The brute force method of computing all possible rules in a transaction set of N possible items is $O(3^N)$ **double check**. When it emerged, *apriori* was described as a "fast" algorithm because it outperformed everything that came before. It is still widely used today. @tan2005chapter6 explains that to achieve its objective of efficiently finding useful association rules, ARM must solve two distinct problems:

1. Overcoming the computational challenge of discovering frequent itemsets in large transaction data
1. Ignoring spurious patterns/rules that occur by chance

These challenges are addressed under the *apriori* algorithm by searching the database iteratively for itemsets that occur more frequently than a given probabilistic threshold, known as *support*. Rule discovery is deferred until all frequent itemsets are found i.e those occuring with > minimum support.

**Support for itemset $X$ in Transaction Data $T = \{t_1,\ t_2,\ \dots,\ t_N\}$** 
\begin{equation}
  Supp(X) = |\{t_j|X \in\ t_j, t_j \in T\}| = \frac{|X|}{N}
  (\#eq:supportitemset)
\end{equation}

At iteration n, the support for each itemset of size n is calculated. So the first iteration finds all the individual items. Each subsequent iteration, new candidate itemsets with n members are formed by merging smaller itemsets from the growing collection using a method to ensure duplication does not occur (not discussed here). At each iteration, any that fail to meet the minimum *support* are pruned and ignored henceforth. This rapidly reduces the search space and works because of the anti-monotone property of support; $\forall X,Y : (X \subseteq Y) \longrightarrow Supp(Y) \le Supp(X)$, that is to say, support for an itemset never exceeds support for any of its subsets, @tan2005chapter6.

The resulting collection of itemsets is then mined for rules of the pattern $X \implies Y$ (read as $X\ \text{implies}\ Y$) where $X \subseteq T$ is the *antecedent* of a rule and $Y \subseteq T$ is the *consequent* and $X \cap Y = \varnothing$. Although @agrawal1994fast [p.3] clearly states that the cardinality of the consequent $|Y| \ge 1$, in some implementations a restriction of $|Y| = 1$ is applied. he R package *arules*, @R-arules is one such case. This further reduces the computational complexity is also a useful property for this research where a substitute value for one target variable at a time will be sought using association rules.

**Support of rule $X \implies Y$** 
\begin{equation}
  Supp(X \implies Y) = Supp(X \cup Y) = \frac{|X \cup Y|}{N}
  (\#eq:supportrule)
\end{equation}

Note that the support of rule $X \implies Y$ depends only on the support of the superset $X \cup Y$. So by performing the frequent itemset generation step in full, the support for all possible rules from the collection is already known to exceed the minimum support. 

*Support* is useful for ruling out rules that simply occur by chance but the objective of ARM is to find "strong rules" that are not only frequent but also reliable for inference. For this purpose, different measures of *interestingness* are used. A most comprehensive list is given in @hahsler2017interesting and their properties are discussed thoroughly in @tan2005chapter6. In *apriori*, *confidence* is used by default and *lift* is also common, which adjusts for the frequency of the consequent itemset:

**Confidence of rule $X \implies Y$** 
\begin{equation}
  Conf(X \implies Y) = \frac{Supp(X \cup Y)}{Supp(X)}
  (\#eq:confidence)
\end{equation}

**Lift of rule $X \implies Y$**
\begin{equation}
  Lift(X \implies Y) = \frac{Conf(X \implies Y)}{Supp(Y)} = \frac{Supp(X \cup Y)}{Supp(X) \times Supp(Y)}
  (\#eq:lift)
\end{equation}

@garcia2004mining categorizes ARM among the unsupervised and descriptive techniques as opposed to the other common categorisation (supervised/predictive). The goal of ARM is to find patterns (frequent itemsets and supported rules, in this case) without any prior information about what these patterns might be. The original implementation uses transaction data which is represented as a key-value pair, where the key is a transaction Id (TID) and the value is a list of items purchased in the single transaction identified by TID. See \@ref(tab:transdata) for example. In @R-arules this is represented as an *incidence matrix*, which is a sparse, binary matrix where each row is a transaction and all possible items are each represented in a unique column as present (1) or absent (0). See \@ref(tab:incidencemat). However, this approach is not without its limitations; Transaction data, or incidence matrix data, codifies only the presence or absence of an item. This is binary information and ignores the important variable of quantity or attributes with more than one possible value, such as ordinal data. Also, the binary variables in the (sparse) incidence matrix are said to be *asymmetric* [@tan2005chapter7] because a value of 1 is much more important and far less frequent than a value of 0. 

Overcoming the binary limitation is a trade-off. @ma1998integrating describes a process using a normal relational table with highly discretized data, where the items are represented on columns with various integer values. Each item-value pair is coerced to a binary dummy variable which creates a very wide matrix. This makes ARM applicable to representations of surveys with multi-ranked (non-binary) ordinal data and Likert scales but @tan2005chapter7 explains that the relative scarcity of a useful value is magnified, drastically reducing *support* for each item and producing too many rules. See \@ref(tab:nonbinincmat). @tan2005chapter7 continues with advanced approaches for overcoming this issue based on discretized coninuous data.

The ideas most relevant to this research are adapted here for use with imputation of ordinal variables:

* Pre-processing methods. Combine adjacent values of ( $X \in \{1, 2\}$ or $X \in \{1, 2, 3\}$ ) until minimum support is exceeded:
    * Single Imputation: Applies to antecedent variables.
    * Multiple Imputation: Applies to consequent variables. A cumulative odds model or other smoothing method may be used as a distribution for random draws.

* Statistical methods. Generate frequent itemsets not including the target variable. Compute descriptive statistics of the target variable for freqent item sets. A rule is interesting if the target variable mean for the segment of population covered by the rule is significantly different from the section of the population not covered by the rule.
    * Single Imputation: Introduce the target variable mean as the rule consequent, using appropriate rounding or truncating to meet ordinal value constraints.
    * Multiple Imputation: Introduce target variable statistics as the rule consequent to be used as a distribution for random draws.

* Multi-level (hierachical) methods. Generate rules at a higher level of a hierarchy so naturally grouped variables are combined. Step down into lower levels until minimum support is no longer met. For a Likert scale, this would involve generating itemsets and rules after calculating the aggregate of multiple grouped items (e.g. summation, first principle component etc).
    * Single Imputation: Applies to antecedent variables
    * Multiple Imputation: Combine with other methods.

```{r transdata}
TID <- 1:5
items <- c("{Bread, Milk}"
           , "{Bread, Diapers, Beer, Eggs}"
           , "{Milk, Diapers, Beer, Cola}"
           , "{Bread, Milk, Diapers, Beer}"
           , "{Bread, Milk, Diapers, Cola}")

transdata <- data.frame(TID = TID, Items = items
                           , row.names = TID)
knitr::kable(transdata
             , caption = "Transaction data example. Source: Tan, P. et al (2005)"
             , booktabs = TRUE)

```

```{r incidencemat}
TID <- 1:5
Bread <- c(1, 1, 0, 1, 1)
Milk <- c(1, 0, 1, 1, 1)
Diapers <- c(0, 1, 1, 1, 1)
Beer <- c(0, 1, 1, 1, 0)
Eggs <- c(0, 1, 0, 0, 0)
Cola <- c(0, 0, 1, 0, 1)

incidentmat <- data.frame(TID = TID
                          , Bread = Bread
                          , Milk = Milk
                          , Diapers = Diapers
                          , Beer = Beer
                          , Eggs = Eggs
                          , Cola = Cola
                          , row.names = TID)
knitr::kable(incidentmat
             , caption = "Incidence matrix example. Source: Tan, P. et al (2005)"
             , booktabs = TRUE)
```

```{r nonbinincmat}
TID <- 1:5
Bread_1 <- c(1, 0, 0, 0, 0)
Bread_2 <- c(0, 1, 0, 0, 0)
Bread_3 <- c(0, 0, 0, 1, 1)
Milk_1 <- c(1, 0, 1, 0, 0)
Milk_2 <- c(0, 0, 0, 1, 0)
Milk_3 <- c(0, 0, 0, 0, 1)
Eggs_2 <- c(0, 0, 0, 0, 0)
Eggs_4 <- c(0, 0, 0, 0, 0)
Eggs_6 <- c(0, 1, 0, 0, 0)

nonbinincmat <- data.frame(TID = TID
                          , Bread_1 = Bread_1
                          , Bread_2 = Bread_2
                          , Bread_3 = Bread_3
                          , Milk_1 = Milk_1
                          , Milk_2 = Milk_2
                          , Milk_3 = Milk_3
                          , Eggs_2 = Eggs_2
                          , Eggs_4 = Eggs_4
                          , Eggs_6 = Eggs_6
                          , row.names = TID)
knitr::kable(nonbinincmat
             , caption = "Wide incidence matrix for ordinal values. Adapted from: Tan, P. et al (2005)"
             , booktabs = TRUE)
```

Despite the impressive performance gains by *apriori*, many authors [e.g @mythili2013performance; @tan2005chapter6] raise issues; The cycle of repeated database scans and candidate generations means that performance does not scale well on very large databases (large numbers of transactions/rows or items/columns/dimensions), where the support threshold is low or where the transaction data are dense (maximum itemset width is large). A detailed analysis of the time complexity is not given here, because the size and dimensionality of suitable real-world datasets is expected to be orders of magnitude smaller than retail enterprise transaction data, for which the algoritm was originally designed.

Work has been done develop more compact representations of itemsets and rules, such as maximal and closed itemsets. These concepts lead to depth-first searches such as Equivalence Class Traversal (eclat) which find the frequent itemset border more quickly and lead to more efficient pruning. The frequent pattern tree (FP-Tree) is another, very compact representation of frequent itemsets which may be small enough to fit in main memory. The FP-Growth algorithm can construct an FP-Tree in just two database passes.

Returning to the idea of imputation using similar pattern response matching, it is self-evident that ARM is an algorithm for finding similar patterns. Upon evaluation of a dataset by ARM, a collection of rules (patterns) is returned. However, out-of-the-box this neither offers nor assumes a specific usage for those found patterns. That is left up to the analyst. This research suggests using association rules with a consequent cardinality of one, to substitute values where the antecedent matches frequent response patterns in a survey or questionnaire.

### Classification with Association Rules

\cite{freitas2000understanding} posits that ARM is a deterministic task and very different to classification though does concede that there are special cases where the resulting models can be used for predictions or, more specifically, classification. These special cases involve finding a subset of rules, which \cite{ma1998integrating} calls \textit{class association rules} (CARs). These CARs have the characteristic of having the target variable as the only element in the consequent or right-hand side of the rule. The ``partial classification" described in \cite{ali1997partial} is of particular interest because the objective here is to discover characteristics of the target variables. The authors suggest applicability to situations where some attribute will be modeled based on the other attributes. This is precisely the situation under investigation in this project.\newline

While prediction and imputation differ in scope and goal, the intention of finding a estimated (or ``plausible") value rather than an accurate value could be seen as a mere relaxation of success criteria. If prediction with excellent results is possible from association rules, then it stands to reason that imputation in some form must also be possible.


## Conceptual Model - Bringing it all together?

A particular characteristic of many surveys is the use of ordinal data and multiple-item (Likert) scales. \cite{huisman1999missing} states that there is a strong relationship between the individual items of a Likert scale which measure one latent trait. Techniques that can recognize this within-instance, structural information and preserve it in the imputed dataset should be valuable. \cite{agrawal1994fast} states that association rules mining uses probabilistic measures (support and confidence) for discovering frequent patterns. Association rules mining algorithms work on discretized or categorical data, such as ordinal, nominal and binary. Furthermore, \cite{chandola2005summarization} describe association rules as a compact model of a dataset and as such may be used to enhance other stages of the analysis. So an association rules-based method for survey data would have several advantages over other imputation methods, yet a search of the literature yields no information on the use of association rules in this context.
