# Literature Review

In this section, the research question is decomposed into its component topics and each is reviewed systematically and in detail. The topics are covered in a logical order that reflect the motivation for this research: The breadth of use of surveys and questionnaires and the problem of missing data, the well-understood underlying models and current popular solutions. The potential suitability of association rules as a basis for a new imputation method is explored in the later subsections.

## The Importance of Surveys and Questionnaires

@groves2011three [p. 861] states that "the systematic observation of social and economic phenomena has roots deep in history" but describes the 1930's-1960's period as foundational for the development and popularization of the survey method involving structured questions, random sampling and statistical inference. It was also a golden age with respect to response rates, perhaps due to the novelty and the small numbers of surveys carried out, relative to today. Understandably, the earlier historical discussions in this article are backed up by fewer references than paragraphs covering more recent decades. Nevertheless, the author is well regarded and prolific on this subject matter. As a potted history of surveys and questionnaires, the article provides a very plausible interpretation of the influence of sociological and technological factors which explain why the survey method became so ubiquitous up to and during the 1990's but began to see a decline in usage thereafter. For example, one suggestion is that the popularity of the method was driven in part by usage in the consumer and service sectors as a means to gain customer feedback and thereby maximize profits. This speaks to economic common sense. The aid of computers in the 1960's reducing processing costs and allowing greater numbers of participants e.g. hundreds of thousands [@schultz2015psychology p.32] and more sophisticated computational analysis would have been another driver both in commerce and government research. In academia, the publication of archived data "permitted the rise of quantitative methods in many social science disciplines," @groves2011three [p. 865]. 

Consistent with the description of a decline in popularity, @sheehan2001mail posits that even the change from face-to-face and postal to email-based surveys provided only a temporary stay in the overall decline in usage in more recent times. However, there is no shortage of examples to suggest that the method is still incredibly important today, [@shrive2006dealing; @schultz2015psychology] *more citations*. In fact @schultz2015psychology suggests that merely carrying out a survey in an industrial/organisational setting can raise staff morale and reduce conflict between management and unions. 

@groves2011three goes on to make a number of reasoned predictions about future adaptations of the survey method in light of the emergence of a multitude of new data sources (of which the author's description matches what is now commonly coined as "Big Data") and continues to suggest that these new sources of data are collected organically, i.e. they are "unstructured", so there's an ongoing requirement for the designed data of surveys. One technological development is not discussed in detail in the article, namely the emerging popularity of online surveys construction software (SurveyMonkey, Qualtrix, www.onlinesurveys.ac.uk, SmartSurvey, SogoSurvey to name but a few). This could be because the article is not recent enough or perhaps as @solomon2001conducting describes, because of the caution with which this new technology was viewed by the academic community. Such caution may relate to statistical issues such as coverage bias resulting from "the skew in demographics of the internet population versus the general public" [@hayslett2005pixels p. 78] combined with the practical need for researchers to become more familiar with the technology before being able to construct surveys with sufficiant statistical rigour. Nevertheless, @schultz2015psychology states that most companies now use web-based surveys and @evans2005value shows that online surveys were already a rapidly growing, multi-million dollar industry by the early 2000's. The article describes many weaknesses as well as strengths in the sector. Indeed, it was very much a clarion call for addressing the weaknesses to consolidate the value of online surveys for the modern day. It would be safe to assume that these have been addressed for there has been a rise and rise of services for the creation of casual surveys and polls by the "citizen analyst" as well as seasoned pollsters which are distributed through social media and quick customer ratings devices. These technological innovations may introduce the need for development of new methods that stand up to analytical scrutiny but they do offer new opportunities given their widespread dissemination.

## Characteristics of Survey Data and Prevalent Data Types

@johnson2009working states that nominal and ordinal variables are predominant in surveys. When collecting demographic information, apart from height and weight, most routine information will be categorical in nature, e.g. eye colour, hair colour, ethnic origin. Beyond these simple categorizations, researchers in the social sciences and other disciplines often use surveys to collect data on subjective attitudes, perceptions and psychological traits. Indeed, @christensen2010ordinal [p.3] states that "Ordered categorical data, or simply ordinal data, are commonplace in scientific disciplines where humans are used as measurement instruments." Ordinal variables are ideal for capturing gradings, ratings, opinions, degree of agreement, rankings, levels of attainment and so on. They are also very practical in terms of survey design considerations. In this respect @schultz2015psychology [p.37] explains that "fixed-alternative questions simplify the survey and allow more questions to be asked," and also "answers to fixed-alternative questions can be recorded more easily and accurately than can answers to open ended questions."

There are two categories of ordinal data, according to @anderson1984regression cited in @gertheiss2009penalized

* grouped continuous variables (GCV) - an underlying, continuous variable discretized into buckets, shingles or categories, such as might be returned by a histogram function over age, height or weight. 
* assessed ordered categorial variables (AOCV) - a judgement given by some actor or assessor on the grade, level or rank of a set of information which is not among the observed variables.

Ordinal data generally take the form of a categorical variable with levels $k = \{0,\dots,K\}$ or $k = \{1,\dots,K\}$ or sometimes a symmetric form $k = \{-K,\dots,0,\dots,K\}$ and all are interpreted as a monotonic, ranking representation $Least \succ Less \succ More \succ Most$. Ordinal variables of type AOCV can be used individually but are often combined in a survey as multiple related items (multiple-item response scales). Likert scales are a special case of ordinal variable and a special case of multiple-item response, whereby a collection of statements are scored by the respondent using an integer series which is anchored to phrases of sentiment or attitude (e.g. 1 = strongly disagree, 5 = strongly agree). The collection of responses measure the outwardly manifested facets of some latent concept or factor. The respondents' positions on the latent factor can then be estimated by appropriately summing, averaging or otherwise transforming the multiple responses of the individual items [@carpita2011imputation]. Likert scales are ubiquitous in behavioural science, marketing, usability, customer feedback, psychological and clinical research. Although the term Likert scale is sometimes used in the literature to refer to single items following the classic agree/disagree scoring, the correct definition refers to a combined scale as described here. A thorough definition is given in @gliem2003calculating.

It is common in the literature to find misunderstandings and disagreement about how ordinal variables ought to be treated analytically. In fact, this is one of the great debates of statistics in the last few decades. A range of conflicting views which is typical of the long-running discussion is reviewed in @gertheiss2011testing reviews. The disagreements centre on the question of whether it is appropriate to use the integer class labels directly as explanatory variables in a regression model. @johnson2009working gives detailed examples of why ordinal variables cannot be treated the same as numerical scales and @gertheiss2009penalized explains how this assertion holds even when the ordinal variable is a discretised version of an underlying continuous scale. Essentially, this is because the interpretation depends on arbitrarily assigned categories and mid-point estimates. Upper and lower unbound categories may hide any number of extreme values. This would seem to be very sound reasoning and @joreskog2005structural provides a formalization of the concept of inferred discretization, reproduced in \@ref(eq:ordvar1)-\@ref(eq:ordvar2). These show the relationship between $z$, an observed ordinal variable which varies between 1 to $m$ integer values and $z^*$, an underlying, unobserved variable which has a range from $-\infty$ to $\infty$.

\begin{equation} 
  z = i \Longleftrightarrow \tau_{i-1} < z^* < \tau_i, \quad i = \{1, 2, \dots, m\}
  (\#eq:ordvar1)
\end{equation}

where

\begin{equation} 
  -\infty = \tau_0 < \tau_1 < \tau_2 < \dots < \tau_{m-1} < \tau_m = \infty
  (\#eq:ordvar2)
\end{equation}

The $\tau$ terms are unknown population parameters. These arbitrary threshold values may be estimated in some circumstances, such as from percentage counts of responses $z$ when $z^*$ is assumed to be normally distributed.

When analyzing individual items of Likert and similar scales the debates are even more polarized. @joreskog2005structural takes the firm position that it is wrong even to use means, variances and covariances. The reasons given are that the integer values do not represent a continuous scale. They should not be attributed metric properties as they have neither an origin nor units. @jamieson2004likert goes even further, stating that it is also wrong to presume that intervals between categories are equal. There is no assumption in \@ref(eq:ordvar2) that the $\tau$ terms follow a regular pattern. In practice, this means that the barrier to reach the highly polarised strongly disagree or strongly agree may be inconsistent with a move from neutral to either agree or disagree. Also, these types of scales are often skewed or multimodal and require non-parametric tests such as Mann-Whitney-Wilcoxon (MWW) for analysis. @gliem2003calculating asserts that these data types are intended to be used as a combined scale measuring a latent concept, and asserts that their individual analysis leads to erroneous conclusions. The evidence for this assertion is fairly strong, but potentially flawed. In the paper, an experiment is conducted in which respondents are asked to respond to the same single item a second time after a three week interval. The retest reliability coefficient turns out to be very low at 0.11. This result is compared to a well designed, Likert based survey. A possible short-coming of this experimental design is the fact that when asked a single question, respondents may react very differently to when they are presented with a ten-item questionnaire. Respondents may be prone to providing trivial responses to a single question based on mood alone, rather than weighing up the varied statements of the multi-item scale. A differently designed experiment might provide a number of unrelated items, simulating a Likert based survey.

Other authors [@norman2010likert; @carifio2008resolving] strongly rebut these arguments, citing various sources of empirical evidence of the robustness of parametric tests. @de2010five compares the type I and type II error rates of the t-test against the MWW test using synthetic data which finds that there is no benefit to either test with respect to type I errors and slight differences in performance with type II error rates, and consequently the power of the test. The direction of these differences depends on the underlying populations and may favour t-tests in some cases and MWW in others. Crucially, this article only tests one (synthetic) variable at a time, so while it is useful in the context of individual ordinal variables, it does little to settle the debate on Likert scales.

Valuable contributions to the discussion come from authors and statistical researchers who specialize in discrete data analysis. Among these, the *proportional odds model* (also known as *cumulative logit model* and *ordered logit model*) is favoured for statistical analysis where the dependent variable is ordinal [@friendly2016discrete; @R-ordinal; @christensen2010ordinal; @agresti2013categorical]. *Penalized likelihood estimation* and *penalized regression* are favoured where the predictors are ordinal [@R-ordPens; @gertheiss2009penalized]. Tests based on these models are shown to perform better than models based on either linear assumptions [@gertheiss2011testing] and predictions will always fall within the range of possible responses without the need for rounding, truncation, approximation etc, @christensen2010ordinal.

Fortunately, there is agreement that combining individual responses into their intended scale leads to the least controversial usage and analytical or probabilistic techniques over single items in a Likert scale should be avoided where possible. Cronbach's alpha is the conventionally used statistic for measuring reliability, although even on this topic there has been some contention over acceptable sample size. Sample sizes of less than 300 were not advised although more recently the test was found to be robust down to sample sizes of 30 on condition that the first eigenvalue is greater than 6. Also, the general rule of thumb that an alpha of 0.7 or greater is acceptable should be treated with caution for scales greater than six individual items, [@kline1986handbook; @yurdugul2008minimum; @cortina1993coefficient] cited in @cas2016statistical. In addition to Cronbach's alpha, Exploratory Factor Analysis (EFA) has become more common in recent years for reliability analysis and validation of untested scales and McDonald's omega is more reliable when inter-item correlations can't be assumed constant. This last assumption is probably true in practice far more frequently than not.

This discussion has been detailed and extensive for good reason. Correct and appropriate analysis of ordinal and Likert scales is essential for the success of this research project. A set of reliable tests is required to determine whether imputation has had a detrimental effect on the statistical properties of target variables.

## Non-Response and Missingness

>  "Respondents do not answer every question, countries do not collect statistics every year, archives are incomplete, subjects drop out of panels."

@honaker2011amelia [p.1]

The practicalities of running surveys means that they regularly suffer the problem of non-response and missing data [@plumpton2016multiple; @bono2007missing; @kamakura2000factor]. In the broadest terms, non-response can occur at two levels: Unit and Item. Unit non-response occurs when one or more respondents fail to return anything to the survey organisers. This may be a one-off survey, or may be one in a series of surveys in a longitudinal study. If the non-response rate is high, this can impact the overall sampling frame. Item non-response occurs when individual items (questions) in a survey are skipped, but the whole survey is returned to the organisers. Both types of non-response have serious consequences. However, this work is concerned only with item non-response which will be the focus for the remainder of the review, unless otherwise stated. Understanding the model (patterns, mechanism and magnitude) of missingness in data is critical when selecting or designing techniques to recover from the resulting issues. Fortunately, there has been an enormous amount of research in this area over recent decades.

As always, routine exploratory analysis is the best starting point. For example, when examining the effect of survey length on response rates, @sheehan2001mail reports numerous prior studies that illustrate how longer surveys tend to reduce overall (unit) response rates, particularly for business-oriented surveys. Only one example is given of a study that found the opposite pattern but it is unclear if this is enough to call the overall trend into question. Crucially, there is no discussion of the implication of survey length on any pattern of item non-response and specifically whether longer surveys suffer more incomplete items towards the end than near the beginning. From the historical outline given in @groves2011three, this must be assumed to be the case, given that the focus shifted to shorter questionnaires to reduce hang-up rates at the same time as an increase in popularity of surveys by telephone. A ragged tail pattern of non-response would be a clear indicator of a survey that respondents found overly long.

```{r nonresp-reasons}
reason <- c("Failure to complete the whole questionnaire"
          , "Failure to complete the whole questionnaire"
          , "Unwillingness to answer"
          , "Unwillingness to answer"
          , "Data entry errors"
          , "Data entry errors"
          , "Structural design of questionnaire"
          , "Structural design of questionnaire")
cause <- c("Absent-mindedness"
          , "Skipping a page in error"
          , "Personal issues: income, status, health"
          , "Perceived not relevant to study: drug use, sexual orientation"
          , "Manual conversion of paper forms"
          , "Using incorrect marking of papers e.g. check instead of filled box"
          , "Some questions only apply depending on previous answers"
          , "Poorly designed e.g. too long"
           )
reasonstable <- data.frame(Reason = reason
                  , "Examples" = cause)
knitr::kable(reasonstable
             , caption = "Reasons for item non-response")
```

Table \@ref(tab:nonresp-reasons) is shows a non-exhaustive list of examples of reasons for item non-response. In a single survey instance, more than one such reason could be at play for different items. These different underlying models have various consequences and researchers may have no way of knowing for sure what are the mechanisms behind missingness in the data.

```{r nonresp-patterns}
pattern <- c("Univariate"
             , "Multivarate"
             , "Ragged tail"
             , "Missing blocks"
             , "Missing blocks"
             , "Missing regular")
explanation <- c("Only one item has missing responses"
                 , "Missing responses throughout, seemingly at random"
                 , "Missing all after a certain question"
                 , "Groups of respondents reply to different questions"
                 , "Groups of respondents reply to different questions"
                 , "Questions skipped depending on prior responses")
posscause <- c("Badly worded or irrelevant question"
               , "Any"
               , "Survey is too long"
               , "Data fusion: different surveys combined"
               , "A subsample participate in follow ups"
               , "Structural constraints")
patternstable <- data.frame(Pattern = pattern
                            , Explanation = explanation
                            , Possible_Cause = posscause)

knitr::kable(patternstable
             , caption = "Patterns of non-response"
             , booktabs = TRUE)
```

Table \@ref(tab:nonresp-patterns) and figures \@ref(fig:univmissingness), \@ref(fig:multmissingness), \@ref(fig:mtailmissingness), \@ref(fig:dfusmissingness), \@ref(fig:sbsmmissingness) and \@ref(fig:stdgmissingness) are adapted from @kamakura2000factor to illustrate patterns of both unintentional (1-3) and intentional (4-6) non-response in surveys. These also demonstrate the usefulness of the missing values map, @unwin1996interactive. This is a simple, exploratory plot of a missing indicator matrix. This is a binary matrix representing presence or absence of a value for each respondent and item. Some articles [@he2010missing] recommend using a one to represent a missing value and a zero for present values as this reflects binomial success of the non-response process. However, other authors [@gelman2006data] put forward cases where the reverse makes sense. Clearly, it can be argued that this choice does not matter, so long as the research method is implemented with clarity.

```{r univmissingness, fig.height=2, fig.width=2, fig.cap="Missing Values Map showing univariate missingness"}
set.seed(101)
qnnaire <- matrix(rep(1, 2000), nrow = 100, ncol = 20)

umsng <- rbinom(100, 1, 1 - 0.2)
univ <- qnnaire
univ[, 13] <- umsng

par(mar = c(1, 1, 1, 1)
    , mgp = c(0, 0, 0)
    , cex.main = 0.8)

image(t(univ)
      , main = "Univariate Pattern"
      , xlab = "Item"
      , ylab = "Respondent"
      , xaxt = "n"
      , yaxt = "n"
      , col = c("ivory", "darkslateblue"))
```

```{r multmissingness, fig.height=2, fig.width=2, fig.cap="Missing Values Map showing multivariate missingness"}
mult <- matrix(rbinom(2000, 1, 1 - 0.05), nrow = 100, ncol = 20)

par(mar = c(1, 1, 1, 1)
    , mgp = c(0, 0, 0)
    , cex.main = 0.8)
  
image(t(mult)
      , main = "Multivariate Pattern"
      , xlab = "Item"
      , ylab = "Respondent"
      , xaxt = "n"
      , yaxt = "n"
      , col = c("ivory", "darkslateblue"))
```

```{r mtailmissingness, fig.height=2, fig.width=2, fig.cap="Missing Values Map showing ragged tail missingness"}
mult <- matrix(rbinom(2000, 1, 1 - 0.05), nrow = 100, ncol = 20)

mtail <- qnnaire
mtmsng <- rbinom(100, 1, 1 - 0.1)
mtrow <- which(mtmsng == 0)
mtcount <- 100 - sum(mtmsng)
mtcol <- round(runif(mtcount, 15, 20))

for (i in seq_along(mtrow)) {
  mtail[mtrow[i], mtcol[i]:20] <- 0  
}

par(mar = c(1, 1, 1, 1)
    , mgp = c(0, 0, 0)
    , cex.main = 0.8)
  
image(t(mtail)
      , main = "Ragged Tail Pattern"
      , xlab = "Item"
      , ylab = "Respondent"
      , xaxt = "n"
      , yaxt = "n"
      , col = c("ivory", "darkslateblue"))
```

```{r dfusmissingness, fig.height=2, fig.width=2, fig.cap="Missing Values Map showing data fusion missingness"}
dfus <- qnnaire
dfus[1:50, 11:15] <- 0
dfus[51:100, 6:10] <- 0

par(mar = c(1, 1, 1, 1)
    , mgp = c(0, 0, 0)
    , cex.main = 0.8)

image(t(dfus)
      , main = "Data Fusion Pattern"
      , xlab = "Item"
      , ylab = "Respondent"
      , xaxt = "n"
      , yaxt = "n"
      , col = c("ivory", "darkslateblue"))
```

```{r sbsmmissingness, fig.height=2, fig.width=2, fig.cap="Missing Values Map showing subsample missingness"}
sbsm <- qnnaire
sbsm[26:100, 15:20] <- 0

par(mar = c(1, 1, 1, 1)
    , mgp = c(0, 0, 0)
    , cex.main = 0.8)

image(t(sbsm)
      , main = "Subsample Pattern"
      , xlab = "Item"
      , ylab = "Respondent"
      , xaxt = "n"
      , yaxt = "n"
      , col = c("ivory", "darkslateblue"))
```

```{r stdgmissingness, fig.height=2, fig.width=2, fig.cap="Missing Values Map showing structural design missingness"}
stdg <- qnnaire
stdg_skip1 <- which(rbinom(100, 1, 1 - 0.1) == 0)
stdg_skip2 <- which(rbinom(100, 1, 1 - 0.2) == 0)
stdg[stdg_skip1, 5:6] <- 0
stdg[stdg_skip2, 10:13] <- 0

par(mar = c(1, 1, 1, 1)
    , mgp = c(0, 0, 0)
    , cex.main = 0.8)

image(t(stdg)
      , main = "Structural Pattern"
      , xlab = "Item"
      , ylab = "Respondent"
      , xaxt = "n"
      , yaxt = "n"
      , col = c("ivory", "darkslateblue"))
```

@joreskog2005structural provides and alternative missing data map in a collapsed form showing all possible patterns. Here a missing indicator matrix with a 1 indicating missingness is used. Note that eleven cases had five or more out of six responses missing and would be useless in any analysis. This format is of most use when there aren't too many items to create a combinatorial explosion. In fact, in the article a subset of just six items is used, which represent the items of a single multiple-item response variable. Some questionnaires may run up to hundreds of individual items, including datasets used in this research project so the format may be too unwieldy, unless this is done separately for each multiple-item response. Another question to consider is what sorting criteria did the author use to present this information as it is not obvious from the layout shown, which has been reproduced exactly in Table \@ref(tab:missmapclpse).

```{r missmapclpse}


Frequency <- c(1554, 16, 12, 1, 4, 11, 31, 1, 2, 1, 4, 1, 32, 1, 1, 1, 5, 2, 1, 1, 14, 4, 4, 2, 1, 1, 2, 9)
Pattern <- c("000000"
             , "100000"
             , "010000"
             , "110000"
             , "001000"
             , "000100"
             , "000010"
             , "010010"
             , "110010"
             , "011010"
             , "000110"
             , "001110"
             , "000001"
             , "010001"
             , "110001"
             , "101001"
             , "000101"
             , "100101"
             , "001101"
             , "101101"
             , "000011"
             , "100011"
             , "010011"
             , "110011"
             , "011011"
             , "000111"
             , "011111"
             , "111111")
missingtable <- data.frame(Frequency = Frequency, Pattern = Pattern)
knitr::kable(missingtable
             , caption = "Missing data map, compact form. Source: Jöreskog (2009)"
             , booktabs = TRUE)

```

Secondary citation of Roth 1994 in @plumpton2016multiple that conspicuously little research done on missing data analysis, despite importance in the social sciences, and there is still a gap in practice with what is recommended in the literature with 45% of papers surveyd using complete case analysis and only 8% using MI @plumpton2016multiple secondary citation of Bell ML, Fiero M, Horton NJ, et al. Donald B. Rubin is widely cited for seminal works on modeling, recovering (imputing) and inference using missing data e.g. @rubin2004multiple. Of particular importance in this body of work is formalization of the relationship between the missing values, the incomplete variable and the other variables. The patterns arising from these relationships are categorized as:

* Missing completely at random (MCAR) if process that causes the missing data is distinct from any parameter $\theta$ of the data. In other words the distribution of missing data is unrelated to the value of any variable in the dataset. It is also suggested in @gelman2006data that the probability of non-response is the same for all points of the missing indicator matrix. The missing/non-missing state of individual data points cannot be predicted from any information whatever, whether observed or unobserved other than the overall missingness rate.

* Missing at random (MAR) is the most common scenario according to @schafer1997analysis and applies if the distribution of missing data may be related to available information. That is, other variables in the dataset but not the variable itself. The mechanism is correlated with the observed data according to @gelman2006data, it is often reasonable to model this as a logistic regression. The article continues to assert that it is generally not possible to be certain even of an MAR assumption because a correlation with unobserved data or a latent variable can never be ruled out altogether.

* Missing not at random (MNAR) if the distribution of missing data is related to data that is unobserved. @gelman2006data further sub-divides this category in two. Firstly, when missingness depends on unobserved predictors, for example, dissatisfied customers being less likely to respond than satisfied customers would generate a MNAR unit non-response. An example of an MNAR unit non-response might be a survey where people in a particular education bracket responded less frequently to questions about income where there were no questions identifying the level of educational attainment. The second sub-division is where the distribution of missing data is related to the value of the partially missing variable itself, such that answers given by complete respondents differ substantially from answers that would have been given by non-respondents. For example, if respondents from a lower income bracket are reluctant to respond to questions about earnings. This is the most challenging situation to recover and of most concern to researchers. In fact, @tsikriktsis2005review states that there are no statistical means to recover when missing data is MNAR and no way to ameliorate the potential bias in results based on analysis of such data. While it is not explicit in the article, @tsikriktsis2005review must be referring to this second sub-division of the category. @gelman2006data contradicts this assertion by suggesting that the problem can be mitigated by adding more predictors. However, this assumes that more data is available when it would be expected that most researchers would provide all the data they have. 

In a historic article, @rubin1976inference as cited in @wu2015comparison lays the foundation for these definitions, which are now used throughout the literature, and deteremines that ignoring the mechanism of non-response is the proper procedure when, and only when, the data is MCAR. Missingness in MAR cases is "ignorable" in regression models only when controlling for all the variables that correlate with non-response. This is an important milestone in statistics discourse and after this time, references to these assumptions and the terms "ignorable" and "not ignorable" (with respect to non-response) are seen in results analysis generally. Furthermore, @gelman2006data also suggests that most missingness is not MCAR. This means that most missingness is not ignorable, which is a crucially important point for any statistical analysis. It is vital to make clear statements of assumptions made about missingness in data and its treatment prior to releasing results. @white2011multiple secondary citation of Sterne et al. suggest reporting guidelines that include careful comparison of MI results with the results of complete-case analysis [46].  @plumpton2016multiple secondary citation of Bell ML, Fiero M, Horton NJ, et al. says this doesn't happen as much as it should. Up to 66% of papers surveyed in psychology there was an implication that missing data was present but no mention how it was handled.

There is an abundance of examples in the literature of the adverse effects of missingness in survey data. These include biased parameter estimates (central tendency, dispersion and correlation coefficients) and loss of statistical power [@madow1983incomplete; @bean1995long; @roth1999missing; @raaijmakers1999effectiveness]. **Check if these discuss the recovery method used**

Carpita has some useful stuff on models of missing data

@white2011multiple quoted  Inadequate handling of the missing data in a statistical analysis can lead to biased and/or inefficient estimates of parameters such as means or regression coefficients, and biased standard errors resulting in incorrect confidence intervals and significance tests. In all statistical analyses, some assumptions are made about the missing data. Little and Rubin’s framework [1] is often used to classify the missing data as being (i) missing completely at random (MCAR—the probability of data being missing does not depend on the observed or unobserved data), (ii) missing at random (MAR—the probability of data being missing does not depend on the unobserved data, conditional on the observed data) or (iii) missing not at random (MNAR—the probability of data being missing does depend on the unobserved data, conditional on the observed data). For example, blood pressure data are MAR if older individuals are more likely to have their blood pressure recorded (and age is included in the analysis), but they are MNAR if individuals with high blood pressures are more likely to have their blood pressure recorded than other individuals of the same age. It is not possible to distinguish between MAR and MNAR from the observed data alone, although the MAR assumption can be made more plausible by collecting more explanatory variables and including them in the analysis.

10. facotr analysis method
500. plausible values
has some stuff on missing data simul.

## Techniques for Recovering from Missingness

> "It is the duty of researchers and analysts to firstly minimise the extent of missing data by ensuring appropriate methods for enhancing data capture are implemented, but also to handle missingness in a way best suited to the data and research question."

@plumpton2016multiple [p.13]

The author is unequivocal on the first point, and while it is stating the obvious that prevention is always better than cure, it does bear repeating. The second point provides support for the motivation behind this research. Analytical and processing techniques should be carefully chosen to meet the challenges of a given situation. The prevalence of surveys which collect mostly categorical data calls for methods better suited to data with these characteristics. 

The problems with missing data are many and of great concern to researchers. The problems are reputational as well as technical. @plumpton2016multiple also reports that "poor handling and reporting of missing data may result in misleading conclusions and are one of the main reasons for publication rejections." @gelman2006data provides a very comprehensive treatment of the topic which serves as the basis for much of this section. The authors describe non-response in a social indicators survey as "a distraction to our main goal of studying trends in attitudes and economic conditions, and we would like to simply clean the dataset so it could be analyzed as if there were no missingness" @gelman2006data [p.529]. This  statement perfectly sums up the motivation for missing data analysis and recovery per se illustrates why many techniques have been developed. Researchers want to analyze their data with confidence and statistical rigour, despite the prevalence of non-response.

Many statistical functions and operations simply do not work with missing data, whether calculated manually or by software. Many software systems (e.g. R, SAS, SPSS), sometimes with a warning and sometimes silently, will perform a list-wise deletion to automatically exclude cases with *any* missing values in the variables and analyze only the complete cases. If there is only one missing value out of tens or hundreds of potential predictor variables, then much valuable information is lost. Even in the era of "Big Data," where TeraBytes may be trawled for accidental insight, a well-designed survey may still have a high cost per respondent may still be high due to the discipline of sampling and selection. Rather than simply allowing statistical software to make default, often inappropriate choices, it is advisable to carry out a formal missing data analysis. Then, based on these findings, to implement some other form of pre-processing that recovers a dataset. The method chosen will have a significant impact on the reliability of any subsequent analysis. 

@carpita2011imputation describes four categories of recovery procedures as follows:

1. Ignore or omit
1. Weighting procedures
1. Model-based procedures
1. Imputation procedures

Each method has advantages and disadvantages. For example, (1) includes list-wise deletion which is simplest to implement but potentially introduces significant problems. @bono2007missing sums up advice which is well-supported throughout the literature; Although list-wise deletion is a default option, is only appropriate for data which is MCAR and the proportion of missing data is small. However, this instruction will not hold if the dataset is wide (having many variables). In this case, even a low proportion of missing data could lead to a very high number of rows being discarded as the missing data are dispersed throughout the dataset. @plumpton2016multiple imposes a further condition; that the missingness is only found in the dependent variable. Although this was not specifically mentioned in any of the other texts reviewed it certainly caps a plethora of significant limitations with the deletion method. The only article found to encouraged list-wise deletion under specific circumstances other than those listed was @joreskog2005structural, which offers the common sense advice that list-wise deletion is recommended for cases with all or most responses missing. Obviously these individual data rows would add little value to any statistical model. In practice, the MCAR conditions themselves are rarely provable. (1) also includes simply omitting a data point from an aggregate or multiple-item scale. This will obviously result in an underestimated score on the latent variable and would probably never be recommended up to date research methodologies.

Methods in category (2) can be applied after list-wise deletion to rebalance a sample and can even be used to deal with unit non-response. Essentially remaining cases are weighted or duplicated so the desired sampling and stratification proportions are restored. @groves2011three points out that the theoretical assumptions for this method to be valid could never be met and @gelman2006data posits that weighting becomes too complicated if more than one variable contains non-response. Indeed support for these older methods is either thin or non-existent in the contemporary literature. A modification of this method is called available case analysis. It uses subsampling of the complete cases to answer different research questions. This has the problem that analyses may be based on inconsistent subsamples, representing potentially different populations. Given all the shortcomings associated with complete case analysis and following the emergence of ubiquitous computing power to run more sophisticated methods, it is not surprising they have fallen out of favour. Nevertheless, some authors [@plumpton2016multiple] **others?** suggest that routine applications of these methods persist.

@carpita2011imputation introduces category (3) but only in passing and gives Expectation Maximization (EM) and data augmentations (the Bayesian counterpart of EM) as examples. @gelman2006data [p.540] also only briefly describes (3) as a process in which a "multivariate model is required, and this can be particularly tricky when some of the variables are discrete" which also implies EM, but then unexpectedly associates this with non-ignorable missing data models, in which domain knowledge is used to model the mechanism of missingess. This latter point is somewhat inconsistent. EM algorithms form the basis of multiple imputation techniques which will be discussed shortly, so perhaps this rather brief treatment of (3) indicates that it is a foundational concept for a more sophisticated technique. The remainder of this subsection with deal with (4).

@bono2007missing explains that imputations strategies replace missing data points with substitute values that are estimated from patterns or relationships found in the observed data. This means the full sample can be used for analysis. Many methods are well developed for continuous variables, but sources indicate that there has been much less research for datasets comprising categorical and ordinal variables, [@finch2010imputation; @leite2010performance]. Furthermore, there is strong evidence of significant differences in the performance of various imputation strategies over datasets exhibiting different underlying characteristics [@wu2015comparison}; @rodwell2014comparison; @sim2015missing]. These experimental results indicate a need to take into account not only the model and magnitude of missingness but also other characteristics particular to the target data. For this reason there is real benefit in having a range of imputation methods from which to choose the most suitable in a given situation. **anything wrong with those experiments?**

There are many different kinds of imputation strategies and different ways to categorize them:

* Single (or point) versus Multiple Imputation
* Parametric versus non-parametric Imputation
* Simple versus predictive

Of these, the most important distinction is probably Single versus Multiple.  

### Single Imputation

Single imputation strategies are often quick and simple to implement but lead to biased parameter estimates. This can happen for two reasons. Firstly, because the analysis is done as if the simulated value is a true, observed value. Secondly, depending on the techniques, the choice of substitute value can affect the statistical properties of the imputed variable. Consider a univariate normal scenario where all missing values are replaced with the mean of the observed data. While this action preserves the mean of the random variable, the variance is depressed. This is demonstrated by way of a Monte Carlo simulation, following the methodology given in @robert2009introducing:

```{r sim_grandmean_imp}
set.seed(1)
n <- 1000
m <- 100
p <- 0.15
k <- 4
varx <- numeric(n)
varxmis <- numeric(n)
varximp <- numeric(n)
meanx <- numeric(n)
meanxmis <- numeric(n)
meanximp <- numeric(n)

x <- rnorm(m, mean = 10, sd = 2)

d <- list()
for (i in 1:n) {
  x_star <- sample(x, m, replace = TRUE)
  
  nonresp <- rbinom(m, size = 1, prob = p)
  x_mis <- ifelse(nonresp == 1
                  , NA
                  , x_star)
  x_imp <- ifelse(is.na(x_mis)
                  , mean(x_mis, na.rm = TRUE)
                  , x_mis)
  if ((i / n * k) %in% 1:k) {
    d[[i / n * k]] <- densityplot(~x_star+x_mis+x_imp
                                  , plot.points = FALSE
                                  , main = "Density before and after mean imputation"
                                  , xlab = "Bootstrap samples 250, 500, 750 and 1000"
                                  , auto.key = list(columns = 2)
                                  , par.settings = MyLatticeTheme
                                  , scales = MyLatticeScale)  
  }
  
  meanx[i] <- mean(x_star)
  meanxmis[i] <- mean(x_mis, na.rm = TRUE)
  meanximp[i] <- mean(x_imp)  
  varx[i] <- var(x_star)
  varxmis[i] <- var(x_mis, na.rm = TRUE)
  varximp[i] <- var(x_imp)
}

function1 <- mean
function2 <- function(x) {
  quantile(x, c(0.025, 0.975))
}

mcmc_results <- function(x) {
  t(rbind(
    data.frame(
      t(sapply(x, function1))
      , row.names = "Emp.mean")
    , sapply(x, function2)
  )[c(2, 1, 3),])
}

means <- list(sim_mean = meanx
              , mis_mean = meanxmis
              , imp_mean = meanximp)
vars <- list(sim_var = varx
     , mis_var = varxmis
     , imp_var = varximp)
```

1. A random variable $X = x_1...x_{100},\ X \sim \mathcal{N}(10, 2)$ is generated.
1. $N =$ `r n` bootstrap samples $X^*$ are taken.
1. For each $X^*$, $X^*_{mis}$ is generated by setting `r p * 100`% of values to missing by a coin flip (Bernoulli) function.
1. $X^*_{imp}$ is generated by replacing missing values with the mean of $X^*_{mis}$.
1. The empirical means and variances of $X^*$, $X^*_{mis}$ and $X^*_{imp}$ are returned with their 95% credible intervals.


```{r sim-grandmean-imp-means}
knitr::kable(mcmc_results(means)
            , caption = "Empirical mean of X, X with missing data and imputed X"
            , booktabs = TRUE)
```
```{r sim-grandmean-imp-vars}
knitr::kable(mcmc_results(vars)
            , caption = "Empirical variance of X, X with missing data and imputed X"
            , booktabs = TRUE)
```
```{r sim-grandmean-imp-plot, fig.cap="Monte Carlo simulation of biased variance estimate after mean imputation"}
c(d[[1]], d[[2]], d[[3]], d[[4]])
```

This informative example shows how the mean and variance of the MCAR data is almost identical to the underlying distribution. Using mean imputation gives a mean which is identical to the MCAR data to many decimal places, as can be seen in Table \@ref(tab:sim-grandmean-imp-means). However, the variance is much reduced in the imputed dataset, Table \@ref(tab:sim-grandmean-imp-vars). Density plots of selected bootstrap iterations show very clearly what has happened \@ref(fig:sim-grandmean-imp-plot). Not shown here, but also notable, is that bivariate correlations would also be artificially reduced by using a constant value to replace missing data.

Mean imputation, or any other technique that relies only on a measure of central tendency is too simplistic an approach for many missing data problems. Aside from the issues demonstrated here, the method takes no information from other variables if they are available. If data are MAR, then a dependency between the missingness and the observed data has been established and should be exploited. Other simple, parametric imputation methods that do so include group mean imputation, where the mean stratified over one of more categorical variable is used. Predictive mean imputation is better still, generating suitable values from a predictive model (usually linear, or whatever is appropriate for the data).

```{r sim_predmean_imp}
set.seed(2)
n <- 1000
m <- 250
p <- 0.20
k <- 4
corxy <- numeric(n)
corxyimp <- numeric(n)

x <- rnorm(m, mean = 10, sd = 2)
eps <- rnorm(m, mean = 0, sd = 1)
y <- 3 + x * 0.5 + eps

fitxy <- lm(y ~ x)
errs <- fitxy$residuals

MyTempTheme <- MyLatticeTheme
MyTempTheme$superpose.symbol$col <- myPalContrasts[c(5, 3)]

d <- list()
for (i in 1:n) {
  y_star <- y + sample(errs
                      , size = m
                      , replace = TRUE)
  corxy[i] <- cor(x, y_star)
  
  nonresp <- rbinom(m, size = 1, prob = p)

  fit_star <- lm(y_star[nonresp == 0] ~ x[nonresp == 0])
  y_imp <- y_star
  y_pred <- predict(fitxy
                    , newdata =
                      data.frame(x =
                                   x[nonresp == 1]))
  y_imp[nonresp == 1] <- y_pred
  corxyimp[i] <- cor(x, y_imp)
  
  if ((i / n * k) %in% 1:k) {
    d[[i / n * k]] <- 
      xyplot(y_star + y_imp ~ x
             , pch = c(1, 19)
             , cex = c(0.8, 0.6)
             , alpha = c(1, 0.3)
             , panel = function(x, y, ...) {
               panel.xyplot(x, y, ...)
               panel.xyplot(x[nonresp == 1]
                           , y_pred)
             }
             , main = "Scatter plot before and after predictive mean imputation"
             , xlab = "Bootstrap samples 250, 500, 750 and 1000"
             , ylab = "Bootstrapped and Imputed Y"
             , auto.key = list(columns = 2)
             , par.settings = MyLatticeTheme
             , scales = MyLatticeScale)  
  }
}
corrs <- list(sim_corr = corxy
              , imp_corr = corxyimp)
```

```{r sim-predmean-imp-corrs}
knitr::kable(mcmc_results(corrs)
            , caption = "Empirical correlation of X with Y and imputed Y"
            , booktabs = TRUE)
```
```{r sim-predmean-imp-plot, fig.cap="Monte Carlo simulation of biased correlation estimate after predictive mean imputation. Original values in open circles, fuchsia colour. The imputed dataset shares most of the same values in filled circles, green colour, de-emphasised with transparency. Predicted values have been emphasised in darker green."}
c(d[[1]], d[[2]], d[[3]], d[[4]])
```

Again, it is very informative to run a simulation of the predictive mean imputation technique to understand its shortcomings. Following again the methodology given in @robert2009introducing:

1. A random variable $X = x_1...x_{250},\ X \sim \mathcal{N}(10, 2)$ is generated.
1. A linearly dependent variable $Y$ is generated using the linear equation $Y = \beta_0 + \beta_1X + e, \ \beta_0 = 3, \ \beta_1 = 0.5, e \sim \mathcal{N}(0, 1)$
1. An OLS regression is performed on $Y \sim X$ and the residuals are saved as $err$.
1. $N =$ `r n` samples $Y^*$ are generated by bootstrapping $err$ and adding the result to $Y$.
1. For each $Y^*$, $Y^*_{mis}$ is generated by setting `r p * 100`% of values to missing by a coin flip (Bernoulli) function.
1. A new OLS regression is performed on $Y^*_{mis} \sim X$
1. $Y^*_{imp}$ is generated by predicting new values from $X$ where $Y^*_{mis} \notin Y^*$
1. The empirical correlations $Cor(X, Y^*)$ and $Cor(X, Y^*_{imp})$ are returned with their 95% credible intervals.

The results in Table \@ref(tab:sim-predmean-imp-corrs) show a systematic over-estimate of bivariate correlation because all the imputed values fall directly on the regression line.

The examples serve to demonstrate why imputation is quite a different process to prediction. The purpose of imputation is not to achieve a measure of accuracy or smoothing, but instead to faithfully represent a distribution. As such, methods that estimate a posterior distribution of the parameters of the observed data have grown in popularity

In contrast to grand mean, group mean or predictive mean imputation, simple (naive) random imputation is the process of sample with replacement values from the observed data but ignores any useful information from the other variables. Hence methods have been developed that that do take into account information from all available variables. For example, methods that use multivariate normal models to infer sampling distributions for imputation. @gelman2006data describes this as a complicated task, especially where the missingness is multivariate. This requires an iterative process, in which some starting value is set for all missing values, and new values are imputed one variable at a time using all other values (initialised and complete). When all variables have been imputed, this completes a cycle, and the process iterates until convergence. These methods become more and more complex when modeling non-normal multivariate data, e.g. stratified, clustered, hierarchical etc. Such automated models must be checked for convergence and validity before the results can be used in further analysis.

Can use zero coding, top/bottom coding to reduce sensitivity to extreme values and improve the predictive power of the model. A simple cutoff or a transform above a cutoff. Linear model prediction increases central tendency - see @gelman2006data for equations why based on R squared page 536. Random prediction imputation, for univariate normal data, or data which has been transformed to be so, can create a random normal vector from the vector of pred values (means) and resid st.devs as the st.dev so the scatter is defived from the unexplained variance in the model. Transforms and multi-stage imputations required for non-normal data (counts, zero-inflated modes and so on.)

There are also non-parametric alternatives of single imputation. Many are best suited for specific situations. For example, "last value carried forward" is only applicable in longitudinal, or time-based surveys. It can result in erratic reeadings for trends analysis. For missing data in a nominal variable it is sometimes acceptable simply to add a new category, "missing." Logical rules, survey structure or domain knowledge can be applied. For example, a respondent reporting low or moderate drinking might have the number of alcohol units per week imputed if missing. Similarly, someone reporting they worked zero hours per week might be also assigned an income of zero.

The most important non-parametric imputation methods are generally referred to as "similar pattern response matching" [@joreskog2005structural p.3] and also Hot-deck imputation. @gelman2006data seems to refer to Hot-deck and matching interchangeably, which seems reasonable given the descriptions of these elsewhere in the literature. @joreskog2005structural defines the intuition behind matching imputations as follows; If an individual with missing variable *x* has same response pattern in other variables as another case, then it's likely that variable *x* also matches. The article goes on to say that if multiple individuals are found with the matching response pattern then that's even stronger evidence for value *x*. @gelman2006data [p.538] describes the process as a "non-parametric version of regression" but it is not clear what the grounds are for this description. Certainly the method is useful where (linear) regression is challenging.

It should be noted that Cold-deck imputation is also mentioned in the literature, but only as a brief aside [@bono2007missing; ]  ** more ** In this process, substitute values are found by refernece to prior studies, not the same dataset where the missing values are found. Perhaps it  is not discussed much because the literature generally focuses on statistical and computational techniques. Or perhaps it is simply not considered to be important or useful. This is not made clear.

The detail of Hot-deck imputation is in the way matches are made and donor cases found. Similarity between individual cases can be based a distance function, such as Euclidean distance. In such cases, there would be an assumption that the matching variables would be numeric scales, where regression models presumably would also work. In such cases, the question of how to choose the right method becomes more nuanced.

 is suitable for continuous and MCAR and secondary citation Brown 1994 says compares well to listwise deletion. 

Importantly, shows that missing values can remain after this method and should be removed by listwise deletion. This point corroborates what this current research is hoping to achieve. The intention of this research is to go a step further and find all the similarity patterns.

on a scoring functionAnother way is to use other variable to create a score for propensity to missingness and then use these values to impute missing. This one doesn't quite make sense and one has to assume it means use propensity score to find the donor cases to then impute missing.







@shrive2006dealing as example of a "self-report" ordinal scale where multiple impute worked best, even for very high rates of missingness. Also one missing response in the whole instrument - makes it a real waste to use listwise deletion.

@he2010missing stat on how many records lost in listwise deletion and good comparison of technique



@plumpton2016multiple Complete case analysis is appropriate for MCAR but not if missing in covariates or parts of a composite outcome. If MCAR assumption does not hold, the complete case data will not be representative of the underlying population.

### Mulitple Imputation (MI)

@graham2007many states that MI and FIML (full information maximum likelihood) are the two most common techniques used with missing data, but FIML is far more computationally expensive. @white2011multiple explains how MI uses EM to generate a multivariate normal model, in the form of a posterior distribution of the model parameters conditional on the observed data. Random draws are taken from this distribution to create a predictive model (usually a linear regression) which is then used to predict values for the missing data in the dataset, one variable at a time. The imputed values are introduced to the predictive model as it progresses over each variable. The predictions are repeated iteratively over each variable until the model converges or the maximum number of iterations is expended. The random draw and iterative prediction process are repeated m times, to create m completed datasets, differing only in the values of the substituted variables. Statistical and other analysis can be performed on the m imputations, and the results pooled. The variability in the imputed items introduces uncertainty into the analysis and the variance in estimates attributable to the missing data to be easily calculated.

The name multiple imputation actually refers to the m imputations, but the literature contains many variations, referring sometimes to the EM algorithm, the multivariate model, the possibility of imputing more than one missing variable or the iterative process used to converge each of the m imputations. It is easy to see why the literature is inconsistent on this matter!

The theory established by @rubin2004multiple cited in @graham2007many and @schafer1997analysis cited in @graham2007many suggest m = 3-5 is enough imputations. These articles are cited very widely and are conferred a great deal of authority by other authors. Nevertheless, there are some disagreements; @wu2005role states that there is no support in the literature for the number 5 and that one imputation is often enough. In stark contrast, @graham2007many posits that 3-5 is nowhere near enough and @white2011multiple develops this further to suggests 1 imputation per 1% of missing data provides the same rule of thumb arrived at through other logic (go back and get precise) so it should be concluded that this is a very valuable method. With the power of modern computing today, it seems unnecessary to stick to a bare minimum number of runs. These days, it is probably trivial to increase m for all but the largest datasets, and there are no counter-indications for doing so.

@plumpton2016multiple MI has taken a long time to gain in popularity despite the proven benefits. But there are specific issues with applying it to surveys with multi-item scales. Likert scales are not specifically mentioned but it is safe to assume this specific case is covered by the arguments given.


QUOTED

* a high number of variables
* complexity of the data set
* categorical (non-Normal) variables
* categories with low observed frequency (sparsity in responses)
* questions which are conditional upon previous responses
* and multiple multi-item scales, which are summed (either directly or weighted) during analysis. 
QUOTED

@gelman2006data multiple imputation - replaces each value with several, reflecting uncertainty about the model. here different from prediction tasks and more like a bootstrapped estimation (my words). Common to see 5 imputations, might come from slightly different models, then run a complete analysis on each model. Then average over the parameter estimates. equations on @gelman2006data [p.542] 

King et al (2001) review many of the practical costs and benefits of multiple imputation. 



@plumpton2016multiple quoted Multiple imputation for a single incomplete variable works by constructing an imputation model relating the incomplete variable to other variables and drawing from the posterior predictive distribution of the missing data conditional on the observed data [1]. The approach allows for uncertainty in the missing data values by introducing variability in the imputed items. In MICE, variables are initially ordered by level of missingness. Missing values are initially replaced for each variable, for example by drawing at random from the observed values of that variable. Imputation is then conducted on each variable sequentially using the observed and currently imputed values of all other variables in the imputation model. In order to stabilise, this imputation step (known as a cycle) is repeated (typically 10 times) to produce one imputed data set. The process is repeated until the desired number of imputed data sets is reached 

@white2011multiple Multiple imputation by chained equations In large data sets it is common for missing values to occur in several variables. Multiple imputation by chained equations (MICE) [9] is a practical approach to generating imputations (MI Stage 1) based on a set of imputation models, one for each variable with missing values. MICE is also known as fully conditional specification [10] and sequential regression multivariate imputation [11]. Initially, all missing values are filled in by simple random sampling with replacement from the observed values. The first variable with missing values, x1 say, is regressed on all other variables x2,...,xk, restricted to individuals with the observed x1. Missing values in x1 are replaced by simulated draws from the corresponding posterior predictive distribution of x1. Then, the next variable with missing values, x2 say, is regressed on all other variables x1,x3,...,xk, restricted to individuals with the observed x2, and using the imputed values of x1. Again, missing values in x2 are replaced by draws from the posterior predictive distribution of x2. The process is repeated for all other variables with missing values in turn: this is called a cycle. In order to stabilize the results, the procedure is usually repeated for several cycles (e.g. 10 or 20) to produce a single imputed data set, and the whole procedure is repeated m times to give m imputed data sets. 

Particular attention to multi-item scales: QUOTED
As missing data in a single item of a multi-item scale leads to a missing total, the rate of missing data in scale totals can be very high. Imputing at the level of scale total whilst ignoring individual items may therefore introduce unnecessary bias.

A recent study considered imputing at item level rather than imputing scale totals [15]. When the pattern of missingness tended towards all items being missing for a respondent, little difference was seen between methods. When the pattern of missingness tended towards individual items being missing, for sample sizes of n > 100, imputing at item level was shown to be more accurate.

Another study proposed methods for handling multiitem scales at the item score level [16], and further emphasised how mean imputation or single imputation leads to bias and underestimation of standard errors. The study concludes that missing data should be handled by applying multiple imputation to the individual items. 

However, the size and complexity of large survey data can cause complete MI prediction models to fail to converge when the model is specified at item level, rendering the ideal method computationally infeasible.
QUOTED


MI is a maximum likelihood estimation, along with expectation maximization (EM). @plumpton2016multiple MI is robust to departures from normality, small sample sizes and high proportion of missing data. MI also less computationally expensive.

but graphs in @plumpton2016multiple clearly show an attempt to use MI for continuous variables and it's not clear how this translates to categorical.


If missing data differ systematically from non-missing data, this is the MNAR case and is non-ignorable. Specifically, this means that list-wise deletion will lead to biased estimates of population parameters.

## Measuring the Benefit of Imputation

The goal of imputation is to find substitute values for missing data that are neutral to or (preferably) enhance the results of subsequent data analysis. This requires different measures than a classification problem which measures accuracy by comparing predicted values to known values (although this can still be useful to do). Also, @jonsson2004evaluation shows that hot-decking methods, (which covers the association rules based method under investigation) do not always find a suitable donor value. This means that a final round of listwise deletion of remaining incomplete cases may still be necessary, with all the inherent assumptions and risks of such action. It suggests that succuss measures of an association rules based imputation method need to take into account the efficacy as well. Various metrics for evaluating and benchmarking imputation methods are suggested in the reference texts. Alternative approaches exist; pantanowitz2008evaluating is an example of indirectly ascertaining the quality of imputation by measuring the effect on prediction accuracy of a classification technique.


This work will use a combination of reference measures and indirect classification techniques to benchmark against state of the art imputation methods with a variety of datasets.

@graham2007many has good formulas for calculating the variances properly. These formulas allow fo cases where independent variables are correlated with the missing data, so the fraction of missing information is sometimes less than the fraction of missing data. This fraction of missing information is unreliably estimated unless number of imputations is large. The biggest problem of not having enough imputations is the loss of statistical power compared to conventional wisdom of the subject and is most relevant in situations where accuarately identifying levels of risk is of most importance, such as reduction of risk and prevention science.


## Association Rules

Association rules mining (ARM) was first popularised by the development of a fast algorithm for large databases, *apriori*, in @agrawal1993mining. It was developed in response to the vast amount of data collected by large retail enterprises on their customer purchase data. 
Many of the fundamental concepts of ARM were formalised in @agrawal1993mining article and its follow up, @agrawal1994fast which are considered to be the seminal works on the topic. The following definitions are adapted from @agrawal1994fast [p.2] and @tan2005chapter6 [pp.329-330]:

Let $I = \{i_1, i_2,\dots, i_k\}$ be the set of all items. Let $T = \{t_1, t_2,\dots,t_N\}$ be a set of $N$ transactions where each transaction $t_j \subseteq I$. A transaction $t_j$ is said to contain an itemset $X$ (a set of some items in $I$) if $X \subseteq t_j$.

The brute force method of computing all possible rules in a transaction set of N possible items is $O(3^N)$. When it emerged, *apriori* was described as a "fast" algorithm because it outperformed everything that came before. It is still widely used today. @tan2005chapter6 explains that to achieve its objective of efficiently finding useful association rules, ARM must solve two distinct problems:

1. Overcoming the computational challenge of discovering frequent itemsets in large transaction data
1. Ignoring spurious patterns/rules that occur by chance

These challenges are addressed under the *apriori* algorithm by searching the database iteratively for itemsets that occur more frequently than a given probabilistic threshold, known as *support*. Rule discovery is deferred until all frequent itemsets are found i.e those occuring with > minimum support.

**Support for itemset $X$ in Transaction Data $T = \{t_1, t_2,\dots,t_N\}$** $$Supp(X) = |\{t_j|X \in\ t_j, t_j \in T\}| = \frac{|X|}{N}$$

At iteration n, the support for each itemset of size n is calculated. So the first iteration finds all the individual items. Each subsequent iteration, new candidate itemsets with n members are formed by merging smaller itemsets from the growing collection using a method to ensure duplication does not occur (not discussed here). At each iteration, any that fail to meet the minimum *support* are pruned and ignored henceforth. This rapidly reduces the search space and works because of the anti-monotone property of support; $\forall X,Y : (X \subseteq Y) \longrightarrow Supp(Y) \le Supp(X)$, that is to say, support for an itemset never exceeds support for any of its subsets, @tan2005chapter6.

The resulting collection of itemsets is then mined for rules of the pattern $X \implies Y$ (read as $X\ \text{implies}\ Y$) where $X \subseteq T$ is the *antecedent* of a rule and $Y \subseteq T$ is the *consequent* and $X \cap Y = \varnothing$. Although @agrawal1994fast [p.3] clearly states that the cardinality of the consequent $|Y| \ge 1$, in some implementations a restriction of $|Y| = 1$ is applied. he R package *arules*, @R-arules is one such case. This further reduces the computational complexity is also a useful property for this research where a substitute value for one target variable at a time will be sought using association rules.

**Support of rule $X \implies Y$** $$Supp(X \implies Y) = Supp(X \cup Y) = \frac{|X \cup Y|}{N}$$

Note that the support of rule $X \implies Y$ depends only on the support of the superset $X \cup Y$. So by performing the frequent itemset generation step in full, the support for all possible rules from the collection is already known to exceed the minimum support. 

*Support* is useful for ruling out rules that simply occur by chance but the objective of ARM is to find "strong rules" that are not only frequent but also reliable for inference. For this purpose, different measures of *interestingness* are used. A most comprehensive list is given in @hahsler2017interesting and their properties are discussed thoroughly in @tan2005chapter6. In *apriori*, *confidence* is used by default and *lift* is also common, which adjusts for the frequency of the consequent itemset:

**Confidence of rule $X \implies Y$** $$Conf(X \implies Y) = \frac{Supp(X \cup Y)}{Supp(X)}$$

**Lift of rule $X \implies Y$** $$Lift(X \implies Y) = \frac{Conf(X \implies Y)}{Supp(Y)} = \frac{Supp(X \cup Y)}{Supp(X) \times Supp(Y)}$$

@garcia2004mining categorizes ARM among the unsupervised and descriptive techniques as opposed to the other common categorisation (supervised/predictive). The goal of ARM is to find patterns (frequent itemsets and supported rules, in this case) without any prior information about what these patterns might be. The original implementation uses transaction data which is represented as a key-value pair, where the key is a transaction Id (TID) and the value is a list of items purchased in the single transaction identified by TID. See \@ref(tab:transdata) for example. In @R-arules this is represented as an *incidence matrix*, which is a sparse, binary matrix where each row is a transaction and all possible items are each represented in a unique column as present (1) or absent (0). See \@ref(tab:incidencemat). However, this approach is not without its limitations; Transaction data, or incidence matrix data, codifies only the presence or absence of an item. This is binary information and ignores the important variable of quantity or attributes with more than one possible value, such as ordinal data. Also, the binary variables in the (sparse) incidence matrix are said to be *asymmetric* [@tan2005chapter7] because a value of 1 is much more important and far less frequent than a value of 0. 

Overcoming the binary limitation is a trade-off. @ma1998integrating describes a process using a normal relational table with highly discretized data, where the items are represented on columns with various integer values. Each item-value pair is coerced to a binary dummy variable which creates a very wide matrix. This makes ARM applicable to representations of surveys with multi-ranked (non-binary) ordinal data and Likert scales but @tan2005chapter7 explains that the relative scarcity of a useful value is magnified, drastically reducing *support* for each item and producing too many rules. See \@ref(tab:nonbinincmat). @tan2005chapter7 continues with advanced approaches for overcoming this issue based on discretized coninuous data.

The ideas most relevant to this research are adapted here for use with imputation of ordinal variables:

* Pre-processing methods. Combine adjacent values of ( $X \in \{1, 2\}$ or $X \in \{1, 2, 3\}$ ) until minimum support is exceeded:
    * Single Imputation: Applies to antecedent variables.
    * Multiple Imputation: Applies to consequent variables. A cumulative odds model or other smoothing method may be used as a distribution for random draws.

* Statistical methods. Generate frequent itemsets not including the target variable. Compute descriptive statistics of the target variable for freqent item sets. A rule is interesting if the target variable mean for the segment of population covered by the rule is significantly different from the section of the population not covered by the rule.
    * Single Imputation: Introduce the target variable mean as the rule consequent, using appropriate rounding or truncating to meet ordinal value constraints.
    * Multiple Imputation: Introduce target variable statistics as the rule consequent to be used as a distribution for random draws.

* Multi-level (hierachical) methods. Generate rules at a higher level of a hierarchy so naturally grouped variables are combined. Step down into lower levels until minimum support is no longer met. For a Likert scale, this would involve generating itemsets and rules after calculating the aggregate of multiple grouped items (e.g. summation, first principle component etc).
    * Single Imputation: Applies to antecedent variables
    * Multiple Imputation: Combine with other methods.

```{r transdata}
TID <- 1:5
items <- c("{Bread, Milk}"
           , "{Bread, Diapers, Beer, Eggs}"
           , "{Milk, Diapers, Beer, Cola}"
           , "{Bread, Milk, Diapers, Beer}"
           , "{Bread, Milk, Diapers, Cola}")

transdata <- data.frame(TID = TID, Items = items
                           , row.names = TID)
knitr::kable(transdata
             , caption = "Transaction data example. Source: Tan, P. et al (2005)"
             , booktabs = TRUE)

```

```{r incidencemat}
TID <- 1:5
Bread <- c(1, 1, 0, 1, 1)
Milk <- c(1, 0, 1, 1, 1)
Diapers <- c(0, 1, 1, 1, 1)
Beer <- c(0, 1, 1, 1, 0)
Eggs <- c(0, 1, 0, 0, 0)
Cola <- c(0, 0, 1, 0, 1)

incidentmat <- data.frame(TID = TID
                          , Bread = Bread
                          , Milk = Milk
                          , Diapers = Diapers
                          , Beer = Beer
                          , Eggs = Eggs
                          , Cola = Cola
                          , row.names = TID)
knitr::kable(incidentmat
             , caption = "Incidence matrix example. Source: Tan, P. et al (2005)"
             , booktabs = TRUE)
```

```{r nonbinincmat}
TID <- 1:5
Bread_1 <- c(1, 0, 0, 0, 0)
Bread_2 <- c(0, 1, 0, 0, 0)
Bread_3 <- c(0, 0, 0, 1, 1)
Milk_1 <- c(1, 0, 1, 0, 0)
Milk_2 <- c(0, 0, 0, 1, 0)
Milk_3 <- c(0, 0, 0, 0, 1)
Eggs_2 <- c(0, 0, 0, 0, 0)
Eggs_4 <- c(0, 0, 0, 0, 0)
Eggs_6 <- c(0, 1, 0, 0, 0)

nonbinincmat <- data.frame(TID = TID
                          , Bread_1 = Bread_1
                          , Bread_2 = Bread_2
                          , Bread_3 = Bread_3
                          , Milk_1 = Milk_1
                          , Milk_2 = Milk_2
                          , Milk_3 = Milk_3
                          , Eggs_2 = Eggs_2
                          , Eggs_4 = Eggs_4
                          , Eggs_6 = Eggs_6
                          , row.names = TID)
knitr::kable(nonbinincmat
             , caption = "Wide incidence matrix for ordinal values. Adapted from: Tan, P. et al (2005)"
             , booktabs = TRUE)
```

Despite the impressive performance gains by *apriori*, many authors [e.g @mythili2013performance; @tan2005chapter6] raise issues; The cycle of repeated database scans and candidate generations means that performance does not scale well on very large databases (large numbers of transactions/rows or items/columns/dimensions), where the support threshold is low or where the transaction data are dense (maximum itemset width is large). A detailed analysis of the time complexity is not given here, because the size and dimensionality of suitable real-world datasets is expected to be orders of magnitude smaller than retail enterprise transaction data, for which the algoritm was originally designed.

Work has been done develop more compact representations of itemsets and rules, such as maximal and closed itemsets. These concepts lead to depth-first searches such as Equivalence Class Traversal (eclat) which find the frequent itemset border more quickly and lead to more efficient pruning. The frequent pattern tree (FP-Tree) is another, very compact representation of frequent itemsets which may be small enough to fit in main memory. The FP-Growth algorithm can construct an FP-Tree in just two database passes.

Returning to the idea of imputation using similar pattern response matching, it is self-evident that ARM is an algorithm for finding similar patterns. Upon evaluation of a dataset by ARM, a collection of rules (patterns) is returned. However, out-of-the-box this neither offers nor assumes a specific usage for those found patterns. That is left up to the analyst. This research suggests using association rules with a consequent cardinality of one, to substitute values where the antecedent matches frequent response patterns in a survey or questionnaire.

## Classification with Association Rules

\cite{freitas2000understanding} posits that ARM is a deterministic task and very different to classification though does concede that there are special cases where the resulting models can be used for predictions or, more specifically, classification. These special cases involve finding a subset of rules, which \cite{ma1998integrating} calls \textit{class association rules} (CARs). These CARs have the characteristic of having the target variable as the only element in the consequent or right-hand side of the rule. The ``partial classification" described in \cite{ali1997partial} is of particular interest because the objective here is to discover characteristics of the target variables. The authors suggest applicability to situations where some attribute will be modeled based on the other attributes. This is precisely the situation under investigation in this project.\newline

While prediction and imputation differ in scope and goal, the intention of finding a suitable (or ``plausible") value rather than an accurate value could be seen as a mere relaxation of success criteria. If prediction with excellent results is possible from association rules, then it stands to reason that imputation in some form must also be possible.


## Conceptual Model - Bringing it all together?

A particular characteristic of many surveys is the use of ordinal data and multiple-item (Likert) scales. \cite{huisman1999missing} states that there is a strong relationship between the individual items of a Likert scale which measure one latent trait. Techniques that can recognize this within-instance, structural information and preserve it in the imputed dataset should be valuable. \cite{agrawal1994fast} states that association rules mining uses probabilistic measures (support and confidence) for discovering frequent patterns. Association rules mining algorithms work on discretized or categorical data, such as ordinal, nominal and binary. Furthermore, \cite{chandola2005summarization} describe association rules as a compact model of a dataset and as such may be used to enhance other stages of the analysis. So an association rules-based method for survey data would have several advantages over other imputation methods, yet a search of the literature yields no information on the use of association rules in this context.
