<<prologue, include=FALSE>>=
knitr::opts_chunk$set(warning = FALSE
                      , message = FALSE
                      , echo = FALSE
                      )
knitr::opts_template$set(
  fig.tile = list(fig.height = 3
                  , fig.width = 3
                  , fig.align='center')
  )
knitr::opts_knit$set(self.contained=FALSE)
@

\documentclass[a4paper]{article}
\usepackage{titling}
\newcommand{\subtitle}[1]{%
  \posttitle{%
    \par\end{center}
    \begin{center}\large#1\end{center}
    \vskip0.5em}%
}
\usepackage{amsmath}
\usepackage{amssymb}
\let\oldemptyset\emptyset
\let\emptyset\varnothing

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
 
\urlstyle{same}

\usepackage[backend=bibtex, maxcitenames=2, style=authoryear]{biblatex}
\addbibresource{likertimputebiblio.bib}

\begin{document}
\title{An Association Rules Based Method for Imputing Missing Ordinal and Likert Scale Data}
\subtitle{Proposal for Master's Dissertation}
\author{Julian Hatwell}
\maketitle

Student Number: S15142087\newline
Tutor: Prof. Mohamed Medhat Gaber

\tableofcontents

\section{Introduction}

Surveys using ordinal scales are ubiquitous in the medical and social sciences, market research, election polling and numerous other applications. These instruments typically suffer missing data which can have serious impacts on analysis and results. This project investigates a novel technique using association rules to impute missing data, specifically of ordinal responses and multiple-item (Likert) scales which are common to surverys and where imputation techniques are not as well developed as they are for continuous data.

\subsection{Motivation}

The practicalities of running surveys means that researchers and analysts who use such instruments are regularly faced with the problem of missing data (\cite{bono2007missing}; \cite{kamakura2000factor}). There is an abundance of examples in the literature  showing that analysis of survey data is adversely affected by missingness (\cite{madow1983incomplete}; \cite{roth1999missing}; \cite{raaijmakers1999effectiveness}). The problems include biased parameter estimates (central tendency, dispersion and correlation coefficients) and loss of statistical power. There are many strategies for recovering from issues caused by missing data which are well developed for continuous variables, but sources indicate that there has been much less research for datasets comprising  categorical and ordinal variables, (\cite{finch2010imputation}; \cite{leite2010performance}).\newline

Furthermore, there is evidence of significant differences in the performance of various imputation strategies over datasets exhibiting different underlying characteristics (\cite{wu2015comparison}; \cite{rodwell2014comparison}; \cite{sim2015missing}). These results indicate a need to take into account not only the model and magnitude of missingness but also other characteristics particular to the target data. For this reason there is real benefit in having a range of imputation methods from which to choose the most suitable in a given situation.\newline

A particular characteristic of many surveys is the use of ordinal data and multiple-item (Likert) scales. \cite{huisman1999missing} states that there is a strong relationship between the individual items of a Likert scale which measure one latent trait. Techniques that can recognize this within-instance, structural information and preserve it in the imputed dataset should have an advantage. According to \cite{agrawal1994fast}, support and confidence measures are probabilistic features. They measure the frequency of occurence of the within-instance relationships in the underlying distribution. Yet a search of the literature yields no information on the use of association rules in the context of imputation.

\subsection{Problem Statement}

The goal of an imputation technique is to maximize the use of collected data by reducing or eliminating the instance of missingness in the dataset by the substitution of missing values with suitable values. The results of analysing the imputed dataset should either be unaffected, or enhanced by the imputation process. A poor quality imputation (one which yields inaccurate analyses) is worse than no imputation at all. Thus, there may be a trade-off between finding a value to substitute each and every missing data points, while preserving structural parameters such as variance-covariance measures.

\subsection{Contribution of This Work}

This research project will investigate a novel, association rules based imputation technique for missing ordinal and Likert scale data, benchmarked against state of the art. \cite{chandola2005summarization} describe association rules as a compact model of a dataset. So an association rules based method would have an advantage over other imputation methods by generating such a model which may be used to enhance other stages of the analysis. 

\section{Literature Review}

\subsection{Ordinal Data and Multiple-Item Scales in Surveys}

\cite{johnson2009working} states that nominal and ordinal variables are predominant in social sciences data and gives detailed examples of why ordinal variables cannot be treated the same as numerical scales. \cite{gertheiss2009penalized} explains how this assertion holds even when the ordinal variable is a discretised version of an underlying continuous scale. Interpretation depends on arbitrarily assigned categories and mid-point estimates. Upper and lower unbound categories may hide any number of extreme values.\newline

\cite{jamieson2004likert} posits that it is inappropriate even to use mean and standard deviations when analyzing Likert and similar scales. These data types anchor phrases of sentiment or attitude to a series of integers (e.g. 1 = strongly agree, 5 = strongly disagree). Although the responses are ranked, it is wrong to presume that intervals between categories are equal. The barrier to reach the highly polarised strongly agree or strongly disagree may be inconsistent with a move from neutral to either agree or disagree. These types of scales are often skewed or polarized and generally require the use of non-parametric analyses and clear statements of assumptions made. \cite{gliem2003calculating} states that these data types are often intended to be used as a combined scale, measuring a latent concept and concludes that their individual analysis leads to erroneous conclusions. Other authors (\cite{norman2010likert}; \cite{carifio2008resolving}) strongly rebut these arguments, citing various sources of empirical evidence of the robustness of parametric tests. This is one of the great debates of statistics in the last few decades but there is agreement that combining individual responses into their intended scale leads to the least controversial usage and analytical or probabilistic techniques over single items in a Likert scale should be avoided where possible. This present work falls squarely on the side of using the range of available responses for both inference and benchmarking.

\subsection{Models of Missingness in Survey Data}

The reader is directed to \cite{rubin2004multiple} for seminal work on models of missingness. In brief, data may be missing for many reasons, such as failure to complete, subjects' unwillingness to answer certain questions, data entry errors or structural design of questionnaires. Missingness may be univariate (in one variable) or multivariate. Several other patterns and mechanisms are formalised. Of particular importance is the relationship between the missing values, the incomplete variable and the other variables. The patterns arising from these relationships are categorized as:

\begin{itemize}
\item missing completely at random (MCAR) if the distribution of missing data is unrelated to the value of any variable in the dataset. The missing/non-missing state of individual data points cannot be predicted from any information whatever, whether observed or unobserved. This is the most difficult assumption to qualify.
\item missing at random (MAR) is the most common scenario according to \cite{schafer1997analysis} and applies if the distribution of missing data may be related to other variables in the dataset but not the variable itself. The mechanism is correlated with the observed data. 
\item missing not at random (MNAR) if the distribution of missing data is related to the value of the variable itself. This is the most challenging situation to recover. In fact, \cite{tsikriktsis2005review} states that there are no statistical means to recover when missing data is NMAR and no way to ameliorate the potential bias in results based on analysis of such data.
\end{itemize}

\subsection{Techniques to Recover From Missing Data}

Many statistical functions and operations simply do not work with missing data, whether calculated manually or by software. It is necessary to implement some form of pre-processing to recover a dataset for appropriate analysis. \cite{carpita2011imputation} describes four categories of  recovery procedures as follows:

\begin{enumerate}
\item ignore or omit
\item weighting procedures
\item model-based procedures
\item imputation procedures
\end{enumerate}

Each method has advantages and disadvantages. For example, (1) is simplest to implement but potentially introduces significant problems. Simply omitting a data point from an aggregate or multiple-item scale will result in an underestimated score on the latent variable. This category also includes listwise deletion, i.e. only retaining complete cases across all variables in the survey. This method can lead to drastically lower numbers of valid respondents and, as a result, bias parameter estimates if the underlying cause of missing data is MNAR or the magnitude of missingness is high. Consequently, deletion methods have fallen out of favour following the emergence of ubiquitous computing power to run more sophisticated methods.\newline

This present work focuses on imputation. There is a wide variety of imputation strategies available to the researcher and these can be broadly split into single and multiple methods. 

\subsubsection{Single Imputation}

These methods attempt to create a complete dataset by replacing the missing values with an estimated value such as a grand mean, median or mode. Such simplistic approaches are discouraged as they bias variance estimates downwards, reducing the separation between different groups and clusters present in the data, \cite{donders2006review}. Methods that make some attempt to recognize between-groups differences, such as use of a group mean are considered old-fashioned and inadequate, \cite{royston2004multiple}. Preferred single imputation methods include prediction from a linear or other model or substituting each missing value with values from one or more similar but complete individuals. This latter category is known as hot-deck imputation and \cite{jonsson2004evaluation} suggest that nearest neighbour methods are a commonly used implementation.

\subsubsection{Multiple Imputation}

These methods, which are now consdered the state of the art, create multiple copies of the dataset, completed by drawing ``plausible" values from an inferred or bootstrapped distribution. Inferences are then made using these multiple datasets which benefit from having well-understood measures of uncertainty in assumed multivariate normal datasets and are robust to some violations. However, surveys often contain variables with a variety of distributional forms and \cite{raghunathan2001multivariate} gives a number of examples of imperative, structural restrictions between items which are also common. This, taken with the earlier discussion on ordinal and Likert scales, may be a strong indication that multiple imputation is a flawed approach for such datasets.

\subsection{Association Rules}

Association Rules Mining (ARM) was first popularised by the development of a fast algorithm for large databases, \textit{apriori}, in \cite{agrawal1993mining}. Since then, (ARM) has been the subject of intensive research. Readers are directed to an abundance of explanations and examples in the literature. \cite{garcia2004mining} categorizes ARM among the unsupervised and descriptive techniques as opposed to the other common categorisation (supervised/predictive). The goal of ARM is to find patterns (frequent itemsets and supported rules, in this case) without any prior information about what these patterns might be. The original implementation uses transaction data but \cite{ma1998integrating} describes a process using a normal relational table with highly discretized data, where the items are represented on columns with various integer values. It is expected that this methodology will be applicable to representations of surveys with ordinal data.

\subsection{Classification with Association Rules}

\cite{freitas2000understanding} posits that ARM is a deterministic task and very different to classification though does concede that there are special cases where the resulting models can be used for predictions or, more specifically, classification. These special cases involve finding a subset of rules, which \cite{ma1998integrating} calls \textit{class association rules} (CARs). These CARs have the characteristic of having the target variable as the only element in the consequent or right-hand side of the rule. The ``partial classification" described in \cite{ali1997partial} is of particular interest because the objective here is to discover characteristics of the target variables. The authors suggest applicability to situations where attributes will be modeled based on other attributes which is precisely the situation characterizing in this project.\newline

While prediction and imputation differ in scope and goal, the intention of finding a plausible value rather than an accurate value could be seen as a simple relaxation of success criteria. If prediction with excellent results is possible from association rules, then it stands to reason that imputation in some form is also possible.

\subsection{Summary of the Literature Review}

Ordinal data and multiple response scales are a both a common and special case which, due to the practicalities of data collection, are highly prone to missingness and non-response. The magnitude, pattern and distribution of missingness depends on the underlying cause and has consequences for the analysis which include serious loss of statistical power and biased parameter estimates. There are many techniques to impute data (substitute  missing values with ``plausible" values) which can recover the analysis with varying degrees of success. These depend on the features of the data and the characteristics of missingness. The success of these techniques can be assessed using well understood measures.\newline

Since ARM was proposed, the concept has been the subject of intense research. ARM is best suited to applications on highly discretized data. While the technique is inherently a descriptive, knowledge discovery tool, it has been successfully adapted for classification but never used for imputation (based on an extensive literature search).  

\section{Research Methodology}

\subsection{Programming Environment}

The R programming environment (\cite{rsoftware}) has been selected for the implementation of this project. Other systems support ARM, statistical analysis and general purpose scripting/programming either individually or in combination, only R offers a pre-existing set of relevant, open sournce libraries and functions plus unlimited extensibility through its functional and object oriented programming paradigm. Other tools considered and ruled out were:

\begin{itemize}
\item SAS Software. All analytical procs are compiled programmes and can't be extended. Macro language is a limited set of data and file manipulation functions.
\item Oracle Data Mining. A point and click canvas, lacking the programmability required.
\item RapidMiner. A point and click canvas, lacking the programmability required.
\item Python. This could be a viable alternative but would require additional time for the researcher to reach an adequate level of competence. Python offers a native library based on the FP-growth algorithm, which is significantly faster than \textit{apriori} which is an advantage over R. FP-growth is available to R but only via C++ integration with the Rcpp package, \cite{rcpppackage}.
\end{itemize}

\subsection{Datasets}

Both synthetic and real-world datasets will be used. Synthetic datasets offer the advantage of comparing substitute values with known values, statistical testing with  known population parameters such as the underlying distributions of latent and/or observed variables, as well as testing the efficacy of the techniques under controlled conditions of missingness. For example, a technique can be tested on the same dataset with 10\%, 20\% and 30\% of missing values. This approach is best for the earlier stages of development and refining the method.\newline

Two scenarios are considered for real-world datasets. Firstly, there are benchmark datasets which are widely used in data mining research to compare performance with existing techniques. These are available from \url{https://archive.ics.uci.edu/ml/datasets.html}, \url{https://www.kaggle.com/datasets}. The second scenario is to use datasets from other sources (open, or private with permission). This second scenario offers an opportunity respond to unexpected situations and bring a novel technique closer to real-world applicability but it is much more challenging to draw conclusions about performance and accuracy because underlying population parameters are rarely known. Such datasets will be analysed only if time allows.

\subsection{Association Rules Mining Algorithm}

It is not the intention of this project to improve or extend ARM nor to compare different ARM algorithms, but simply to use association rules as the basis of a novel imputation technique. While \cite{li2001cmar} state significant performance benefits from using FP-growth, this requires additional steps to implement in R that will detract from the project timelines. Because of the deterministic nature of ARM, the resulting rulesets should be the same whichever algorithm is used. As such, integration of FP-growth for improved computational performance is left to future work.

\subsection{Imputation Algorithm}

Deliberate attention will be given to the algorithm efficiency, as well as its efficacy and accuracy. Previous work on classification algorithms offer ideas for optimisation. \cite{li2001cmar} propose a novel method which uses pruning rules to manage the number of generated rules and a combined $\chi^2$ measure for combining rules for greater classification accuracy which compares favourably with other classification techniques on benchmark datasets. \cite{yin2003cpar} extend this work with a greedy algorithm that restricts the search space to itemsets containing the target variable rules with only the target variable as the consequent. 

\subsection{Benchmarking and Success Criteria}

The goal of imputation is to find substitute values for missing data that are neutral to or (preferably) enhance the results of subsequent data analysis. This requires different measures than a classification problem which simply  compare predicted values to known values (although this can still be useful). Also, \cite{jonsson2004evaluation} shows that hot-decking methods do not always find a suitable donor value and a final round of listwise deletion of remaining incomplete cases may still be necessary. This suggests that succuss measures of imputation need to take into account the efficacy as well. \cite{jonsson2006benchmarking} propose various metrics for evaluating and benchmarking imputation methods which measure both the ability to find a suitable value as well as its accuracy or approximation of accuracy. Alternative approaches exist; \cite{pantanowitz2008evaluating} is an example of indirectly ascertaining the quality of imputation by measuring the effect on prediction accuracy of a classification technique. This work will use a combination of such techniques and benchmark against state of the art imputation methods with a variety of datasets .\newline

\section{Ethical Consideration}

This project does not carry any foreseeable potential for harm to any individuals or animals, breaches of data protection, conflicts of interest or reputational damage to Birmingham City University or any other institution. The data used in pursuit of the stated aims is either publicly available or permission has been granted for the expressed purpose. Please see the Stage One Ethical Review Form in the appendices.

\section{Supervision}

This project is carried out towards the Masters of Science in Business Intelligence at Birmingham City University, Faculty of Computing and supervised by Prof. Mohamed Medhat Gaber.

\section{Time Frame}

<<gantt>>=
library(plotrix)

Ymd.format <- "%Y/%m/%d"
gantt.info <- list(
  labels=c("Rapid Lit. Review"
           , "Submit Proposal"
           , "Extended Lit. Review"
           , "Algorithm Design"
           , "Benchmarking\n Funcs Design"
           , "Synth. Data Prep."
           , "Functional Code"
           , "Run Experiments"
           , "Results Analysis\n and Write Up"
           , "Submit Report"
           , "Viva Week")
  , starts=as.POSIXct(strptime(
    c("2017/05/22"
      , "2017/06/07"
      , "2017/06/09"
      , "2017/06/19"
      , "2017/06/23"
      , "2017/06/30"
      , "2017/07/01"
      , "2017/07/09"
      , "2017/08/08"
      , "2017/09/20"
      , "2017/09/25")
    , format=Ymd.format))
  , ends=as.POSIXct(strptime(
    c("2017/06/08"
      , "2017/06/08"
      , "2017/07/08"
      , "2017/06/26"
      , "2017/06/30"
      , "2017/07/04"
      , "2017/07/08"
      , "2017/08/20"
      , "2017/09/20"
      , "2017/09/21"
      , "2017/09/29")
    , format=Ymd.format))
  , priorities=c(1
                 , 2
                 , 1
                 , 1
                 , 1
                 , 1
                 , 1
                 , 1
                 , 1
                 , 2
                 , 2))

months <- seq(as.Date("2017/06/01"
                      , "%Y/%m/%d")
              , by="month"
              , length.out=12)

monthslab <- format(months, format="%b")

vgridpos <- as.POSIXct(months, format=Ymd.format)

vgridlab <- monthslab

timeframe <- as.POSIXct(
  c("2017/06/01", "2017/10/01")
  , format=Ymd.format)

gantt.chart(gantt.info
            , taskcolors =
              c("darkgoldenrod1"
              , "red")
            , xlim = timeframe
            , main = "Dissertation 2017 Gantt Chart"
            , priority.legend = FALSE
            , priority.label = "Legend"
            , priority.extremes = c("Low", "High")
            , vgridpos = vgridpos
            , vgridlab = vgridlab
            , hgrid = TRUE)
legend("topright", c("Research"
                     , "Deadline")
       , pch = 15
       , col = c("goldenrod", "red"))
@


\subsection{Deliverables}

\section{Skills Audit}

\section{Support Required}

\printbibliography

\section{Appendices}
\subsection{Stage One Ethical Review Form}
\end{document}