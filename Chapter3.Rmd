# Research Methods

Common design patterns have been identified from similar research found in the literature. These were discussed in the literature review section. The following paragraphs provide a further evaluation with a view to formulating a template experimental design for the applied experimental stage of this project.

## Experimental Design Factors

\textbf{Sample size choice:} This was quite variable among the papers reviewed which used two or three sample sizes, ranging between 200 to 2000. The choice is frequently a trade off between precision and computational cost which is greater for larger samples and increases the number of experiments geometrically for each factor. This research will test two samples sizes; 500, 1500 which covers a broad range.

\textbf{Proportion of missing data:} This was usually varied in regular increments between 0.1 and 0.3, though ranged as high as 0.55 in the articles reviewed. A proportion of 0.5 or greater is only suitable for finding the limits of a technique, rather than benchmarking, because all techniques are challenged. This research will just use proportions of 0.1 and 0.3 to span the useful range but limit the number of factors.

\textbf{Missingness patterns:} The most common missingness mechanism tested is MAR because many papers on this topic refer specifically to MI, with its underlying MVN assumptions. MCAR is less well tested, presumably because it is considered ignorable and rare in real world scenarios. Tests of MNAR are even fewer in number, perhaps because the general consensus in the literature is that this is an intractable problem. However, the most relevant paper to this research describes an experimental process with detail on the generation of two different kinds of MNAR patterns which are relatively trivial to implement. As this project is investigating a novel approach, it would be interesting to look for any niche applications of the technique, such as effectiveness in the MNAR scenario. Therefore this project will make use of a wide variety of patterns, including MNAR.

\textbf{Range of item scores:} Articles that focussed on ordinal and Likert scales varied the number of levels among 2, 3, 5, 6, and 7. In the literature on Likert scales, the values of 5 and 7 are very common and others are rarely seen. Therefore, to maintain this project's focus on Likert scales and limit the number of factors, the values will be restricted to 5 or 7.

\textbf{Range of technique applied in benchmarking:} The relevant articles are very varied on this point and many report the inclusion of mean imputation, regression imputation and list-wise deletion. This may be a symptom of the relative age of some of the research, or potentially flaws in the research design. It is not considered worthwhile to compare any novel technique to those techniques regarded as flawed or outdated in the modern literature. This research will focus on techniques that are considered state of the art and those that are proven to be effective on surveys and questionnaires and non-normally distributed data types. MICE, non-parametric (Hot-deck and nearest neighbours), ABP, person-item mean, two-way imputation and item correlation substitution are considered. List-wise deletion is of interest for comparison only in the MCAR case, but will be applied across all experiments to facilitate the automation.

\textbf{The number of imputations in multiply imputed techniques:} Based on the findings in the literature review, a rule of thumb of 1 imputation per 1% of missing data will be applied for any techniques that perform any kind of MI.

\textbf{Data distributions:} Some experiments generated synthetic data with different levels of skew, kurtosis and bi/multi-modality and inter-item correlation. There was a great variety of methods implemented, ranging from the very precise but very complicated, to the more pragmatic and approximate. During the literature review, it was observed that there is no real value added by creating very precise distributional qualities on an initially continuous variable, given the fact synthetic data synthetic data has to be discretized into an ordinal scale for the experimental work. This research will therefore adopt the more pragmatic approach, generating the data required using approximate techniques and accepting the resulting empirical distributions as population parameters for the starting conditions. This also means that experiments with synthetic and real-world data are easier to compare.

\textbf{Synthetic or real-world data:} Many experimenters use synthetic data because of the possibility to set up precise initial conditions. However, there is a possibility to introduce design bias through the choice of model that generates the data. As this project is investigating a novel approach, it would be interesting to look for any niche applications of the technique, such as effectiveness in data with unusual underlying models or distributions. Therefore this project will make use of a selection of generative methods as well as real-world data.

\textbf{Number of experimental replications:} One thousand replications per set of initial conditions was a consistent choice among the papers review.

```{r hatwell-design-matrix, results='asis'}
factor_name <- c("Number of ordinal categories"
                , "Sample size"
                , "Proportion of missing data"
                , "Missing data mechanism"
                , "Imputation Strategy"
                , "Benchmark Methods"
                , "Experimental Data")
factor_options <- c("5, 7"
                , "500, 1500"
                , "10%, 30%"
                , "MCAR, MAR, MNAR"
                , "Benchmarking novel ARM-based method"
                , "PM, CIM, TW, ICS, MICE, ABP, H-d"
                , "Synthetic, semi-synthetic and real-world sub-sample")

cp_design_matrix <- knitr::kable(data.frame(
  "Design Factor" = factor_name
  , Options = factor_options
  , check.names = FALSE)
        , caption = "Experimental Design Matrix: Conditions. Adapted from source: Carpita and Manisera (2011)")

cp_design_matrix
```


## Methodology

The practical application of the chosen methodology is as follows:

For one set of initial conditions:

1. For synthetic data, a very large sample of 500000 was generated and the population statistics were measured from this. For real-world data, the sample statistics are taken as population statistics prior to any further data preparation (bootstrapping,  subsampling, etc)
1. Generate (synthesis or bootstrapping) one thousand replicates of the dataset
1. Generate the missing data pattern on each replicate
1. Run each imputation method with its own copy of the replicate
1. Estimate the statistics of interest included comparisons relative to population.
1. Pool results

This method is useful as it introduces Monte Carlo error into the estimation process. All statistics of interest can be quoted as a mean with 95% confidence from the empirical distribution of each statistical test. ANOVA tests are possible to look for significant difference is performance between the design factors, if required.

## Datasets

To generate synthetic datasets containing 5-level item scales, the parameters given in @de2010five are used. These distributions are categorized by friendly names such as "Very Strongly Agree" and "Neutral to Disagree." The expected values for the meta-population statistics are given in the article and can be easily compared to the generated data to ensure it fits the intended distribution.

To generate synthetic datasets containing 7-level item scales, the steps given in @wu2015comparison are followed:

1. Generate data on a continuous scale using SEM [lavaan package; @R-lavaan] with the desired inter-item relationships.
1. Discretize the resulting data using the thresholds given in @rhemtulla2012can cited in @wu2015comparison. This provides a standard method for 7-level items with symmetric, moderately skewed and severely skewed distributions are possible.

The following real-world datasets have also been assessed:

* wiki4HE dataset (UCI Machine Learning Repository) [@meseguer2014factors; @gunduzfokoue2013]

```{r wiki4HE_data}
wiki4 <- read.csv("wiki4HE.csv"
                  , sep = ";"
                  , na.strings = "?")
```

* Turkiye Student Evaluation (UCI Machine Learning Repository) @gunduzfokoue2013

```{r turkiye}
turk <- read.csv("turkiye.csv")
```

* Young People's Survey Dataset [@yps2013]

```{r young}
yps <- read.csv("responses.csv")
```

* Job Satisfaction and Procedural Fairness data sets @carpita2011imputation

```{r carpita}
library(xlsx)
read_jspf <- function(sheet, cols) {
  read.xlsx2("JSPF.xls"
          , sheetName = sheet
          , stringsAsFactors = FALSE
          , colClasses = c("numeric"
                           , "character"
                           , rep("numeric", cols)))
}

js <- read_jspf("JS", 13)
pf <- read_jspf("PF", 10)
```

* OxIS 2013 databases provided by the Oxford Internet Institute on 21/04/2017.

```{r oxis}
oxis <- read.csv("oxis.csv")
names(oxis)[1] <- "qa01a"
```

* The [Online Personality Test (not clinically administered)](http://personality-testing.info/)

```{r optest}
temp <- tempfile()
download.file("http://personality-testing.info/_rawdata/BIG5.zip", temp, mode="wb")
perstest <- read.table(unz(temp, "BIG5/data.csv"), header = TRUE, sep="\t")
unlink(temp); rm(temp)
```

## Programming Environment

The R programming environment, @R-base has been selected for the implementation of this project. Only R and Python offer pre-existing sets of relevant, open source libraries and functions plus unlimited extensibility through a functional and object oriented programming paradigm but R is selected based on the Skills Audit conducted for the project research proposal. Other tools considered and ruled out were:

* SAS Software. All analytical procs are compiled programmes and can't be extended. Macro language is a limited set of data and file manipulation functions.
* Oracle Data mining. A point and click canvas, lacking the programmability required.
* RapidMiner. A point and click canvas, lacking the programmability required.
* Python. This could be a viable alternative but would require additional time for the researcher to reach an adequate level of competence. Python offers a native library based on the FP-growth algorithm, which is significantly faster than *apriori* which is an advantage over R. An FP-growth reference implementation is available to R but only via C integration. However, the efficiency of the ARM algorithm should only be of concern for huge datasets and not under the experimental conditions foreseen for this project.

## Package Authoring: The arulesimp Package

To facilitate the automation of experiments, the novel technique will be developed as an R package. This will encapsulate all the programmatic code and will ensure code re-use to help minimize scripting errors. In addition to the imputation method, a number of utility and convenience functions will also be contained in the package.

By following the best practice guidance in @Wickham:2015:RP:2904414, it is hoped that the package will be produced of a quality suitable for publication on the Comprehensive R Archive Network (CRAN), which is the primary repository for published R packages. It is curated to ensure that packages meet a set of strict guidelines, which helps to ensure consistency and reliability for other users.

The package is named arulesimp (*a*ssociation *rules imp*utation) as a nod to the arules package which it imports and invokes to run the ARM step of the novel imputation technique. In addition to this, there are other functions for generating synthetic data, simulating missingness pattern in data and various convenience functions for pre-processing data into a format suitable for the underlying arules package, such as converting all variables to factors. Some demonstration code follows:

### Synthetic data functions
```{r ari_dewinter_demo, echo=TRUE}
set.seed(102130)
# library(arulesimp) # already loaded

# synthesise Likert data using
# De Winter et al (2010) empirical distributions

# pick a selection of distributions
my_vars <- c("strongly_agree", "neutral_to_agree"
             , "multimodal", "strongly_disagree")

# a quick look at the generating parameters
data("dewinter_dist")
dewinter_dist[my_vars, ]

# synthesise some data
dw <- synth_dewinter(var_names = my_vars
               , dists = my_vars
               , 10000)

# explore
smry_dw <- as.data.frame(psych::describe(dw))
smry_dw[, c("mean", "sd", "skew", "kurtosis")]
head(dw, 10)
```

```{r ari_wu_demo, echo=TRUE}
set.seed(100132)
# synthesise Likert data using # Wu's two-factor model
# and Rhemtulla's discretizing thresholds

# this lavaan data model reproduces the configuration in Wu's paper
wu_data_model <-
"Factor1 =~ 0.7*A1 + 0.7*A2 + 0.7*A3 + 0.7*A4 + 0.7*A5 + 0.7*A6
Factor2 =~ 0.7*B1 + 0.7*B2 + 0.7*B3 + 0.7*B4 + 0.7*B5 + 0.7*B6
Factor1 ~~ 0.3*Factor2"

# this function returns a list of 3 data sets
# after discretizing the lavaan model
# by Rhemtulla's thresholds for symmetric, 
# moderately asymmetric and severely asymetric
wu <- synth_wu_data(wu_data_model, sample_size = 1000)
```

```{r ari_wu_demo_plots}
# symmetric
lattice::barchart(~table(wu$sym$A1)
                    , main = "Synthetic Data by Wu's model\n7-level Likert scale, symmetric"
                    , ylab = "A1 symmetric"
                    , xlab = "count"
                    , par.settings = MyLatticeTheme
                    , scales = MyLatticeScale)
# severely asymmetric
lattice::barchart(~table(wu$sasym$A1)
                    , main = "Synthetic Data by Wu's model\n7-level Likert scale, severely asymmetric"
                    , ylab = "A1 severely asymmetric"
                    , xlab = "count"
                    , par.settings = MyLatticeTheme
                    , scales = MyLatticeScale)
```

### Synthetic missingness functions
```{r ari_synmiss_demo, echo=TRUE}
set.seed(120131)
# generate missing data patterns to order
# the default settings are MCAR, 
# 0.2 proportion of missing and
# all variables implicated
head(synth_missing(dw)$data, 10)

# MAR data can be generated by providing some covariates
# and choosing a method from wu_ranking, carpita and princomp.
# A plot can be ordered to show how the missingness pattern
# was distributed by the covariates
# This can be done through the syn_control parameter object
# and its contructor function, missing_control()
non_response_cols <- paste0(rep(c("A", "B"), each = 3), 1:3)
non_response_cols

wu_mar <- synth_missing(wu$sasym
            , syn_control = missing_control("MAR"
                                          , method = "princomp"
                                          , nr_cols = non_response_cols
                                          , dep_cols = c("A6", "B6")
                                          , prob = 0.3)
            , plot_probs = TRUE)

head(wu_mar$data, 10)

# The returned object contains
# a missing indicator matrix
# and other values suggested in
# Carpita and Manisera (2011)
# which are useful for calculating
# Person Item means and other statistics
# which pertain to Likert data and imputation
head(wu_mar$mim$mim, 10) # missing indicator matrix
head(wu_mar$mim$A_comp_i, 10) # indices of missing items for each respondent
head(wu_mar$mim$B_comp_j$A1, 10) # indices of missing respondents for each item
head(wu_mar$mim$B_comp_j$B1, 10) # indices of missing respondents for each item
```

```{r ari_synmiss_demo_plot}
wu_mar$dplot$par.settings <- MyLatticeTheme
wu_mar$dplot$scales <- MyLatticeScale
wu_mar$dplot
```

### Benchmark Imputation Functions

## Other R Packages

R's extensive library of add-on packages has been surveyed in parallel with literature review. The following is a listing of packages that contain at least one key function or dataset that will be used during the applied phase of the project:

* lavaan, @R-lavaan is a library dedicated to Latent Variable Analysis. It contains a suite of functions for generating synthetic data based on any specified model of latent and interacting factors. It will be used to model data in a similar manner to the experiments carried out in @wu2015comparison following the techniques described in @chapman2015r

* Amelia II, @R-Amelia is a package for running a modified version of classic MI, i.e. based on EM and a MVN joint distribution and not chained equations. The computational challenges are overcome by a bootstrapping step. For ordinal data, the package authors recommend either accepting a continuously valued imputation, or applying a built-in transformation which is essentially the same as the non-deterministic rounding described in @carpita2011imputation. It is a very comprehensive implementation with advanced features such as the ability to configure cell level priors to introduce domain knowledge into the model and diagnostic features to check goodness of fit.

```{r Amelia_demo, echo=TRUE}
# library(Amelia) # already loaded

# include all vars, note the time series
a.out <- amelia(wu_mar$data, m = 3)
# a quick look at the imputed data
# imputation 1 of 3, variable A1
head(a.out$imputations$imp1$A1, 10)
range(a.out$imputations$imp1$A1)

# even with rounding, this data makes no sense
# as it exceeds the natural range of the scale

# Amelia can be configured for ordinal data
a.out <- amelia(wu_mar$data, m = 3, ords = "A1")
# another quick look at the imputed data
head(a.out$imputations$imp1$A1, 10)
range(a.out$imputations$imp1$A1)

# though it still uses MVN distribution under the hood
# so it can be challenged by high skewed or multimodal data
# which can be seen in diagnostic plots
# especially the over-impute plot which shows that
# the imputation may be no better than random
```

```{r Amelia_demo_plot}
freqs <- as.data.frame(table(a.out$imputations$imp1$A1, wu_mar$mim$mim[, "A1"]))
names(freqs) <- c("A1", "missing", "count")
freqs$A1 <- as.integer(freqs$A1)

MyTempTheme <- MyLatticeTheme
MyTempTheme$plot.polygon$col <- NULL

lattice::barchart(A1~count, data = freqs
                  , col = rep(c(myPalDark[3]
                  , myPal[4]), 2)
                  , stack = FALSE
                  , main = "Distribution after imputation by Amelia"
                  , sub = "levels 4-7 were especially under-represented by the imputation"
                  , ylab = "A1 severely asymmetric"
                  , xlab = "count"
                  , par.settings = MyTempTheme
                  , scales = MyLatticeScale
                  , key=simpleKey(text = c("not missing"
                               , "imputed")
                    , space="top", columns=2, points=FALSE
                     , col = c(myPalDark[3], myPal[4])))

compare.density(a.out, var = "A1")
overimpute(a.out, var = "A1")
```

* VIM, @R-VIM is a package that provides some imputation methods not available elsewhere including Hot-deck and nearest neighbours as well as a huge suite of diagnostic plots for visualizing missingness and imputed values.

```{r visualize_missingness, echo=TRUE}
# library(VIM) # already loaded

# The VIM package has some excellent vizualizations
aggr_plot <- aggr(wu_mar$data
          , col = c(myPalDark[3]
                    , myPal[4])
          , numbers = TRUE
          , sortVars = TRUE
          , labels = names(wu_mar$data)
          , cex.axis = .7
          , gap = 3
          , ylab = c("Histogram of missing data","Pattern"))
```

* Other packages may be used for utility functions but not all will be listed where not specifically related to the research.

arules (apriori and eclat), arulesViz
arulesCBA, arulesNBminer (classification)
mice, mi

polr ordered categorigal (Market Research R book to check) and also 
https://www.analyticsvidhya.com/blog/2016/02/multinomial-ordinal-logistic-regression/

mi package has possibility to add noise on different rounds of imputation. (dealing with determinism)

## Notes still to tidy



use a combination of reference measures and indirect classification techniques to benchmark against state of the art imputation methods with a variety of datasets.


@white2011multiple alternative descriptions of Normal, Logistic and other models of imputation used in Wu et al. uses 100 imputations on MI when comparing methods and looks at regression coefficients and st. err. Very little detail. Not clear which is correct as only values given and not relative to something. Recommends Wald tests (Likelihood ratio - ordpesns, I think). Their apporach is deliberately "not meant to be definitive." Guards against using normal assupmtions on skewed data because results will not resemble the original scale ("lack of face value validity" p.381). Describes transformation by box cox and PMM as ways to deal with non-normality


@shrive2006dealing as example of a "self-report" ordinal scale where multiple impute worked best, even for very high rates of missingness. Also one missing response in the whole instrument - makes it a real waste to use listwise deletion.

If missing data differ systematically from non-missing data, this is the MNAR case and is non-ignorable. Specifically, this means that list-wise deletion will lead to biased estimates of population parameters.



10. facotr analysis method
500. plausible values
has some stuff on missing data simul.


carpita also raises the validity of treating "don't know" answers as missing, presumably could add this to "rather not say"



If we denote the $n \times p$ dataset as $D$ (with observed part $D^{obs}$ and unobserved part $D^{mis}$), 


@joreskog2005structural uses a variance ratio of matching compared to missing (maybe for pre-imputation work)

ordPens package uses RLRT to assess significant difference in levels of an ordinal predictor effect on dependent. Could changes to this value after imputation be an indication of a change of statistical properties? 

predict ordinal after imputation?

penalised regression - do parameters change?

Shrive - mean and sd, compare with known. Spearmans corrlation of overall scale score, and kappa of classification accuracy.

To indicate if a cell $d_{ij}$ is missing, Let $M$ be the missingness matrix where cell 

$$m_{ij} =
\begin{cases} 
1, \text{if}\ d_{ij} \in D^{mis} \\
0, \text{otherwise}
\end{cases}$$

"non-integer imputations carry more information about the underlying distribution than would be carried if we were to force the imputations to be integers. Thus whenever the analysis model permits, missing ordinal observations should be allowed to take on continuously valued imputations."

complete case
multiple imputation (mice package used prop. odds if vars are ordinal)
single impute ordinal logistic regression (cumulative odds model)
Rubin and Schenker (1986) proposed the Approximate Bayesian Bootstrap (ABB) (carpita) LaplacesDemon

Go through imputation play and vignettes agains to use as demos

carpita's ABPN method?

There's an empirical example at the end. Worth following up. (Wu2015)
Wu's results and conclusions, worth writing up? Question remains (they raise themselves) that their experimental data was generated by latent variable model and this model performed very well in there simulation.

@shrive2006dealing a good methodology paper



KNN imputation. VIM and elsewhere
VIM good information in vignette about other packages
KNN imputation is computationally expensive because of calculating dist each time. ref vim vignette
