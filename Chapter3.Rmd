# Research Methods

Common design patterns have been identified from similar research found in the literature. These were discussed in the literature review section. The following paragraphs provide a further evaluation with a view to formulating a template experimental design for the applied experimental stage of this project.

## Experimental Design Factors

\textbf{Sample size choice:} This was quite variable among the papers reviewed which used two or three sample sizes, ranging between 200 to 2000. The choice is frequently a trade off between precision and computational cost which is greater for larger samples and increases the number of experiments geometrically for each factor. This research will test two samples sizes; 500, 1500 which covers a broad range.

\textbf{Proportion of missing data:} This was usually varied in regular increments between 0.1 and 0.3, though ranged as high as 0.55 in the articles reviewed. A proportion of 0.5 or greater is only suitable for finding the limits of a technique, rather than benchmarking, because all techniques are challenged. This research will just use proportions of 0.1 and 0.3 to span the useful range but limit the number of factors.

\textbf{Missingness patterns:} The most common missingness mechanism tested is MAR because many papers on this topic refer specifically to MI, with its underlying MVN assumptions. MCAR is less well tested, presumably because it is considered ignorable and rare in real world scenarios. Tests of MNAR are even fewer in number, perhaps because the general consensus in the literature is that this is an intractable problem. However, the most relevant paper to this research describes an experimental process with detail on the generation of two different kinds of MNAR patterns which are relatively trivial to implement. As this project is investigating a novel approach, it would be interesting to look for any niche applications of the technique, such as effectiveness in the MNAR scenario. Therefore this project will make use of a wide variety of patterns, including MNAR.

\textbf{Range of item scores:} Articles that focussed on ordinal and Likert scales varied the number of levels among 2, 3, 5, 6, and 7. In the literature on Likert scales, the values of 5 and 7 are very common and others are rarely seen. Therefore, to maintain this project's focus on Likert scales and limit the number of factors, the values will be restricted to 5 or 7.

\textbf{Multi-stage experimental design:} @carpita2011imputation implemented a common sense experimental design which isolates the discovery of the best performing novel method variant from the benchmarking process in two stages. During the research phase of this project, a number of potential variants and improvements have been uncovered which may be combined in many different ways. The computational cost of trying all combinations is prohibitive, and so a multi-stage approach, building on @carpita2011imputation will be adopted.

\textbf{Interestingness Measures:} The performance of ARM can be subtly manipulated by applying any one of a large selection of documented interestingness measures which can be used to sort the found rules. There are too many such measures to test them all in a reasonably designed experiment but two were discussed in the literature on CBA as being useful for predictive tasks. Predictive (Laplace's) accuracy, presented in @yin2003cpar, downweights how confidence rules that have low support in the data. Chi squared is a good measure of how unlikely a rule was to have been discovered by chance, which is equivalent to choosing a predictive model with high sensitivity and specificity. The weighted Chi square statistic, proposed in @li2001cmar goes further by finding the Chi square as a proportion of the maximum possible Chi square score, given the data. This project will include comparisons of different sort orders for the importance of found rules, based on these statistics. 

\textbf{Range of technique applied in benchmarking:} The relevant articles are very varied on this point and many report the inclusion of mean imputation, regression imputation and list-wise deletion. This may be a symptom of the relative age of some of the research, or potentially flaws in the research design. It is not considered worthwhile to compare any novel technique to those techniques regarded as flawed or outdated in the modern literature. This research will focus on techniques that are considered state of the art and those that are proven to be effective on surveys and questionnaires and non-normally distributed data types. MICE, non-parametric (Hot-deck and nearest neighbours), ABP, person-item mean, two-way imputation and item correlation substitution are considered. List-wise deletion is of interest for comparison only in the MCAR case, but will be applied across all experiments to facilitate the automation.

\textbf{The number of imputations in multiply imputed techniques:} Based on the findings in the literature review, a rule of thumb of 1 imputation per 1% of missing data will be applied for any techniques that perform any kind of MI.

\textbf{Data distributions:} Some experiments generated synthetic data with different levels of skew, kurtosis and bi/multi-modality and inter-item correlation. There was a great variety of methods implemented, ranging from the very precise but very complicated, to the more pragmatic and approximate. During the literature review, it was observed that there is no real value added by creating very precise distributional qualities on an initially continuous variable, given the fact synthetic data synthetic data has to be discretized into an ordinal scale for the experimental work. This research will therefore adopt the more pragmatic approach, generating the data required using approximate techniques and accepting the resulting empirical distributions as population parameters for the starting conditions. This also means that experiments with synthetic and real-world data are easier to compare.

\textbf{Synthetic or real-world data:} Many experimenters use synthetic data because of the possibility to set up precise initial conditions. However, there is a possibility to introduce design bias through the choice of model that generates the data. As this project is investigating a novel approach, it would be interesting to look for any niche applications of the technique, such as effectiveness in data with unusual underlying models or distributions. Therefore this project will make use of a selection of generative methods as well as real-world data.

\textbf{Number of experimental replications:} One thousand replications per set of initial conditions was a consistent choice among the papers reviewed. However, association rules mining and prediction are more computationally expensive than the methods applied and several variants need to be compared before the final benchmark. Therefore, 10 rounds of each experiment will be run for the first three phases, to find the best variant before running 500 rounds of the benchmarks.

```{r hatwell-design-matrix, results='asis'}
factor_name <- c("Number of ordinal categories"
                , "Sample size"
                , "Proportion of missing data"
                , "Missing data mechanism"
                , "Imputation Strategy"
                , "Stage 1: Rule Selection"
                , "Stage 1a: Default Class"
                , "Stage 2: Int. Measures"
                , "Stage 3: Procedural Variant"
                , "Stage 4: Benchmarking"
                , "Experimental Data")
factor_options <- c("5, 7"
                , "500, 1500"
                , "10%, 30%"
                , "MCAR, MAR, MNAR"
                , "Benchmarking novel ARM-based method"
                , "Best Rule, Top N Mean, Top N Maj. Vote, RHS Freq*"
                , "No Default, Most frequent, Non-deterministic"
                , "Confidence, Laplace, Chisq, Wt Chisq, RHS Freq*"
                , "All at once, Iterative, Iterative w/ Propensity, Scale Means**"
                , "PM, CIM, TW, ICS, MI (EMB), MICE, ABP"
                , "Synthetic, semi-synthetic and real-world sub-sample")

cp_design_matrix <- knitr::kable(data.frame(
  "Design Factor" = factor_name
  , Options = factor_options
  , check.names = FALSE)
        , caption = "Experimental Design Matrix: Conditions. Adapted from source: Carpita and Manisera (2011)")

cp_design_matrix
```

RHS Freq* appears at both stage 1 and stage 2 because it considers all rule matches. There is an overlap with both rule selection and best rule ordering.

Scale Means** is an echo of @plumpton2016multiple, but rather than using the scale totals instead the individual items, this research will include the (mathematically) rounded overall mean as an additionally engineered feature. The mean is chosen because more than one item in each scale may contain missing data which could drastically bias a scale sum. It also reduces the number of possible values. The new variable contains additional information on the relationship between the items of a multi-item scale, which should help the imputation.

## Methodology

The practical application of the chosen methodology is as follows:

For one set of initial conditions:

1. For synthetic data, a very large sample of 500000 was generated and the population statistics were measured from this. For real-world data, the sample statistics are taken as population statistics prior to any further data preparation (bootstrapping,  subsampling, etc)
1. Generate (synthesis or bootstrapping) one thousand \footnote{Due to computational constraints, the final experiments were restricted to 250 runs.} replicates of the dataset
1. Generate the missing data pattern on each replicate
1. Run each imputation method with its own copy of the replicate \footnote{Due to computational constraints, for the MI methods the value of \textit{m} were set to 3 or 5 for missingness proportions of 0.1 and 0.5, respectively.} of the dataset
1. Perform a listwise deletion if any incomplete cases remain
1. Estimate the statistics of interest included comparisons relative to population.
1. Pool and report results

This method is useful as it introduces Monte Carlo error into the estimation process. All statistics of interest can be quoted as a mean with 95% confidence from the empirical distribution of each statistical test. ANOVA tests are possible to look for significant difference in performance between the design factors, if required.

## Success Measures

A wide variety of well established statistics for measuring the effectiveness of imputation was covered in the literature review. It would be painstaking to attempt to measure and report everything, so for simplicity, the following statistics will be taken:

**Likert and multiple item scales:**

* Mean Absolute Scale Error (MASE, using the overall scale mean), see \@ref(eq:sse) & \@ref(eq:mase). This statistic measures the mean absolute difference between the original scale score and the scale score after imputation for each respondent. Taking the absolute difference maintains the magnitude of any error, but the direction of bias is lost.
* Relative bias of scale mean \@ref(eq:relbias). Relative bias allows the effect on very different statistics to be compared and the direction is maintained
* Relative bias of scale standard deviation \@ref(eq:relbias)
* Cronbach's alpha relative error (CAE), see \@ref(eq:cae) & \@ref(eq:relbias). This is equivalent to a relative bias calculation.
* 95% CI coverage using a t-distribution for all statistics with the exception of Cronbach's alpha which is modeled using an F-distribution, see \@ref(eq:alphaciupper) & \@ref(eq:alphacilower). A high CI coverage would indicate that the imputation process has had a neutral effect on the statistical properties under investigation.
* Ability by individual attribute (data point) and by case can be measured for methods that do not finish with a default class imputation.

**Individual items and unrelated items:**

* Relative bias of item mean \@ref(eq:relbias)
* Relative bias of item standard deviation \@ref(eq:relbias)
* Relative bias of inter-item correlation with Fisher's r-to-Z transform, see \@ref(eq:relbias) & \@ref(eq:fisherz)
* Relative bias of split half correlation with Fisher's r-to-Z transform, see \@ref(eq:relbias) & \@ref(eq:fisherz). The split half method combines half the individual items at random and assessing the inter-item correlation for stability across all the items. Here it is used to test whether two non-related scale items have becomre more or less correlated by deliberately combining items that were subject to the imputation.

**In addition, on Phase 4, benchmarking:**

Various vizualisations will also be applied to highlight the important results.

## Datasets

To generate synthetic datasets containing 5-level item scales, the parameters given in @de2010five are used. These distributions are categorized by friendly names such as "Very Strongly Agree" and "Neutral to Disagree." The expected values for the meta-population statistics are given in the article and can be easily compared to the generated data to ensure it fits the intended distribution. See \@ref(fig:ari-dewinter-demo-plot)

To generate synthetic datasets containing 7-level item scales, the steps given in @wu2015comparison are followed:

1. Generate data on a continuous scale using SEM [lavaan package; @R-lavaan] with the desired inter-item relationships.
1. Discretize the resulting data using the thresholds given in @rhemtulla2012can cited in @wu2015comparison. This provides a standard method for 7-level items with symmetric, moderately skewed and severely skewed distributions are possible. See \@ref(fig:ari-wu-demo-plot1) and \@ref(fig:ari-wu-demo-plot2)

The following real-world datasets have also been assessed:

* wiki4HE dataset (UCI Machine Learning Repository) [@meseguer2014factors; @gunduzfokoue2013]

```{r wiki4HE_data}
wiki4 <- read.csv("wiki4HE.csv"
                  , sep = ";"
                  , na.strings = "?")
```

* Turkiye Student Evaluation (UCI Machine Learning Repository) @gunduzfokoue2013

```{r turkiye}
turk <- read.csv("turkiye.csv")
```

* Young People's Survey Dataset [@yps2013]

```{r young}
yps <- read.csv("responses.csv")
```

* Job Satisfaction and Procedural Fairness data sets @carpita2011imputation

```{r carpita}
library(xlsx)
read_jspf <- function(sheet, cols) {
  read.xlsx2("JSPF.xls"
          , sheetName = sheet
          , stringsAsFactors = FALSE
          , colClasses = c("numeric"
                           , "character"
                           , rep("numeric", cols)))
}

js <- read_jspf("JS", 13)
pf <- read_jspf("PF", 10)
```

* OxIS 2013 databases provided by the Oxford Internet Institute on 21/04/2017.

```{r oxis}
oxis <- read.csv("oxis.csv")
names(oxis)[1] <- "qa01a"
```

* The [Online Personality Test (not clinically administered)](http://personality-testing.info/)

```{r optest, cache=TRUE}
temp <- tempfile()
download.file("http://personality-testing.info/_rawdata/BIG5.zip", temp, mode="wb")
perstest <- read.table(unz(temp, "BIG5/data.csv"), header = TRUE, sep="\t")
unlink(temp); rm(temp)
```

## Programming Environment

The R programming environment, @R-base has been selected for the implementation of this project. Only R and Python offer pre-existing sets of relevant, open source libraries and functions plus unlimited extensibility through a functional and object oriented programming paradigm but R is selected based on the Skills Audit conducted for the project research proposal. Other tools considered and ruled out were:

* SAS Software. All analytical procs are compiled programmes and can't be extended. Macro language is a limited set of data and file manipulation functions.
* Oracle Data mining. A point and click canvas, lacking the programmability required.
* RapidMiner. A point and click canvas, lacking the programmability required.
* Python. This could be a viable alternative but would require additional time for the researcher to reach an adequate level of competence. Python offers a native library based on the FP-growth algorithm, which is significantly faster than *apriori* which is an advantage over R. An FP-growth reference implementation is available to R but only via C integration. However, the efficiency of the ARM algorithm should only be of concern for huge datasets and not under the experimental conditions foreseen for this project.

## Package Authoring: The arulesimp Package

To facilitate the automation of experiments, the novel technique will be developed as an R package. This will encapsulate all the programmatic code and will ensure code re-use to help minimize scripting errors. In addition to the imputation method, a number of utility and convenience functions will also be contained in the package.

By following the best practice guidance in @Wickham:2015:RP:2904414, it is hoped that the package will be produced of a quality suitable for publication on the Comprehensive R Archive Network (CRAN), which is the primary repository for published R packages. It is curated to ensure that packages meet a set of strict guidelines, which helps to ensure consistency and reliability for other users.

The package is named arulesimp (*a*ssociation *rules imp*utation) as a nod to the arules package which it imports and invokes to run the ARM step of the novel imputation technique. In addition to this, there are other functions for generating synthetic data, simulating missingness pattern in data and various convenience functions for pre-processing data into a format suitable for the underlying arules package, such as converting all variables to factors. Some demonstration code follows:

### Synthetic data functions
```{r ari-dewinter-demo, echo=TRUE}
set.seed(102130)
# library(arulesimp) # already loaded

# synthesise Likert data using
# De Winter et al (2010) empirical distributions

# pick a selection of distributions
my_vars <- c("strongly_agree", "neutral_to_agree"
             , "multimodal", "strongly_disagree")

# a quick look at the generating parameters
data("dewinter_dist")
dewinter_dist[my_vars, ]

# synthesise some data
dw <- synth_dewinter(var_names = my_vars
               , dists = my_vars
               , 10000)

# explore
smry_dw <- as.data.frame(psych::describe(dw))
smry_dw[, c("mean", "sd", "skew", "kurtosis")]
head(dw, 10)
```

```{r ari-dewinter-demo-plot, fig.height=3, fig.cap='A selection of variables synthesised using synth_dewinter function in arulesimp which implements the distributions given in De Winter (2010)'}
densityplot(~strongly_agree +
             neutral_to_agree +
             multimodal +
             strongly_disagree
            , data = dw
            , par.settings = MyLatticeTheme
            , scales = MyLatticeScale
            , xlab = "Selection of De Winter distributions"
            , auto.key = list(columns = 2)
            , plot.points = FALSE)
```

```{r ari_wu_demo, echo=TRUE}
set.seed(100132)
# synthesise Likert data using # Wu's two-factor model
# and Rhemtulla's discretizing thresholds

# this lavaan data model reproduces the configuration in Wu's paper
wu_data_model <-
"Factor1 =~ 0.7*A1 + 0.7*A2 + 0.7*A3 + 0.7*A4 + 0.7*A5 + 0.7*A6
Factor2 =~ 0.7*B1 + 0.7*B2 + 0.7*B3 + 0.7*B4 + 0.7*B5 + 0.7*B6
Factor1 ~~ 0.3*Factor2"

# this function returns a list of 3 data sets
# after discretizing the lavaan model
# by Rhemtulla's thresholds for symmetric, 
# moderately asymmetric and severely asymetric
wu <- synth_wu_data(wu_data_model, sample_size = 1000)
```

```{r ari-wu-demo-plot1, fig.height=3, fig.cap='Symmetric distribution of one variable by Wu\' method synthesised using synth_wu_data function in the arulesimp package. See Wu (2015)'}
# symmetric
lattice::barchart(~table(wu$sym$A1)
                    , main = "Synthetic Data by Wu's model\n7-level Likert scale, symmetric"
                    , ylab = "A1 symmetric"
                    , xlab = "count"
                    , par.settings = MyLatticeTheme
                    , scales = MyLatticeScale)
```

```{r ari-wu-demo-plot2, fig.height=3, fig.cap='Severely asymmetric distribution of one variable by Wu\'s method synthesised using synth_wu_data function in the arulesimp package. See Wu (2015)'}
# severely asymmetric
lattice::barchart(~table(wu$sasym$A1)
                    , main = "Synthetic Data by Wu's model\n7-level Likert scale, severely asymmetric"
                    , ylab = "A1 severely asymmetric"
                    , xlab = "count"
                    , par.settings = MyLatticeTheme
                    , scales = MyLatticeScale)
```

### Synthetic missingness functions
```{r ari_synmiss_demo, echo=TRUE}
set.seed(120131)
# generate missing data patterns to order
# the default settings are MCAR, 
# 0.2 proportion of missing and
# all variables implicated
head(synth_missing(dw)$data, 10)

# MAR data can be generated by providing some covariates
# and choosing a method from wu_ranking, carpita and princomp.
# A plot can be ordered to show how the missingness pattern
# was distributed by the covariates
# This can be done through the syn_control parameter object
# and its contructor function, missing_control()
non_response_cols <- paste0(rep(c("A", "B"), each = 3), 1:3)
non_response_cols

wu_mar <- synth_missing(wu$sasym
            , syn_control = missing_control("MAR"
                                          , method = "princomp"
                                          , nr_cols = non_response_cols
                                          , dep_cols = c("A6", "B6")
                                          , prob = 0.3)
            , plot_probs = TRUE)

head(wu_mar$data, 10)

# The returned object contains a missing indicator matrix
# and other values suggested in Carpita and Manisera (2011)
# which are useful for calculating Person Item means
# and other statistics which pertain to Likert data and imputation
head(wu_mar$mim$mim, 10) # missing indicator matrix
head(wu_mar$mim$A_comp_i, 10) # indices of missing items for each respondent
head(wu_mar$mim$B_comp_j$A1, 10) # indices of missing respondents for each item
head(wu_mar$mim$B_comp_j$B1, 10) # indices of missing respondents for each item
```

```{r ari-synmiss-demo-plot, fig.cap="Distribution of missingness probability for MAR pattern with the princomp method. This works well to strongly separate respondents by assiging probabilities proportional to the first principle component of the covariates. Ranking method gives a flat response, while the method given by Carpita gives a central tendency."}
wu_mar$dplot$par.settings <- MyLatticeTheme
wu_mar$dplot$scales <- MyLatticeScale
wu_mar$dplot
```

### Benchmark Imputation Functions

### ARimpute function

laplace accuracy @yin2003cpar

## Other R Packages

R's extensive library of add-on packages has been surveyed in parallel with literature review. The following is a listing of packages that contain at least one key function or dataset that will be used during the applied phase of the project:

### psych
psych, @R-psych has a number of analytical functions for Likert scales, including a comprehensive Cronbach's alpha function and a Fisher's R-to-z transform.

### lavaan
lavaan, @R-lavaan is a library dedicated to *la*tent *va*riable *an*alysis. It contains a suite of functions for generating synthetic data based on any specified model of latent and interacting factors. It will be used to model data in a similar manner to the experiments carried out in @wu2015comparison following the techniques described in @chapman2015r

### Amelia II
Amelia II, @R-Amelia is a package for running a modified version of classic MI, i.e. based on EM and a MVN joint distribution and not chained equations. The computational challenges are overcome by a bootstrapping step. For ordinal data, the package authors recommend either accepting a continuously valued imputation, or applying a built-in transformation which is essentially the same as the non-deterministic rounding described in @carpita2011imputation. It is a very comprehensive implementation with advanced features such as the ability to configure cell level priors to introduce domain knowledge into the model and diagnostic features to check goodness of fit.

```{r Amelia_demo, echo=TRUE}
set.seed(1230532)
# library(Amelia) # already loaded

# include all vars, note the time series
a.out <- amelia(wu_mar$data, m = 3, p2s = 0) # print to screen = silent
# a quick look at the imputed data
# imputation 1 of 3, variable A1
head(a.out$imputations$imp1$A1, 10)
range(a.out$imputations$imp1$A1)

# even with rounding, this data makes no sense
# as it exceeds the natural range of the scale

# Amelia can be configured for ordinal data
a.out <- amelia(wu_mar$data, m = 3, ords = "A1", p2s = 0) # print to screen = silent
# another quick look at the imputed data
head(a.out$imputations$imp1$A1, 10)
range(a.out$imputations$imp1$A1)

# This is better, though it still uses MVN distribution under the hood
# so it can be challenged by high skewed or multimodal data
# which can be seen in diagnostic plots
# especially the over-impute plot which shows that
# the imputation may be no better than random
```

```{r Amelia-demo-plot1, fig.cap="Barchart showing the new distribution of the 7 levels of variable A1 after imputation. The light blue values were missing prior to imputation and may or may not have had the current value in the untreated data."}
freqs <- as.data.frame(table(a.out$imputations$imp1$A1, wu_mar$mim$mim[, "A1"]))
names(freqs) <- c("A1", "missing", "count")
freqs$A1 <- as.integer(freqs$A1)

MyTempTheme <- MyLatticeTheme
MyTempTheme$superpose.polygon$col <- c(myPalDark[3], myPal[4])

lattice::barchart(A1~count, data = freqs
                  , groups = missing
                  , stack = TRUE
                  , main = "Distribution after imputation by Amelia"
                  , sub = "levels 5-7 were especially under-represented by the imputation"
                  , ylab = "A1 severely asymmetric"
                  , xlab = "count"
                  , par.settings = MyTempTheme
                  , scales = MyLatticeScale
                  , auto.key=list(text = c("not missing"
                               , "imputed")
                    , space="top", columns=2
                    , points=FALSE, rectangles=TRUE
                     ))
```
```{r Amelia-demo-plot2, fig.cap="bserved and Imputed diagnostic plot from the Amelia package. The imputed distribution has identified the skew but has also over-compensated, resulting in an under-representation of values 4 to 7."}
compare.density(a.out, var = "A1")
```
```{r Amelia-demo-plot3, fig.cap="Overimpute diagnostic from the Amelia package. This process performs multiple imputations over the observed data to show how the model would have imputed them if absent. It is very useful to see if the model is accurate. In this case, the predictions do not appear to follow the unity line at all. Rather, it appears that any value might be imputed as any other value with fairly equal chance. It might be inferred that Amelia has done no better than naive random imputation on this data."}
overimpute(a.out, var = "A1")
```

### Zelig
Zelig, @R-Zelig is a framework for statistical analysis which provides a seamless interface for parameter estimation from objects returned by Amelia and mi.

```{r zelig_demo, echo=TRUE}
# library(Zelig) # already loaded

# estimate the regression parameters of A1 with its related variables
# untreated data first
summary(lm(A1 ~ A2 + A3 + A4 + A5 + A6
               , data = wu$sasym))$coefficients

# using Zelig on the Amelia object
z.out <- zelig(A1 ~ A2 + A3 + A4 + A5 + A6
               , data = a.out$imputations
               , model = "ls"
               , cite = FALSE)
summary(z.out)

# similarly, the regression of total score B on total score A
summary(lm(I(B1 + B2 + B3 + B4 + B5 + B6) ~
              I(A1 + A2 + A3 + A4 + A5 + A6)
              , data = wu$sasym))$coefficients

z.out <- zelig(I(B1 + B2 + B3 + B4 + B5 + B6) ~
              I(A1 + A2 + A3 + A4 + A5 + A6)
              , data = a.out$imputations
              , model = "ls"
              , cite = FALSE)
summary(z.out)
# A quick look at the t-values and p-values shows that
# multiple imputation has estimated with greater uncertainty
```

### mice
mice, @R-mice implemented the MICE algorithm which can use PMM for non-normally distributed data.
```{r mice_demo, echo=TRUE}
# library(mice) # already loaded
mice.out <- mice(wu_mar$data, m = 3, maxit = 10
                 , meth = 'pmm', seed = 500
                 , print = FALSE)

# it's easy to estimate parameters using the with() and pool() functions
A1_fit <- with(mice.out, lm(A1 ~ A2 + A3 + A4 + A5 + A6))
summary(pool(A1_fit))[, c(1:3, 5)]

B_regress <- with(mice.out
                  , lm(I(B1 + B2 + B3 + B4 + B5 + B6) ~
                        I(A1 + A2 + A3 + A4 + A5 + A6)))
summary(pool(B_regress))[, c(1:3, 5)]

# diagnostic plots are implemented in lattice graphics.
# Of these, various dot and scatter plots are not useful with Likert
# The density plot can help to reveal any systematic bias
# in the imputation model, as was seen with Amelia.
```
```{r micee-demo-plot, fig.cap='A density plot from the mice output is useful for showing any disparity between the distribution of imputed values compared to observed values.'}
densityplot(mice.out)
```

### VIM
VIM, @R-VIM is a package that provides some imputation methods not available elsewhere including Hot-deck and nearest neighbours as well as a huge suite of diagnostic plots for visualizing missingness and imputed values.

```{r visualize_missingness, echo=TRUE}
# library(VIM) # already loaded

# The VIM package has some excellent vizualizations
aggr_plot <- aggr(wu_mar$data
          , col = c(myPalDark[3]
                    , myPal[4])
          , numbers = TRUE
          , sortVars = TRUE
          , labels = names(wu_mar$data)
          , cex.axis = .7
          , gap = 3
          , ylab = c("Histogram of missing data","Pattern"))
```

### arules and arulesCBA
The arules package, @R-arules, features an interface to the most commonly used C implementation of *apriori*, @borgelt2017aprori. The arulesimp package built for this project relies on arules for the novel ARImpute function. The arulesCBA package @R-arulesCBA provides a framework for classification tasks based on arules that follows the standard R formulation. However, after prototyping, it was found that the CBA function was not completely suited to the project requirements.

```{r arulesCBA demo, echo=TRUE}
# library(arulesCBA) # already loaded

# arules requires all variables to be factors
# As a first step, some convenience functions from
# arulesimp package are demonstrated for pre-processing

data("iris")
iris_Likert <- all_factor(cut_equal(iris[, -5], levs = 7))
iris_Likert$Species <- iris$Species

head(iris_Likert)
str(iris_Likert)

irisCBA <- CBA(Species ~ ., data = iris_Likert
               , support = 0.05, confidence = 0.8)
inspect(rules(irisCBA))
irisCBA_preds <- predict(irisCBA, iris_Likert)

caret::confusionMatrix(irisCBA_preds, iris_Likert$Species)$table
```

### Other packages
Hmisc, @R-Hmisc and mi, @R-mi also implement MICE but the mice package is adopted in this project based on its simplicity to use. The ordPens package, @R-ordPens uses RLRT to test for significant differences between the effect of an ordinal predictor on dependent a dependent variable in different models. Essentially this is an ANOVA test for ordinal predictors and would be very useful to measure changes to statistical properties following imputation. Unfortunately it takes a long time to run each test and this turns out to be too costly for experiments running into hundreds of thousands of iterations.

Other packages are used for utility such as parallelization of experimental runs, but these will not be listed where not specifically related to the research.

## Notes still to tidy

other packages
polr ordered categorigal (Market Research R book to check) and also 
https://www.analyticsvidhya.com/blog/2016/02/multinomial-ordinal-logistic-regression/


make a table of datasets and characteristics


Shrive - mean and sd, compare with known. Spearmans corrlation of overall scale score, and kappa of classification accuracy.

"non-integer imputations carry more information about the underlying distribution than would be carried if we were to force the imputations to be integers. Thus whenever the analysis model permits, missing ordinal observations should be allowed to take on continuously valued imputations."

Rubin and Schenker (1986) proposed the Approximate Bayesian Bootstrap (ABB) (carpita) LaplacesDemon

There's an empirical example at the end. Worth following up. (Wu2015)

Wu's results and conclusions, worth writing up? Question remains (they raise themselves) that their experimental data was generated by latent variable model and this model performed very well in there simulation.