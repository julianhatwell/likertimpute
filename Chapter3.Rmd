# Notes still to tidy

@white2011multiple alternative descriptions of Normal, Logistic and other models of imputation used in Wu et al. uses 100 imputations on MI when comparing methods and looks at regression coefficients and st. err. Very little detail. Not clear which is correct as only values given and not relative to something. Recommends Wald tests (Likelihood ratio - ordpesns, I think). Their apporach is deliberately "not meant to be definitive." Guards against using normal assupmtions on skewed data because results will not resemble the original scale ("lack of face value validity" p.381). Describes transformation by box cox and PMM as ways to deal with non-normality

## MVN definitiion
If we denote the ($n√ók$) dataset as $D$ (with observed part $D^{obs}$ and unobserved part $D^{mis}$), then this assumption is
$$D \sim N_k(\mu, \Sigma)$$
which states that D has a multivariate normal distribution with mean vector mu and covariance matrix Sigma. 

@joreskog2005structural uses a variance ratio of matching compared to missing (maybe for pre-imputation work)

ordPens package uses RLRT to assess significant difference in levels of an ordinal predictor effect on dependent. Could changes to this value after imputation be an indication of a change of statistical properties? 

predict ordinal after imputation?

penalised regression - do parameters change?

Shrive - mean and sd, compare with known. Spearmans corrlation of overall scale score, and kappa of classification accuracy.

To indicate if a cell $d_{ij}$ is missing, Let $M$ be the missingness matrix where cell 

$$m_{ij} =
\begin{cases} 
1, \text{if}\ d_{ij} \in D^{mis} \\
0, \text{otherwise}
\end{cases}$$

"non-integer imputations carry more information about the underlying distribution than would be carried if we were to force the imputations to be integers. Thus whenever the analysis model permits, missing ordinal observations should be allowed to take on continuously valued imputations."

complete case
multiple imputation (mice package used prop. odds if vars are ordinal)
single impute ordinal logistic regression (cumulative odds model)
Rubin and Schenker (1986) proposed the Approximate Bayesian Bootstrap (ABB) (carpita)

Go through imputation play and vignettes agains to use as demos

There's an empirical example at the end. Worth following up. (Wu2015)
Wu's results and conclusions, worth writing up? Question remains (they raise themselves) that their experimental data was generated by latent variable model and this model performed very well in there simulation.

@shrive2006dealing a good methodology paper

# Research Methods

This work will use a combination of reference measures and indirect classification techniques to benchmark against state of the art imputation methods with a variety of datasets.

The experiments designed for similar research follow a common pattern. Firstly, various design factors can be manipulated to assess imputation methods under different conditions:

* Sample size choice was quite variable among the papers reviewed, ranging between 200 to 1250.
* The proportion of missing data is usually varied in regular increments between 0.1 and 0.5
* The most common missingness mechanism tested in MAR because this is well suited to testing the performance of MI. MCAR is less well test, presumably because it is considered ignorable and rare in real world scenarios. Tests of MNAR are fewer in number, perhaps because the general consensus in the literature is that this is an intractable problem. However, @carpita2011imputation is investigating a non-parametric method and does explore the MNAR scenario, offering methods to synthesise this kind of data.
* Statistical properties of the variables are varied. It is common to see investigations of the effects of different levels of inter-item correlation, and skewness introduced into synthetic datasets.
* Vary item score of Likert scales between 3, 5 and 7.
* Vary m the number imputation for MI

Most researchers conducted each experiment in their design matrix 1000 times and followed a process generally similar to the following:

Master step, optional: For synthetic data, a huge sample e.g. 500000 was generated and the population statistics were measure from this. 

1. If synthetic data was used, this was generated using a non-deterministic process. In the case of real-world data, the common practice was to take bootstrap samples from the complete cases for each run. 
1. Once the experimental data were generated, the statistics of interest were measured on the complete dataset.
1. A synthetic missing data process was run.
1. Optional step: The statistics of interest were estimated in the complete cases scenario.
1. The imputation method was run.
1. The statistics of interest were estimated were measured again.

This method is useful as it introduces Monte Carlo error into the estimation process. Researchers were then able to quote a mean with 95% confidence from the empirical distribution of each statistical test.

## Measures of effectiveness of technique


## R Packages

psych alpha - with inter-item, item-totals, alpha drop and r drop.

psych fisherz - 

cocor package for comparing correlations

NOTE CAE in carpita section is just relative bias.

MSE ratio, no point in coding up

maybe get alpha confidence intervals by bootstrapping psych package.

ordPens, ordinal
arules (apriori and eclat), arulesViz
arulesCBA, arulesNBminer (classification)
amelia, vim, mice, mi

polr ordered categorigal (Market Research R book to check) and also 
https://www.analyticsvidhya.com/blog/2016/02/multinomial-ordinal-logistic-regression/

mi is written by gelman and hill (and others)

mi package has possibility to add noise on different rounds of imputation. (dealing with determinism)

VIM good information in vignette about other packages
KNN imputation is computationally expensive because of calculating dist each time. ref vim vignette

Amelia II uses Expectation Maximisation with bootstrapping as a fast, stable algo for MI. Can configure cell level priors to introduce domain knowledge into the MI process. Diagnostic features to check goodness of fit. ASSUMES DATA IS MULTIVARIATE NORMAL

Although MI is thought to be robust to this assumption of MVN, Amelia II offers some transformations to address needs of real world data sets. MI also assumes that data is MAR. Missingness depends on the observed data $D^{obs}$

## Datasets

@de2010five synthetic based on these distributions

This work makes use of the following datasets:

wiki4HE dataset (UCI Machine Learning Repository) [@meseguer2014factors; @gunduzfokoue2013]

```{r wiki4HE_data}
wiki4 <- read.csv("wiki4HE.csv"
                  , sep = ";"
                  , na.strings = "?")
```

Turkiye Student Evaluation (UCI Machine Learning Repository) @gunduzfokoue2013

```{r turkiye}
turk <- read.csv("turkiye.csv")
```

Young People's Survey Dataset [@yps2013]

```{r young}
yps <- read.csv("responses.csv")
```

@carpita2011imputation

```{r carpita}
library(xlsx)
read_jspf <- function(sheet, cols) {
  read.xlsx2("JSPF.xls"
          , sheetName = sheet
          , stringsAsFactors = FALSE
          , colClasses = c("numeric"
                           , "character"
                           , rep("numeric", cols)))
}

js <- read_jspf("JS", 13)
pf <- read_jspf("PF", 10)
```

OxIS 2013 databases provided by the Oxford Internet Institute on 21/04/2017.

```{r oxis}
oxis <- read.csv("oxis.csv")
names(oxis)[1] <- "qa01a"
```

## online personality test, not clinically administered
temp <- tempfile()
download.file("http://personality-testing.info/_rawdata/BIG5.zip", temp, mode="wb")
perstest <- read.table(unz(temp, "BIG5/data.csv"), header = TRUE, sep="\t")
unlink(temp); rm(temp)


## benchmarks
### Accuracy


## Packages

packages norm cat mix pan

Research is the order you do things to meet your objectives.
Consider alternative research methods for each objective.
Research methods enable you to achieve your objectives.

Ethnography, Lit review, Structured questions, Experimentation, Survey. Case Study.
