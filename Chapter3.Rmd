# Research Methods

This chapter covers all the practical aspects of the project, including the algorithm design, the experimental design and other practical considerations

## Algorithm Design

### Conceptual Model {#sec:concmod}

> "The model methodology is centered on defining an abstract model for a real system."

@amaral2011computing [p.2]

> "A model can be described by text and block diagrams to specify how a system is constructed from a collection of interacting components."

@amaral2011computing [p.8]

```{r concept-model, fig.cap='Conceptual model for the proposed ARM based imputation software package architecture.'}
knitr::include_graphics("concept_model.png", dpi = 100)
```

Concurrently with the literature review, proof of concept and prototyping in code has been conducted. Through an iterative and incremental development process, more sophisticated concepts have been pegged onto the core modules as the code base has stabilised. During this process, a useful model emerged which was quickly adopted as the template for the software package architecture. See Figure \@ref(fig:concept-model). This model illustrates the overall workflow for an AR-based imputation process, incorporating the key learning from the literature review.

### Algorithm

For practical reasons, the novel AR-based method has the *apriori* algorithm at its core. However, this would ideally be substituted in the future with the FP-growth algorithm. These techniques are well-documented in the literature, so they will not be broken down into detailed algorithmic steps here. Furthermore, *apriori*, as implemented in the arules package for R, @R-arules, finds all rules. These must then be converted into CARs by filtering only specific consequent conditions. This is an inefficient process and not intentionally part of the AR-based imputation algorithm. It does not make sense to document the novel algorithm this way so the steps below simply begin with a set of CARs as input without defining how these are generated.

Recalling the data matrix $X$ and its corresponding missing indicator matrix $M$ as given in \@ref(eq:carpitaX) and \@ref(eq:carpitaM), let $J$ be the set of variables, such that for each $j$ there is some missing data, $M_{+j} > 0,\ j \in \{1,\ 2,\ \dots , p\  \}$. Recall also that $B(j)$ is the set of row indices for respondents who have a value in $j$ ($M_{ij} = 0$) and $B'(j)$, the complement set of row indices with a missing value in $j$ ($M_{ij} = 1$). Assuming AR-based imputation does not always estimate a value for every missing data point, let $M'_{ij}$ be the missing indicator matrix after AR-based imputation.

Let $\textit{lhs}(r)$ be the antecedent, or left-hand side of rule $r$ and $\textit{rhs}(r)$ be the consequent, or right-hand side of rule $r$. Let $R_J$ be the set of all CARs found for the set of variables in J, $R_j$ be the set of CARs for variable $j$. For any single CAR $r_j$, the cardinality of $\textit{rhs}(r_j)$ is one, its value is $k \in \{\ 1, 2,\ \dots\ ,\ K\ \}$ where $K$ is the number of ordinal levels. $X_{i}\ \textit{satisfies}\ r_j$ when all key-value pairs in $\textit{lhs}(r_j)$ have a match in the key-value pair representation of $X_{i}$.

Let $\textit{agg}()$ be the aggregate function set by user (e.g. rounded mean, majority vote) with $N$ as the early stopping parameter. $N = 1$ is the *best rule* method. Let $\textit{def}()$ be the default class function set by user (e.g. majority class, posterior frequency distribution). Let $\textit{srt}()$ be the precedence sorting function set by user (e.g. by *Confidence*, Chi squared, etc).

\begin{algorithm}
\caption{Association Rules based imputation}\label{ARI1}
\begin{algorithmic}[1]
\Procedure{AR-based Imputation}{$X$, $R$, $J$, $N$, $\textit{agg}()$, $\textit{def}()$, $\textit{srt}()$}
  \ForAll{$j \in J$}
    \State $C \gets [\ ]$
    \For{$r_j \in \textit{srt}(R_j)$}
      \For{$i \in B'(j)$}
        \While{$length(C) \leq N$}
          \If{$X_i\ \textit{satisfies}\ \textit{lhs}(r_j)$}
            \State $C \gets append[\ C,\ \textit{rhs}(r_j)\ ]$
          \EndIf
        \EndWhile
        \State $X_{ij} \gets \textit{agg}(C)$
      \EndFor
    \EndFor
    \If{$M'_{+j} > 0$}
      \State $X_{ij} \gets \textit{def}(j), \forall{i \in M'_{ij} = 1}$
    \EndIf
  \EndFor
  \State $\textbf{return }{X}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

Although algorithm 1 uses nested looping, the early stopping when N is reached helps to reduce the search for matching rules for each row. However, for implementation in R, such nested loops are best avoided. Algorithm 2 is an alternative which is closer to the final implementation in R code. It prioritizes vectorwise operations over loops.

\begin{algorithm}
\caption{Association Rules based imputation (alternative)}\label{ARI2}
\begin{algorithmic}[2]
\Procedure{AR-based Imputation}{$X$, $R$, $J$, $N$, $\textit{agg}()$, $\textit{def}()$, $\textit{srt}()$}
  \ForAll{$j \in J$}
    \State $U \gets \textit{length}(\ B'(j)\ )$
    \State $V \gets \textit{length}(R_j)$
    \State $v \gets 1\ \textbf{to}\ V$
    \State $C \gets [\ ]_{(U \times V)}$
    \For{$u \gets 1\ \textbf{to}\ U$}
      \State $i \gets B'(j)_u$
      \If{$X_i\ \textit{satisfies}\ \textit{lhs}(\ \textit{srt}(R_j)_v\ )$}
        \State $C_{uv} \gets \textit{rhs}(\ \textit{srt}(R_j)_v\ )$
      \Else
        \State $C_{uv} \gets \textit{null}$
      \EndIf
    \EndFor
    \ForAll{u}
      \State $C'_u \gets \textit{first}\ N\ \textit{non null values in }{C_u}$
      \State $C''_u \gets \textit{agg}(C'_u)$
      \State $i \gets B'(j)_u$
      \State $X_{ij} \gets C''_u$
    \EndFor
    \If{$M'_{+j} > 0$}
      \State $X_{ij} \gets \textit{def}(j), \forall{i \in M'_{ij} = 1}$
    \EndIf
  \EndFor
  \State $\textbf{return }{X}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

For the Iterative Sequential Association Rules based imputation algorithm, the short-hand *makeCARs()* is used for the rule discovery step. Two user-defined stopping parameters are included for limiting the number of iterations or stopping when a target convergence is achieved.

\begin{algorithm}
\caption{Iterative Sequential Association Rules based imputation}\label{ARI3}
\begin{algorithmic}[3]
\Procedure{ISAR-based Imputation}{$X$, $J$, $N$, $\textit{agg}()$, $\textit{def}()$, $\textit{srt}()$, $\textit{itermax}$, $\textit{targc}$}
\State $X' \gets X$
\State $\textbf{sort}\ J\ \textbf{by}\ M_{+j}\  \textbf{ascending}$
\For{$j \in J$}
  \State $j* \gets \textit{bootstrap } X_j$
  \For{$i \in M_{ij} = 1$}
    \State $X'_{ij} \gets \textit{random value drawn from}\ j*$ 
  \EndFor
\EndFor
\State $\textit{iter} \gets 0$
\State $c \gets \textit{targc} + 1$
\While{$\textit{iter} < \textit{itermax}\ \land \ c > \textit{targc}$}
  \State $\textit{iter} \gets \textit{iter} + 1$
  \For{$j \in J$}
    \State $X'_j \gets X_j$
    \State $R_j \gets \textit{makeCARs}(j)$
    \State $X'' \gets \textit{run AR-based imputation }(\ X = X',\ R = R_j,\ J = j,\ \textit{agg}(),\ \textit{def}(),\ \textit{srt}()\ )$
  \EndFor
  \State $c \gets \frac{1}{|J|} \sum_{j \in J}{\sum_{i \in B'(j)} \sqrt(X''_{ij} - X'_{ij})^2}$
  \State $X' \gets X''$
\EndWhile
\State $\textbf{return}\ X'$
\EndProcedure
\end{algorithmic}
\end{algorithm}

## Experimental Design

\secref{sec:succrit} identified and critiqued the experimental designs from similar research found in the literature. The following paragraphs formulate the common design factors into a model process for the applied experimental stage of this project.

### Experimental Design Factors

\textbf{Objective and guiding principles:} The objective of the experimental stage is ultimately to compare a novel technique against existing benchmark techniques. However, the literature review unearthed a plethora of variations, tweaks and tunings for an AR-based imputation technique, any of which may improve general performance. It would be computationally and operationally very difficult to run all possible variants, let alone to compare them against the several benchmarks. Yet, there is a real motivation to find an optimally performing variant otherwise there is a risk of failing to discover the novel technique's true potential. The literature review also discussed several data scenarios which should be explored in order to investigate any niche uses for the novel technique. 

\textbf{Multi-stage experimental design:} @carpita2011imputation implemented a common sense, two-stage experimental design which isolates the discovery of the best performing novel method variant from the benchmarking process. There are potentially hundreds of unique variants in this experiment and they cannot each be assessed. Therefore, a multi-stage approach, building on @carpita2011imputation will be adopted. The results from each stage will be used to narrow the options for subsequent stages.

\textbf{Sample size choice:} This was quite variable among the papers reviewed which used two or three sample sizes, ranging between 200 to 2000. The choice is frequently a trade off between precision and computational cost which is greater for larger samples and each additional option increases the number of experiments geometrically. This research will test just one samples size of 1000 which is good for a general purpose experiment as the research needs to focus on the optimization and benchmarking. Niche scenarios with larger or smaller sample sizes will be deferred for further work.

\textbf{Proportion of missing data:} This was usually varied in regular increments between 0.1 and 0.3, though ranged as high as 0.55 in the articles reviewed. A proportion of 0.5 or greater is only suitable for finding the limits of a technique, rather than benchmarking, because all techniques are severely challenged. This research will just use `r missing_prop` to limit the number of design factors while providing a sufficiently challenging scenario to separate out poorer performance.

\textbf{Missingness patterns:} The most common missingness mechanism tested is MAR because many papers on this topic refer specifically to MI, with its underlying MVN assumptions. MCAR is less well tested, presumably because it is considered ignorable and rare in real world scenarios. Tests of MNAR are even fewer in number, perhaps because the general consensus in the literature is that this is an intractable problem. However, the most relevant paper to this research describes an experimental process with detail on the generation of two different kinds of MNAR patterns which are relatively trivial to implement. This is one area where it is valuable, even at an early stage, to assess degrees of performance because the assumptions about missingness patterns are fundamental to the performance of the benchmarks.

\textbf{Interestingness Measures:} The performance of ARM can be subtly manipulated by applying any one of a large selection of documented interestingness measures which can be used to rank the rules. There are too many such measures to test them all in a reasonably designed experiment but two were discussed in the literature on CBA as being useful for predictive tasks; Laplace's accuracy, presented in @yin2003cpar, downweights rules that have high *Confidence* but low *Support* in the data and Chi squared is a good measure of how unlikely a rule was to have been discovered by chance, which is equivalent to choosing a predictive model with high sensitivity and specificity. The weighted Chi square statistic, proposed in @li2001cmar goes further by weighting the Chi square as a proportion of the maximum possible Chi square score based on the cases that satisfy any given rule. This project will compare *Confidence*, Laplace's Accuracy, Chi square and weighted Chi square. It should be noted that, for pattern discovery, minimum *Support* is set extremely low at 0.02 in order to find even rare patterns. However, this should be considered in context of tabular, not transaction data along with the dataset size, which is only 1000 rows. This means that a pattern must appear in at least 20 rows to clear this threshold. This use case is very different from the enterprise retail origins of ARM. It is important that at least one matching rule can be found in the vast majority of cases, otherwise the method has to fall back on the default class too frequently. The *Confidence* measure for the initial rule discovery, prior to ranking, is also set to a fairly low value of 0.2 for all variants. These values were found to be effective during proof of concept stages, but a methodical investigation of these tuning parameters should be included in any future refinement.

\textbf{Range of techniques applied in benchmarking:} The relevant articles are very varied on this point and many report the inclusion of mean imputation, regression imputation and list-wise deletion. This may be a symptom of the relative age of some of the research, or potentially flaws in the research design. It is not considered worthwhile to compare any novel technique to those techniques regarded as flawed or outdated in the modern literature. This research will focus on techniques that are considered state of the art and those that are proven to be effective on surveys and questionnaires and non-normally distributed data types. MI and MICE, person-mean, corrected item mean, two-way imputation and item correlation substitution are included.

\textbf{The number of imputations in multiply imputed techniques:} Based on the findings in the literature review, a rule of thumb of 1 imputation per 1% of missing data has superseded the previous advice of 3-5 imputations. However, this results in a suggested 30 iterations for the MI and MICE benchmarks, which is computationally demanding. This research will fall back on the classic advice and apply m=5 to these two benchmarks, as well as the iterative-sequential implementation of the novel method.

\textbf{Synthetic or real-world data:} Many experimenters use synthetic data because of the possibility to set up precise initial conditions. However, the literature review hinted at a possibility to introduce design bias through the choice of model that generates the data. Real-world data avoids this problem because there are no design assumptions and unusual empirical distributions may be involved. This project is investigating a novel approach and while it will be very interesting to look for many niche applications of the technique, there is a danger to broaden the scope too far at an early stage. Therefore, this project will implement a synthetic dataset, as close as possible to that described in @wu2015comparison. SEM will be used so that related items can be guaranteed a high degree of inter-item correlation.

\textbf{Data distributions:} Some experiments generated synthetic data with different levels of skew, kurtosis and bi/multi-modality and inter-item correlation. There was a great variety of methods implemented, ranging from the very precise but very complicated, to the more pragmatic and approximate. During the literature review, it was observed that there is no real value added by creating very precise distributional qualities on an initially continuous variable, given the fact synthetic data has to be discretized into an ordinal scale for the experimental work. This research will therefore adopt the method described in @wu2015comparison, which was one of the most pragmatic approaches for generating the data required. Synthetic data is discretized according to hard-coded thresholds. A very large, one-off sample is generated and the population parameters for the starting conditions are estimated from this empirical distribution. This also makes experiments with synthetic and real-world data are easier to compare. Both symmetric and severely asymmetric data is synthesized for comparison, and to ensure that benchmark methods based on MVN assumptions are suitably challenged.

\textbf{Range of item scores:} Articles that focussed on ordinal and Likert scales varied the number of levels among 2, 3, 5, 6, and 7. In the literature on Likert scales, the values of 5 and 7 are very common and others are rarely seen. Because of the multi-stage design and many method variants to test, this project requires a narrow design scope to maintain a manageable number of combinations. The range will be restricted to 7, as used in the method described in @wu2015comparison.

\textbf{Number of experimental replications:} One thousand replications per set of initial conditions was a consistent choice among the papers reviewed. However, association rules mining and prediction are more computationally expensive and can be quite time consuming when combined with iterative imputation. The multi-stage design of this experiment makes a large number of replicas impractical. Therefore, 100 rounds of each experiment will be run.

```{r hatwell-design-matrix, results='asis'}
factor_name <- c("Number of ordinal categories"
                , "Sample size"
                , "Proportion of missing data"
                , "Missing data mechanism"
                , "Imputation Strategy"
                , "Stage 1: Rule Selection"
                , "Stage 2: Default Class"
                , "Stage 3: Int. Measures"
                , "Stage 4: Procedural Variant"
                , "Stage 5: Benchmarking"
                , "Experimental Data")
factor_options <- c("7"
                , sample_sizes
                , paste0(missing_prop * 100, "%")
                , paste(wu_missing_patt, collapse = ", ")
                , "Benchmarking novel AR-based method"
                , "Best Rule, Top N Mean, Top N Maj-Vote, RHS Freq"
                , "Most frequent, Non-deterministic"
                , "Confidence, Laplace, Chisq, Wt Chisq"
                , "All at once, Iterative, Propensity*, Scale Means**"
                , "PM, CIM, TW, ICS, MI (EMB), MICE"
                , "Synthetic, semi-synthetic and real-world sub-sample")

cp_design_matrix <- knitr::kable(data.frame(
  "Design Factor" = factor_name
  , Options = factor_options
  , check.names = FALSE)
        , caption = "Experimental Design Matrix.")

cp_design_matrix
```

Propensity* as discussed in @carpita2011imputation is a logistic regression model of the binary representation of a variable's missingness indicator vector on all the other variables. See \@ref(eq:MAR). The fitted probabilities of the model are used to split the data, e.g, by high/low propensity and then fit a separate imputation model on each split. This can be combined with an iterative process and run to convergence.

Scale Means** combines the ideas of @plumpton2016multiple and @tan2005chapter7, to include the (mathematically) rounded scale mean as an additionally engineered feature. The mean will be used for this because more than one item in each scale may contain missing data and that could drastically bias a scale sum. It also means the new features are on the same scale as the original variables. The new variables add value because they contain additional information on the relationship between the items of a multi-item scale.

### Experimental Methodology

The practical application of the chosen methodology is as follows:

For one set of initial conditions:

1. For synthetic data, a very large sample of 500000 was generated and the population statistics were measured from this. For real-world data, the sample statistics are taken as population statistics prior to any further data preparation (bootstrapping,  subsampling, etc)
1. Generate (synthesis or bootstrapping) 100 replicates of the dataset
1. Generate the missing data pattern on each replicate
1. Run each imputation method with its own copy of the replicate of the dataset
1. Perform a listwise deletion if any incomplete cases remain
1. Estimate the statistics of interest including comparisons relative to population parameters.
1. Pool and report results

### Success Measures

The following statistics will be measured for each experiment:

* Mean Absolute Scale Error (MASE). See \@ref(eq:sse) & \@ref(eq:mase). This statistic measures the mean absolute difference between the MIRV totals (the scale score) before and after imputation for each respondent. MASE values of < 0.5 have a neutral effect on the scale totals because of rounding.
* Scale mean, being the mean of scale means for each subject.
* Scale standard deviation, being the standard deviation of scale means for each subject.
* Cronbach's alpha.
* Split half correlation with Fisher's r-to-Z transform, see  \@ref(eq:fisherz). The split half method combines half the individual items at random and assessing the inter-item correlation for stability across all the items. Here it is used to test whether two non-related scale items have become more or less correlated by deliberately combining items from separate scales that were subject to the imputation, A1-A3 and B1-B3. The r-to-Z transform allows significance tests to be applied.
* Ability by individual attribute (data point) and by case is measured at Stage 1 where the process does not finish with a default class imputation.
* Relative bias of each statistic is considered during Stage 5 as part of the benchmarking. See \@ref(eq:relbias). A value of < 0.05 is deemed acceptable for a well-performing imputation technique.

## Practical Considerations

### Datasets

#### Real-world Data

The following real-world datasets were assessed for suitability for inclusion in the experiments:

* wiki4HE dataset (from the UCI Machine Learning Repository) [@meseguer2014factors; @gunduzfokoue2013]

* Turkiye Student Evaluation (UCI Machine Learning Repository) @gunduzfokoue2013

* Young People's Survey Dataset [@yps2013]

* Job Satisfaction and Procedural Fairness data sets @carpita2011imputation

* OxIS 2013 databases provided by the Oxford Internet Institute on 21/04/2017.

* The [Online Personality Test (not clinically administered)](http://personality-testing.info/)

#### Synthetic Data Methods

To generate synthetic datasets containing 5-level item scales, the parameters given in @de2010five can be used. These distributions are categorized by friendly names such as "Very Strongly Agree" and "Neutral to Disagree." The expected values for the meta-population statistics are given in the article and can be easily compared to the generated data to ensure it fits the intended distribution. See \@ref(fig:ari-dewinter-demo-plot).

To generate synthetic datasets containing 7-level item scales, the steps given in @wu2015comparison are followed:

1. Generate data on a continuous scale using SEM [lavaan package; @R-lavaan] with the desired inter-item relationships.
1. Discretize the resulting data using the thresholds given in @rhemtulla2012can cited in @wu2015comparison. This provides a standard method for 7-level items with symmetric, moderately skewed and severely skewed distributions are possible. See \@ref(fig:ari-wu-demo-plot1) and \@ref(fig:ari-wu-demo-plot2)

#### Final Pick

As discussed already, Wu's method was favoured for the experimental stage of this work. However, De Winter's method has been implemented in the arulesimp package. After appropriate legal and licensing checks, real-world datasets that are already available in the public domain may also be shipped with the package for demonstration purposes. These include YPS, Turkiye Student Evaluation, wiki4HE and Online Personality Test.

### Hardware

All the experiments were run on the following machine:

Microsoft Surface with 8 GB RAM, 1 TB SSD HD, x64 processor, Windows 10 64 Bit.

### Programming Environment

The R programming environment, @R-base has been selected for the implementation of this project. Only R and Python offer pre-existing sets of relevant, open source libraries and functions plus unlimited extensibility through a functional and object oriented programming paradigm but R is selected based on the Skills Audit conducted for the project research proposal. The R version used was 3.4.0 (2017-04-21).

Other tools considered and ruled out were:

* SAS Software. All analytical procs are compiled programmes and can't be extended. Macro language is a limited set of data and file manipulation functions.
* Oracle Data mining. A point and click canvas, lacking the programmability required.
* RapidMiner. A point and click canvas, lacking the programmability required.
* Python. This could be a viable alternative but would require additional time for the researcher to reach an adequate level of competence. Python offers a native library based on the FP-growth algorithm, which is significantly faster than *apriori* which is an advantage over R. An FP-growth reference implementation is available to R but only via C integration. However, the efficiency of the ARM algorithm should only be of concern for huge datasets and not under the experimental conditions foreseen for this project.

### Package Authoring: The arulesimp Package

> "No matter how simple the system is, do not allow it to evolve from small pieces without a plan. Think before you build. Most importantly, consider a modular approach."

@amaral2011computing [p.4]

To facilitate the automation of experiments, the novel technique was developed as an R package. The package encapsulates all the programmatic code and helps to minimize scripting errors. By following the best practice guidance in @Wickham:2015:RP:2904414, it is hoped that the package will eventually be of an acceptable quality for publication on the Comprehensive R Archive Network [https://cran.r-project.org/](CRAN), which is the primary repository for published R packages. CRAN is curated by the community to ensure that packages meet a strict set of guidelines, which helps to ensure consistency and reliability for other users.

The package has been named arulesimp (*a*ssociation *rules imp*utation) as a nod to the arules package which it imports and invokes to run the ARM step of the novel imputation method. In addition to this, there are other utility and convenience functions based on the conceptual model devised in \secref{sec:concmod}. These can be used to generate synthetic data for demonstration research purposes, simulating different missingness patterns in data, and functions for pre-processing data into a format suitable for the underlying arules package which requires discretized factor variables. Some demonstration code follows:

#### Synthetic data functions {#sec:syndatfun}
```{r ari-dewinter-demo, echo=TRUE}
set.seed(102130)
# library(arulesimp) # already loaded

# synthesise Likert data using
# De Winter et al (2010) empirical distributions

# pick a selection of distributions
my_vars <- c("strongly_agree", "neutral_to_agree"
             , "multimodal", "strongly_disagree")

# a quick look at the generating parameters
data("dewinter_dist")
dewinter_dist[my_vars, ]

# synthesise some data
dw <- synth_dewinter(var_names = my_vars
               , dists = my_vars
               , 1000)

# explore
smry_dw <- as.data.frame(psych::describe(dw))
smry_dw[, c("mean", "sd", "skew", "kurtosis")]
head(dw, 10)
```

```{r ari-dewinter-demo-plot, fig.height=3, fig.cap='A selection of variables synthesised using synth_dewinter function in arulesimp which implements the distributions given in De Winter (2010)'}
densityplot(~strongly_agree +
             neutral_to_agree +
             multimodal +
             strongly_disagree
            , data = dw
            , par.settings = MyLatticeTheme
            , scales = MyLatticeScale
            , xlab = "Selection of De Winter distributions"
            , auto.key = list(columns = 2)
            , plot.points = FALSE)
```

```{r ari_wu_demo, echo=TRUE}
set.seed(100132)
# synthesise Likert data using Wu's two-factor model
# and Rhemtulla's discretizing thresholds, Wu et al. (2015)

# this lavaan data model reproduces the configuration in Wu et al. (2015)
wu_data_model <-
"Factor1 =~ 0.7*A1 + 0.7*A2 + 0.7*A3 + 0.7*A4 + 0.7*A5 + 0.7*A6
Factor2 =~ 0.7*B1 + 0.7*B2 + 0.7*B3 + 0.7*B4 + 0.7*B5 + 0.7*B6
Factor1 ~~ 0.3*Factor2"

# this function returns a list of 3 data sets
# after discretizing the lavaan model
# by Rhemtulla's thresholds for symmetric, 
# moderately asymmetric and severely asymetric
wu <- synth_wu_data(wu_data_model, sample_size = 1000)
```

```{r ari-wu-demo-plot1, fig.height=3, fig.cap='Symmetric distribution of one variable by Wu\'s method synthesised using synth_wu_data function in the arulesimp package. See Wu (2015)'}
# symmetric
lattice::barchart(~table(wu$sym$A1)
                    , main = "Synthetic Data by Wu's model\n7-level Likert scale, symmetric"
                    , ylab = "A1 symmetric"
                    , xlab = "count"
                    , par.settings = MyLatticeTheme
                    , scales = MyLatticeScale)
```

```{r ari-wu-demo-plot2, fig.height=3, fig.cap='Severely asymmetric distribution of one variable by Wu\'s method synthesised using synth_wu_data function in the arulesimp package. See Wu (2015)'}
# severely asymmetric
lattice::barchart(~table(wu$sasym$A1)
                    , main = "Synthetic Data by Wu's model\n7-level Likert scale, severely asymmetric"
                    , ylab = "A1 severely asymmetric"
                    , xlab = "count"
                    , par.settings = MyLatticeTheme
                    , scales = MyLatticeScale)
```

#### Synthetic missingness functions
```{r ari_synmiss_demo, echo=TRUE}
set.seed(120131)
# generate missing data patterns to order
# the default settings are MCAR, 
# 0.2 proportion of missing and
# all variables implicated
head(synth_missing(dw)$data, 10)

# MAR data can be generated by providing some covariates
# and choosing a method from wu_ranking, carpita and princomp.
# A plot can be ordered to show how the missingness pattern
# was distributed by the covariates
# This can be done through the syn_control parameter object
# and its contructor function, missing_control()
non_response_cols <- paste0(rep(c("A", "B"), each = 3), 1:3)
non_response_cols

wu_mar <- synth_missing(wu$sasym
            , syn_control = missing_control("MAR"
                                          , method = "princomp"
                                          , nr_cols = non_response_cols
                                          , dep_cols = c("A6", "B6")
                                          , prob = 0.3)
            , plot_probs = TRUE)

head(wu_mar$data, 10)

# The returned object contains a missing indicator matrix
# and other values suggested in Carpita and Manisera (2011)
# which are useful for calculating Person Item means
# and other statistics which pertain to Likert data and imputation
head(wu_mar$mim$mim, 10) # missing indicator matrix
head(wu_mar$mim$A_comp_i, 10) # indices of missing items for each respondent
head(wu_mar$mim$B_comp_j$A1, 10) # indices of missing respondents for each item
head(wu_mar$mim$B_comp_j$B1, 10) # indices of missing respondents for each item
```

```{r ari-synmiss-demo-plot, fig.height=3, fig.cap="Distribution of missingness probability for MAR pattern with the princomp method. This works well to strongly separate respondents by assiging probabilities proportional to the first principle component of the covariates. Ranking method gives a flat response, while the method given by Carpita gives a central tendency."}
wu_mar$dplot$par.settings <- MyLatticeTheme
wu_mar$dplot$scales <- MyLatticeScale
wu_mar$dplot
```

#### Benchmark Imputation Functions

The benchmark imputation functions discussed in Carpita and Manisera (2011) have also been implemented along with the non-deterministic rounding function. Other rounding functions may be passed to LikertImpute, such as round(), ceiling() and floor(). Available methods are PM (Person Mean), CIM (Corrected Item Mean), TW (Two-Way), ICS (Item Correlation Substituation).

```{r ari_lik_demo, echo=TRUE}
# Person Mean Imputation
pm <- LikertImpute(dt = wu_mar$data
                   , dt_mim = wu_mar$mim
                   , method = "PM"
                   , rounding = nd_round)
head(pm)
```

#### ARimpute function

The focal point of this research is the novel imputation algorithm, implemented as ARImpute().

```{r ari_imp_demo, echo=TRUE}
# Step 1: Prepare a vector of variable names to be imputed
# or use the utility function missing_values().
mv_sorted <- missing_values(wu_mar$data) # defaults to sorted by missingness asc.

# Step 2: Set up control parameters for generating CARs
c_control <- cars_control(support = 0.02
                          , confidence = 0.2
                          , sort_by = "chiSquared")

# Step 3: Generate CARs for the variables to be imputed.
# Note that apriori requires all factor variables.
# A convenience function is provided for this.
cars <- make_cars(all_factor(wu_mar$data)
                  , c_control = c_control
                  , var_names = names(mv_sorted))

# Step 4: Perform the imputation. Options include:
# selection by: best rule, top N mean, top N maj. vote, rhs frequency (stochastic)
# default classes: 0 = none, 1 = majority class, 2 = class frequency (stochastic)
topnm3 <- ARImpute(cars
                  , wu_mar$data
                  , ari_control = 
                    arulesimp_control(
                      method = "top_n_mean"
                      , top_n = 3
                      , use_default_classes = 0
                    ))

head(topnm3)

# Some missing values remain because the default class was not set.
```

An iterative-sequential method has also been implemented which requires additional control options for maximum iterations and target convergence.

### Other R Packages

R's extensive library of add-on packages has been surveyed in parallel with literature review. The following is a listing of packages that are useful when working with missing data that were essential to the experimental phase of this project.

#### Amelia II
Amelia II, @R-Amelia is a package for running a modified version of classic MI, i.e. not using the sequential, chained equations algorithm. It is based on EM and a MVN joint distribution. The computational challenges, discussed in \secref{sec:multimp}, are overcome by a bootstrapping step. For ordinal data, the package authors recommend either accepting a continuously valued imputation, or applying a built-in transformation which is essentially the same as the non-deterministic rounding described in @carpita2011imputation. It is a very comprehensive implementation with advanced features such as the ability to configure cell level priors. This enables the analyst to introduce domain knowledge into the model. There are also diagnostic features to check goodness of fit of the imputation model.

```{r Amelia_demo, echo=TRUE}
set.seed(1230532)
# library(Amelia) # already loaded

# include all vars, note the time series
a.out <- amelia(wu_mar$data, m = 3, p2s = 0) # print to screen = silent
# a quick look at the imputed data
# imputation 1 of 3, variable A1
head(a.out$imputations$imp1$A1, 10)
range(a.out$imputations$imp1$A1)
```

The last set of results serve to illustrate the main problem with using MI for Likert scales. @honaker2011amelia recommends allowing the algorithm to impute non-integer values as they carry more information about the underlying distribution but this will lead to disasterous results for tabulations and count data analysis. Even with rounding, this data makes no sense as it exceeds the natural range of the scale.

```{r Amelia_demo2, echo=TRUE}
# Amelia can be configured for ordinal data
a.out <- amelia(wu_mar$data, m = 3, ords = "A1", p2s = 0) # print to screen = silent
# another quick look at the imputed data
head(a.out$imputations$imp1$A1, 10)
range(a.out$imputations$imp1$A1)
```

This is better, though it still uses MVN distribution under the hood so it can be challenged by high skewed or multimodal data. Diagnostic plots such as the over-impute plot shows that the imputation may be no better than random guessing.

```{r Amelia-demo-plot2, fig.height=3}
compare.density(a.out, var = "A1")
```

```{r Amelia-demo-plot3, fig.height=3}
overimpute(a.out, var = "A1")
```

Overimpute diagnostic from the Amelia package performs multiple imputations over the observed data to show how the model would have imputed each value as if it were absent. It is very useful to see if the model is accurate. In this case, the predictions do not appear to follow the unity line at all. Rather, it appears that any value might be imputed as any other value with fairly equal chance. From this plot, it is suggested that Amelia has done no better than naive random imputation on this data.

#### mice
The mice package, @R-mice, implements the MICE algorithm which can use PMM for non-normally distributed data.
```{r mice_demo, echo=TRUE}
# library(mice) # already loaded
mice.out <- mice(wu_mar$data, m = 3, maxit = 10
                 , meth = 'pmm', seed = 500
                 , print = FALSE)

# it's easy to estimate parameters using the with() and pool() functions
A1_fit <- with(mice.out, lm(A1 ~ A2 + A3 + A4 + A5 + A6))
summary(pool(A1_fit))[, c(1:3, 5)]

B_regress <- with(mice.out
                  , lm(I(B1 + B2 + B3 + B4 + B5 + B6) ~
                        I(A1 + A2 + A3 + A4 + A5 + A6)))
summary(pool(B_regress))[, c(1:3, 5)]
```

Diagnostic plots are implemented in lattice graphics. Of these, various dot and scatter plots are not useful with Likert scales because of overplotting of discrete values. The density plot can help to reveal any systematic bias in the imputation model, as was seen with Amelia. See Figure \@ref(fig:mice-demo-plot).

```{r mice-demo-plot, fig.cap='A density plot from the mice output is useful for showing any disparity between the distribution of imputed values compared to observed values.'}
densityplot(mice.out)
```

#### VIM
VIM, @R-VIM is a package that provides some imputation methods not available elsewhere including Hot-deck, and nearest neighbours as well as a huge suite of diagnostic plots for visualizing missingness and imputed values. See Figure \@ref(fig:vim-demo-plot).

```{r vim-demo-plot, fig.height=3, echo=TRUE, fig.cap='One of VIM\'s more diagnostic plots. It also generates missingness statistics automatically.'}
# library(VIM) # already loaded

# The VIM package has some excellent vizualizations
aggr_plot <- aggr(wu_mar$data
          , col = c(myPalDark[3]
                    , myPal[4])
          , numbers = TRUE
          , sortVars = TRUE
          , labels = names(wu_mar$data)
          , cex.axis = .7
          , gap = 3
          , ylab = c("Histogram of missing data","Pattern"))
```

#### arules and arulesCBA
The arules package, @R-arules, features an interface to the most commonly used C implementation of *apriori*, @borgelt2017aprori. The arulesimp package built for this project relies on arules and *apriori* for the novel ARImpute() function. The arulesCBA package @R-arulesCBA provides a framework for classification tasks based on arules that follows the standard R formulation. However, after prototyping, it was found that the CBA function was not completely suited to the project requirements.

#### Other packages
Hmisc, @R-Hmisc and mi, @R-mi also implement MICE but the mice package is adopted in this project based on its simplicity to use. The Zelig package, @R-Zelig, is a framework for unifying many popular R packages so they follow a more standard syntax. It contains very useful functions for extracting useful results from the object returned by the amelia() function but was not required for the final cut of this project work. The ordPens package, @R-ordPens uses RLRT to test for significant differences between the effect of an ordinal predictor on dependent a dependent variable in different models. Essentially this is an ANOVA test for ordinal predictors and would be very useful to measure changes to statistical properties following imputation. Unfortunately it takes a long time to run each test and this turns out to be too costly for experiments running into hundreds of thousands of iterations.

Other packages are used for utility functions such as generating synthetic data, and parallelization of experimental runs.